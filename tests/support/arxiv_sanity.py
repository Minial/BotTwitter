page_html = r"""
<!DOCTYPE HTML>
<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Arxiv Sanity Preserver</title>

<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>

<!-- CSS -->
<link rel="stylesheet" type="text/css" href="/static/style.css">

<!-- JS -->
<script src="/static/jquery-1.8.3.min.js"></script>
<script src="/static/d3.min.js"></script>
<script src="/static/as-common.js"></script>

<!-- Google Analytics JS -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-3698471-25', 'auto');
  ga('send', 'pageview');

</script>

<script>

// passed in from flask as json
var tweets = [];
var papers = [{"abstract": "This studies the joint learning of action recognition and temporal\nlocalization in long, untrimmed videos. We employ a multi-task learning\nframework that performs the three highly related steps of action proposal,\naction recognition, and action localization refinement in parallel instead of\nthe standard sequential pipeline that performs the steps in order. We develop a\nnovel temporal actionness regression module that estimates what proportion of a\nclip contains action. We use it for temporal localization but it could have\nother applications like video retrieval, surveillance, summarization, etc. We\nalso introduce random shear augmentation during training to simulate viewpoint\nchange. We evaluate our framework on three popular video benchmarks. Results\ndemonstrate that our joint model is efficient in terms of storage and\ncomputation in that we do not need to compute and cache dense trajectory\nfeatures, and that it is several times faster than its sequential ConvNets\ncounterpart. Yet, despite being more efficient, it outperforms state-of-the-art\nmethods with respect to accuracy.", "authors": ["Yi Zhu", "Shawn Newsam"], "category": "cs.CV", "comment": "WACV 2017 camera ready, minor updates about test time efficiency", "img": "/static/thumbs/1612.07403v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.07403v2", "num_discussion": 0, "originally_published_time": "12/22/2016", "pid": "1612.07403v2", "published_time": "4/4/2017", "tags": ["cs.CV", "cs.MM"], "title": "Efficient Action Detection in Untrimmed Videos via Multi-Task Learning"}, {"abstract": "We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic\nAttention-Controlled CSR (DAC-CSR), for robust facial landmark detection on\nunconstrained faces. Our DAC-CSR divides facial landmark detection into three\ncascaded sub-tasks: face bounding box refinement, general CSR and\nattention-controlled CSR. The first two stages refine initial face bounding\nboxes and output intermediate facial landmarks. Then, an online dynamic model\nselection method is used to choose appropriate domain-specific CSRs for further\nlandmark refinement. The key innovation of our DAC-CSR is the fault-tolerant\nmechanism, using fuzzy set sample weighting for attention-controlled\ndomain-specific model training. Moreover, we advocate data augmentation with a\nsimple but effective 2D profile face generator, and context-aware feature\nextraction for better facial feature representation. Experimental results\nobtained on challenging datasets demonstrate the merits of our DAC-CSR over the\nstate-of-the-art.", "authors": ["Zhen-Hua Feng", "Josef Kittler", "William Christmas", "Patrik Huber", "Xiao-Jun Wu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.05396v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.05396v2", "num_discussion": 0, "originally_published_time": "11/16/2016", "pid": "1611.05396v2", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "Dynamic Attention-controlled Cascaded Shape Regression Exploiting\n  Training Data Augmentation and Fuzzy-set Sample Weighting"}, {"abstract": "While the channel capacity reflects a theoretical upper bound on the\nachievable information transmission rate in the limit of infinitely many bits,\nit does not characterise the information transfer of a given encoding routine\nwith finitely many bits. In this note, we characterise the quality of a code\n(i. e. a given encoding routine) by an upper bound on the expected minimum\nerror probability that can be achieved when using this code. We show that for\nequientropic channels this upper bound is minimal for codes with maximal\nmarginal entropy. As an instructive example we show for the additive white\nGaussian noise (AWGN) channel that random coding---also a capacity achieving\ncode---indeed maximises the marginal entropy in the limit of infinite messages.", "authors": ["Sebastian Weichwald", "Tatiana Fomina", "Bernhard Sch\u00f6lkopf", "Moritz Grosse-Wentrup"], "category": "q-bio.NC", "comment": "", "img": "/static/thumbs/1605.07094v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.07094v2", "num_discussion": 0, "originally_published_time": "5/23/2016", "pid": "1605.07094v2", "published_time": "4/4/2017", "tags": ["q-bio.NC", "cs.IT", "cs.LG", "math.IT", "stat.ML"], "title": "A note on the expected minimum error probability in equientropic\n  channels"}, {"abstract": "Databases are widespread, yet extracting relevant data can be difficult.\nWithout substantial domain knowledge, multivariate search queries often return\nsparse or uninformative results. This paper introduces an approach for\nsearching structured data based on probabilistic programming and nonparametric\nBayes. Users specify queries in a probabilistic language that combines standard\nSQL database search operators with an information theoretic ranking function\ncalled predictive relevance. Predictive relevance can be calculated by a fast\nsparse matrix algorithm based on posterior samples from CrossCat, a\nnonparametric Bayesian model for high-dimensional, heterogeneously-typed data\ntables. The result is a flexible search technique that applies to a broad class\nof information retrieval problems, which we integrate into BayesDB, a\nprobabilistic programming platform for probabilistic data analysis. This paper\ndemonstrates applications to databases of US colleges, global macroeconomic\nindicators of public health, and classic cars. We found that human evaluators\noften prefer the results from probabilistic search to results from a standard\nbaseline.", "authors": ["Feras Saad", "Leonardo Casarsa", "Vikash Mansinghka"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1704.01087v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01087v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.01087v1", "published_time": "4/4/2017", "tags": ["cs.AI", "cs.DB", "cs.LG", "stat.ML"], "title": "Probabilistic Search for Structured Data via Probabilistic Programming\n  and Nonparametric Bayes"}, {"abstract": "Depth from Focus (DFF) is one of the classical ill-posed inverse problems in\ncomputer vision. Most approaches recover the depth at each pixel based on the\nfocal setting which exhibits maximal sharpness. Yet, it is not obvious how to\nreliably estimate the sharpness level, particularly in low-textured areas. In\nthis paper, we propose \u0027Deep Depth From Focus (DDFF)\u0027 as the first end-to-end\nlearning approach to this problem. Towards this goal, we create a novel\nreal-scene indoor benchmark composed of 4D light-field images obtained from a\nplenoptic camera and ground truth depth obtained from a registered RGB-D\nsensor. Compared to existing benchmarks our dataset is 30 times larger,\nenabling the use of machine learning for this inverse problem. We compare our\nresults with state-of-the-art DFF methods and we also analyze the effect of\nseveral key deep architectural components. These experiments show that DDFFNet\nachieves state-of-the-art performance in all scenes, reducing depth error by\nmore than 70% wrt classic DFF methods.", "authors": ["Caner Hazirbas", "Laura Leal-Taix\u00e9", "Daniel Cremers"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.01085v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01085v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.01085v1", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "Deep Depth From Focus"}, {"abstract": "High dimensional sparse learning has imposed a great computational challenge\nto large scale data analysis. In this paper, we are interested in a broad class\nof sparse learning approaches formulated as linear programs parametrized by a\n{\\em regularization factor}, and solve them by the parametric simplex method\n(PSM). Our parametric simplex method offers significant advantages over other\ncompeting methods: (1) PSM naturally obtains the complete solution path for all\nvalues of the regularization parameter; (2) PSM provides a high precision dual\ncertificate stopping criterion; (3) PSM yields sparse solutions through very\nfew iterations, and the solution sparsity significantly reduces the\ncomputational cost per iteration. Particularly, we demonstrate the superiority\nof PSM over various sparse learning approaches, including Dantzig selector for\nsparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME\nfor sparse precision matrix estimation, sparse differential network estimation,\nand sparse Linear Programming Discriminant (LPD) analysis. We then provide\nsufficient conditions under which PSM always outputs sparse solutions such that\nits computational performance can be significantly boosted. Thorough numerical\nexperiments are provided to demonstrate the outstanding performance of the PSM\nmethod.", "authors": ["Haotian Pang", "Tuo Zhao", "Robert Vanderbei", "Han Liu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.01079v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01079v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.01079v1", "published_time": "4/4/2017", "tags": ["cs.LG", "math.OC", "stat.ML"], "title": "Homotopy Parametric Simplex Method for Sparse Learning"}, {"abstract": "Emotional intelligence is one of the key factors to the success of dialogue\nsystems or conversational agents. In this paper, we propose Emotional Chatting\nMachine (ECM) which generates responses that are appropriate not only at the\ncontent level (relevant and grammatical) but also at the emotion level\n(consistent emotional expression). To the best of our knowledge, this is the\nfirst work that addresses the emotion factor in large-scale conversation\ngeneration. ECM addresses the factor in three ways: modeling high-level\nabstraction of emotion expression by embedding emotion categories, changing of\nimplicit internal emotion states, and using explicit emotion expressions with\nan external emotion vocabulary. Experiments show that our model can generate\nresponses appropriate not only in content but also in emotion.", "authors": ["Hao Zhou", "Minlie Huang", "Tianyang Zhang", "Xiaoyan Zhu", "Bing Liu"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.01074v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01074v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.01074v1", "published_time": "4/4/2017", "tags": ["cs.CL"], "title": "Emotional Chatting Machine: Emotional Conversation Generation with\n  Internal and External Memory"}, {"abstract": "Recent CNN-based object detection methods have drastically improved their\nperformances but still use a single classifier as opposed to \"multiple experts\"\nin categorizing objects. The main motivation of introducing multi-experts is\ntwofold: i) to allow different experts to specialize in different fundamental\nobject shape priors and ii) to better capture the appearance variations caused\nby different poses and viewing angles. The proposed approach, referred to as\nmulti-expert Region-based CNN (ME R-CNN), consists of three experts each\nresponsible for objects with particular shapes: horizontally elongated,\nsquare-like, and vertically elongated. Each expert is a network with multiple\nfully connected layers and all the experts are preceded by a shared network\nwhich consists of multiple convolutional layers.\n  On top of using selective search which provides a compact, yet effective set\nof region of interests (RoIs) for object detection, we augmented the set by\nalso employing the exhaustive search for training. Incorporating the exhaustive\nsearch can provide complementary advantages: i) it captures the multitude of\nneighboring RoIs missed by the selective search, and thus ii) provide\nsignificantly larger amount of training examples to achieve the enhanced\naccuracy.", "authors": ["Hyungtae Lee", "Sungmin Eum", "Heesung Kwon"], "category": "cs.CV", "comment": "Submit to ICCV", "img": "/static/thumbs/1704.01069v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01069v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.01069v1", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "ME R-CNN: Multi-Expert Region-based CNN for Object Detection"}, {"abstract": "In this paper, we present a learning based approach to depth fusion, i.e.,\ndense 3D reconstruction from multiple depth images. The most common approach to\ndepth fusion is based on averaging truncated signed distance functions, which\nwas originally proposed by Curless and Levoy in 1996. While this method\nachieves great results, it can not reconstruct surfaces occluded in the input\nviews and requires a large number frames to filter out sensor noise and\noutliers. Motivated by large 3D model databases and recent advances in deep\nlearning, we present a novel 3D convolutional network architecture that learns\nto predict an implicit surface representation from the input depth maps. Our\nlearning based fusion approach significantly outperforms the traditional\nvolumetric fusion approach in terms of noise reduction and outlier suppression.\nBy learning the structure of real world 3D objects and scenes, our approach is\nfurther able to reconstruct occluded regions and to fill gaps in the\nreconstruction. We evaluate our approach extensively on both synthetic and\nreal-world datasets for volumetric fusion. Further, we apply our approach to\nthe problem of 3D shape completion from a single view where our approach\nachieves state-of-the-art results.", "authors": ["Gernot Riegler", "Ali Osman Ulusoy", "Horst Bischof", "Andreas Geiger"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.01047v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01047v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.01047v1", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "OctNetFusion: Learning Depth Fusion from Data"}, {"abstract": "Echo state networks are simple recurrent neural networks that are easy to\nimplement and train. Despite their simplicity, they show a form of memory and\ncan predict or regenerate sequences of data. We make use of this property to\nrealize a novel neural cryptography scheme. The key idea is to assume that\nAlice and Bob share a copy of an echo state network. If Alice trains her copy\nto memorize a message, she can communicate the trained part of the network to\nBob who plugs it into his copy to regenerate the message. Considering a\nbyte-level representation of in- and output, the technique applies to arbitrary\ntypes of data (texts, images, audio files, etc.) and practical experiments\nreveal it to satisfy the fundamental cryptographic properties of diffusion and\nconfusion.", "authors": ["Rajkumar Ramamurthy", "Christian Bauckhage", "Krisztian Buza", "Stefan Wrobel"], "category": "cs.CR", "comment": "8 pages, ICANN 2017", "img": "/static/thumbs/1704.01046v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01046v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.01046v1", "published_time": "4/4/2017", "tags": ["cs.CR", "cs.NE"], "title": "Using Echo State Networks for Cryptography"}, {"abstract": "In this paper, we study an important yet less explored aspect in video\ndetection and tracking -- stability. Surprisingly, there is no prior work that\ntried to study it. As a result, we start our work by proposing a novel\nevaluation metric for video detection which considers both stability and\naccuracy. For accuracy, we extend the existing accuracy metric mean Average\nPrecision (mAP). For stability, we decompose it into three terms: fragment\nerror, center position error, scale and ratio error. Each error represents one\naspect of stability. Furthermore, we demonstrate that the stability metric has\nlow correlation with accuracy metric. Thus, it indeed captures a different\nperspective of quality. Lastly, based on this metric, we evaluate several\nexisting methods for video detection and show how they affect accuracy and\nstability. We believe our work can provide guidance and solid baselines for\nfuture researches in the related areas.", "authors": ["Hong Zhang", "Naiyan Wang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.06467v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.06467v2", "num_discussion": 0, "originally_published_time": "11/20/2016", "pid": "1611.06467v2", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "On The Stability of Video Detection and Tracking"}, {"abstract": "We develop differentially private hypothesis testing methods for the small\nsample regime. Given a sample $\\cal D$ from a categorical distribution $p$ over\nsome domain $\\Sigma$, an explicitly described distribution $q$ over $\\Sigma$,\nsome privacy parameter $\\varepsilon$, accuracy parameter $\\alpha$, and\nrequirements $\\beta_{\\rm I}$ and $\\beta_{\\rm II}$ for the type I and type II\nerrors of our test, the goal is to distinguish between $p=q$ and\n$d_{\\rm{TV}}(p,q) \\geq \\alpha$.\n  We provide theoretical bounds for the sample size $|{\\cal D}|$ so that our\nmethod both satisfies $(\\varepsilon,0)$-differential privacy, and guarantees\n$\\beta_{\\rm I}$ and $\\beta_{\\rm II}$ type I and type II errors. We show that\ndifferential privacy may come for free in some regimes of parameters, and we\nalways beat the sample complexity resulting from running the $\\chi^2$-test with\nnoisy counts, or standard approaches such as repetition for endowing\nnon-private $\\chi^2$-style statistics with differential privacy guarantees. We\nexperimentally compare the sample complexity of our method to that of recently\nproposed methods for private hypothesis testing.", "authors": ["Bryan Cai", "Constantinos Daskalakis", "Gautam Kamath"], "category": "cs.DS", "comment": "Fixed references", "img": "/static/thumbs/1703.10127v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10127v2", "num_discussion": 0, "originally_published_time": "3/29/2017", "pid": "1703.10127v2", "published_time": "4/4/2017", "tags": ["cs.DS", "cs.CR", "cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "title": "Priv\u0027IT: Private and Sample Efficient Identity Testing"}, {"abstract": "The problem of Non-Gaussian Component Analysis (NGCA) is about finding a\nmaximal low-dimensional subspace $E$ in $\\mathbb{R}^n$ so that data points\nprojected onto $E$ follow a non-gaussian distribution. Although this is an\nappropriate model for some real world data analysis problems, there has been\nlittle progress on this problem over the last decade.\n  In this paper, we attempt to address this state of affairs in two ways.\nFirst, we give a new characterization of standard gaussian distributions in\nhigh-dimensions, which lead to effective tests for non-gaussianness. Second, we\npropose a simple algorithm, \\emph{Reweighted PCA}, as a method for solving the\nNGCA problem. We prove that for a general unknown non-gaussian distribution,\nthis algorithm recovers at least one direction in $E$, with sample and time\ncomplexity depending polynomially on the dimension of the ambient space. We\nconjecture that the algorithm actually recovers the entire $E$.", "authors": ["Yan Shuo Tan", "Roman Vershynin"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.01041v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01041v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.01041v1", "published_time": "4/4/2017", "tags": ["cs.LG", "math.PR", "stat.ML", "68Q87"], "title": "Polynomial Time and Sample Complexity for Non-Gaussian Component\n  Analysis: Spectral Methods"}, {"abstract": "In this work we investigate the use of deep learning for distortion-generic\nblind image quality assessment. We report on different design choices, ranging\nfrom the use of features extracted from pre-trained Convolutional Neural\nNetworks (CNNs) as a generic image description, to the use of features\nextracted from a CNN fine-tuned for the image quality task. Our best proposal,\nnamed DeepBIQ, estimates the image quality by average pooling the scores\npredicted on multiple sub-regions of the original image. The score of each\nsub-region is computed using a Support Vector Regression (SVR) machine taking\nas input features extracted using a CNN fine-tuned for category-based image\nquality assessment. Experimental results on the LIVE In the Wild Image Quality\nChallenge Database and on the LIVE Image Quality Assessment Database show that\nDeepBIQ outperforms the state-of-the-art methods compared, having a Linear\nCorrelation Coefficient (LCC) with human subjective scores of almost 0.91 and\n0.98 respectively. Furthermore, in most of the cases, the quality score\npredictions of DeepBIQ are closer to the average observer than those of a\ngeneric human observer.", "authors": ["Simone Bianco", "Luigi Celona", "Paolo Napoletano", "Raimondo Schettini"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1602.05531v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.05531v5", "num_discussion": 0, "originally_published_time": "2/17/2016", "pid": "1602.05531v5", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "On the Use of Deep Learning for Blind Image Quality Assessment"}, {"abstract": "This paper describes a new entropy-style of equation that may be useful in a\ngeneral sense, but can be applied to a cognitive model with related processes.\nThe model is based on the human brain, with automatic and distributed pattern\nactivity. Methods for carrying out the different processes are suggested. The\nmain purpose of this paper is to reaffirm earlier research on different\nknowledge-based and experience-based clustering techniques. The overall\narchitecture has stayed essentially the same and so it is the localised\nprocesses or smaller details that have been updated. For example, a counting\nmechanism is used slightly differently, to measure a level of \u0027cohesion\u0027\ninstead of a \u0027correct\u0027 classification, over pattern instances. The introduction\nof features has further enhanced the architecture and the new entropy-style\nequation is proposed. While an earlier paper defined three levels of functional\nrequirement, this paper re-defines the levels in a more human vernacular, with\nhigher-level goals described in terms of action-result pairs.", "authors": ["Kieran Greer"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1507.04928v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1507.04928v5", "num_discussion": 0, "originally_published_time": "7/17/2015", "pid": "1507.04928v5", "published_time": "4/4/2017", "tags": ["cs.AI"], "title": "A Brain-like Cognitive Process with Shared Methods"}, {"abstract": "Data acquisition, storage and management have been improved, while the key\nfactors of many phenomena are not well known. Consequently, irrelevant and\nredundant features artificially increase the size of datasets, which\ncomplicates learning tasks, such as regression. To address this problem,\nfeature selection methods have been proposed. This paper introduces a new\nsupervised filter based on the Morisita estimator of intrinsic dimension. It\ncan identify relevant features and distinguish between redundant and irrelevant\ninformation. Besides, it offers a clear graphical representation of the\nresults, and it can be easily implemented in different programming languages.\nComprehensive numerical experiments are conducted using simulated datasets\ncharacterized by different levels of complexity, sample size and noise. The\nsuggested algorithm is also successfully tested on a selection of real world\napplications and compared with RReliefF using extreme learning machine. In\naddition, a new measure of feature relevance is presented and discussed.", "authors": ["Jean Golay", "Michael Leuenberger", "Mikhail Kanevski"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1602.00216v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.00216v6", "num_discussion": 0, "originally_published_time": "1/31/2016", "pid": "1602.00216v6", "published_time": "4/4/2017", "tags": ["stat.ML", "cs.LG"], "title": "Feature Selection for Regression Problems Based on the Morisita\n  Estimator of Intrinsic Dimension"}, {"abstract": "A typical problem in causal modeling is the instability of model structure\nlearning, i.e., small changes in finite data can result in completely different\noptimal models. The present work introduces a novel causal modeling algorithm\nfor longitudinal data, that is robust for finite samples based on recent\nadvances in stability selection using subsampling and selection algorithms. Our\napproach uses exploratory search but allows incorporation of prior knowledge,\ne.g., the absence of a particular causal relationship between two specific\nvariables. We represent causal relationships using structural equation models.\nModels are scored along two objectives: the model fit and the model complexity.\nSince both objectives are often conflicting we apply a multi-objective\nevolutionary algorithm to search for Pareto optimal models. To handle the\ninstability of small finite data samples, we repeatedly subsample the data and\nselect those substructures (from the optimal models) that are both stable and\nparsimonious. These substructures can be visualized through a causal graph. Our\nmore exploratory approach achieves at least comparable performance as, but\noften a significant improvement over state-of-the-art alternative approaches on\na simulated data set with a known ground truth. We also present the results of\nour method on three real-world longitudinal data sets on chronic fatigue\nsyndrome, Alzheimer disease, and chronic kidney disease. The findings obtained\nwith our approach are generally in line with results from more\nhypothesis-driven analyses in earlier studies and suggest some novel\nrelationships that deserve further research.", "authors": ["Ridho Rahmadi", "Perry Groot", "Marieke HC van Rijn", "Jan AJG van den Brand", "Marianne Heins", "Hans Knoop", "Tom Heskes"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1605.06838v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06838v3", "num_discussion": 0, "originally_published_time": "5/22/2016", "pid": "1605.06838v3", "published_time": "4/4/2017", "tags": ["stat.ML", "cs.AI"], "title": "Causality on Longitudinal Data: Stable Specification Search in\n  Constrained Structural Equation Modeling"}, {"abstract": "Glaucoma is the second leading cause of blindness all over the world, with\napproximately 60 million cases reported worldwide in 2010. If undiagnosed in\ntime, glaucoma causes irreversible damage to the optic nerve leading to\nblindness. The optic nerve head examination, which involves measurement of\ncup-to-disc ratio, is considered one of the most valuable methods of structural\ndiagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation\nof optic disc and optic cup on eye fundus images and can be performed by modern\ncomputer vision algorithms. This work presents universal approach for automatic\noptic disc and cup segmentation, which is based on deep learning, namely,\nmodification of U-Net convolutional neural network. Our experiments include\ncomparison with the best known methods on publicly available databases\nDRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,\nour method achieves quality comparable to current state-of-the-art methods,\noutperforming them in terms of the prediction time.", "authors": ["Artem Sevastopolsky"], "category": "cs.CV", "comment": "accepted for publication in \"Pattern Recognition and Image Analysis:\n  Advances in Mathematical Theo...", "img": "/static/thumbs/1704.00979v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00979v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00979v1", "published_time": "4/4/2017", "tags": ["cs.CV", "stat.ML"], "title": "Optic Disc and Cup Segmentation Methods for Glaucoma Detection with\n  Modification of U-Net Convolutional Neural Network"}, {"abstract": "We construct universal prediction systems in the spirit of Popper\u0027s\nfalsifiability and Kolmogorov complexity and randomness. These prediction\nsystems do not depend on any statistical assumptions (but under the IID\nassumption they dominate, to within the usual accuracy, conformal prediction).\nOur constructions give rise to a theory of algorithmic complexity and\nrandomness of time containing analogues of several notions and results of the\nclassical theory of Kolmogorov complexity and randomness.", "authors": ["Vladimir Vovk", "Dusko Pavlovic"], "category": "cs.LG", "comment": "27 pages", "img": "/static/thumbs/1603.04283v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.04283v2", "num_discussion": 0, "originally_published_time": "3/14/2016", "pid": "1603.04283v2", "published_time": "4/4/2017", "tags": ["cs.LG", "68T05", "I.2.6"], "title": "Universal probability-free prediction"}, {"abstract": "Bayesian optimization (BO) is a global optimization strategy designed to find\nthe minimum of expensive black-box functions $g$ typically defined on a\ncontinuous sets of $\\mathcal{R}^d$. Using a Gaussian process (GP) as a\nsurrogate model for the objective and an acquisition function to systematically\nsearch its domain, BO strategies aim to minimize the amount of samples required\nto find the minimum of $g$. Although currently available acquisition functions\naddress this goal with different degree of success, an over-exploration effect\nof the contour of $g$ is typically observed. This is due to the myopic nature\nof most acquisitions that greedily try to over-reduce uncertainty in the border\nof the search domain. In most real problems, however, like the configuration of\nmachine learning algorithms, the function domain is conservatively large and\nwith a high probability the global minimum is not at the boundary. We propose a\nmethod to incorporate this knowledge into the searching process by adding\nvirtual derivative observations at the borders of the search space. We use the\nproperties of GP models that allow us to easily impose conditions on the\npartial derivatives of the objective. The method is applicable with any\nacquisition function, it is easy to use and consistently reduces the number of\nevaluations required to find the minimum of $g$ irrespective of the acquisition\nused. We illustrate the benefits our approach in a simulation study with a\nbattery of objective functions.", "authors": ["Eero Siivola", "Aki Vehtari", "Jarno Vanhatalo", "Javier Gonz\u00e1lez"], "category": "stat.ML", "comment": "10 pages, 12 figures", "img": "/static/thumbs/1704.00963v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00963v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00963v1", "published_time": "4/4/2017", "tags": ["stat.ML", "stat.CO", "stat.ME"], "title": "Bayesian optimization with virtual derivative sign observations"}, {"abstract": "It is known that the learning rate is the most important hyper-parameter to\ntune for training deep neural networks. This paper describes a new method for\nsetting the learning rate, named cyclical learning rates, which practically\neliminates the need to experimentally find the best values and schedule for the\nglobal learning rates. Instead of monotonically decreasing the learning rate,\nthis method lets the learning rate cyclically vary between reasonable boundary\nvalues. Training with cyclical learning rates instead of fixed values achieves\nimproved classification accuracy without a need to tune and often in fewer\niterations. This paper also describes a simple way to estimate \"reasonable\nbounds\" -- linearly increasing the learning rate of the network for a few\nepochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10\nand CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets,\nand the ImageNet dataset with the AlexNet and GoogLeNet architectures. These\nare practical tools for everyone who trains neural networks.", "authors": ["Leslie N. Smith"], "category": "cs.CV", "comment": "Presented at WACV 2017; see https://github.com/bckenstler/CLR for\n  instructions to implement CLR in...", "img": "/static/thumbs/1506.01186v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.01186v6", "num_discussion": 0, "originally_published_time": "6/3/2015", "pid": "1506.01186v6", "published_time": "4/4/2017", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "Cyclical Learning Rates for Training Neural Networks"}, {"abstract": "This paper presents a design of a non-player character (AI) for promoting\nbalancedness in use of body segments when engaging in full-body motion gaming.\nIn our experiment, we settle a battle between the proposed AI and a player by\nusing FightingICE, a fighting game platform for AI development. A middleware\ncalled UKI is used to allow the player to control the game by using body motion\ninstead of the keyboard and mouse. During gameplay, the proposed AI analyze\nhealth states of the player; it determines its next action by predicting how\neach candidate action, recommended by a Monte-Carlo tree search algorithm, will\ninduce the player to move, and how the player\u0027s health tends to be affected.\nOur result demonstrates successful improvement in balancedness in use of body\nsegments on 4 out of 5 subjects.", "authors": ["Pujana Paliyawan", "Takahiro Kusano", "Yuto Nakagawa", "Tomohiro Harada", "Ruck Thawonmas"], "category": "cs.AI", "comment": "A revised version of our paper for 2017 AAAI Spring Symposium Series\n  (Well-Being AI: From Machine ...", "img": "/static/thumbs/1704.00961v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00961v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00961v1", "published_time": "4/4/2017", "tags": ["cs.AI", "cs.CY", "cs.HC", "I.2.1; K.8.0"], "title": "Adaptive Motion Gaming AI for Health Promotion"}, {"abstract": "We present the first parser for UCCA, a cross-linguistically applicable\nframework for semantic representation, which builds on extensive typological\nwork and supports rapid annotation. UCCA poses a challenge for existing parsing\ntechniques, as it exhibits reentrancy (resulting in DAG structures),\ndiscontinuous structures and non-terminal nodes corresponding to complex\nsemantic units. To our knowledge, the conjunction of these formal properties is\nnot supported by any existing parser. Our transition-based parser, which uses a\nnovel transition set and features based on bidirectional LSTMs, has value not\njust for UCCA parsing: its ability to handle more general graph structures can\ninform the development of parsers for other semantic DAG structures, and in\nlanguages that frequently use discontinuous structures.", "authors": ["Daniel Hershcovich", "Omri Abend", "Ari Rappoport"], "category": "cs.CL", "comment": "16 pages; Accepted as long paper at ACL2017", "img": "/static/thumbs/1704.00552v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00552v2", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00552v2", "published_time": "4/4/2017", "tags": ["cs.CL"], "title": "A Transition-Based Directed Acyclic Graph Parser for UCCA"}, {"abstract": "A new resolution enhancement method is presented for multispectral and\nmulti-resolution images, such as these provided by the Sentinel-2 satellites.\nStarting from the highest resolution bands, band-dependent information\n(reflectance) is separated from information that is common to all bands\n(geometry of scene elements). This model is then applied to unmix\nlow-resolution bands, preserving their reflectance, while propagating\nband-independent information to preserve the sub-pixel details. A reference\nimplementation is provided, with an application example for super-resolving\nSentinel-2 data.", "authors": ["Nicolas Brodu"], "category": "cs.CV", "comment": "Source code with a ready-to-use script for super-resolving Sentinel-2\n  data is available at http://...", "img": "/static/thumbs/1609.07986v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.07986v3", "num_discussion": 0, "originally_published_time": "9/26/2016", "pid": "1609.07986v3", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "Super-resolving multiresolution images with band-independant geometry of\n  multispectral pixels"}, {"abstract": "In this paper, we describe a methodology to infer Bullish or Bearish\nsentiment towards companies/brands. More specifically, our approach leverages\naffective lexica and word embeddings in combination with convolutional neural\nnetworks to infer the sentiment of financial news headlines towards a target\ncompany. Such architecture was used and evaluated in the context of the SemEval\n2017 challenge (task 5, subtask 2), in which it obtained the best performance.", "authors": ["Youness Mansar", "Lorenzo Gatti", "Sira Ferradans", "Marco Guerini", "Jacopo Staiano"], "category": "cs.CL", "comment": "6 pages, 1 figure; accepted for publication at the International\n  Workshop on Semantic Evaluation (...", "img": "/static/thumbs/1704.00939v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00939v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00939v1", "published_time": "4/4/2017", "tags": ["cs.CL", "cs.CY"], "title": "Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring\n  Sentiment towards Brands from Financial News Headlines"}, {"abstract": "Previous approaches to training syntax-based sentiment classification models\nrequired phrase-level annotated corpora, which are not readily available in\nmany languages other than English. Thus, we propose the use of tree-structured\nLong Short-Term Memory with an attention mechanism that pays attention to each\nsubtree of the parse tree. Experimental results indicate that our model\nachieves the state-of-the-art performance in a Japanese sentiment\nclassification task.", "authors": ["Ryosuke Miyazaki", "Mamoru Komachi"], "category": "cs.CL", "comment": "6 pages", "img": "/static/thumbs/1704.00924v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00924v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00924v1", "published_time": "4/4/2017", "tags": ["cs.CL"], "title": "Japanese Sentiment Classification using a Tree-Structured Long\n  Short-Term Memory with Attention"}, {"abstract": "This paper presents an algorithm for the unsupervised learning of latent\nvariable models from unlabeled sets of data. We base our technique on spectral\ndecomposition, providing a technique that proves to be robust both in theory\nand in practice. We also describe how to use this algorithm to learn the\nparameters of two well known text mining models: single topic model and Latent\nDirichlet Allocation, providing in both cases an efficient technique to\nretrieve the parameters to feed the algorithm. We compare the results of our\nalgorithm with those of existing algorithms on synthetic data, and we provide\nexamples of applications to real world text corpora for both single topic model\nand LDA, obtaining meaningful results.", "authors": ["Matteo Ruffini", "Marta Casanellas", "Ricard Gavald\u00e0"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1612.03409v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.03409v2", "num_discussion": 0, "originally_published_time": "12/11/2016", "pid": "1612.03409v2", "published_time": "4/4/2017", "tags": ["stat.ML"], "title": "A New Spectral Method for Latent Variable Models"}, {"abstract": "The probability density function of a probability distribution is a\nfundamental concept in probability theory and a key ingredient in various\nwidely used machine learning methods. However, the necessary framework for\ncompiling probabilistic functional programs to density functions has only\nrecently been developed. In this work, we present a density compiler for a\nprobabilistic language with failure and both discrete and continuous\ndistributions, and provide a proof of its soundness. The compiler greatly\nreduces the development effort of domain experts, which we demonstrate by\nsolving inference problems from various scientific applications, such as\nmodelling the global carbon cycle, using a standard Markov chain Monte Carlo\nframework.", "authors": ["Sooraj Bhat", "Johannes Borgstr\u00f6m", "Andrew D. Gordon", "Claudio Russo"], "category": "cs.PL", "comment": "", "img": "/static/thumbs/1704.00917v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00917v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00917v1", "published_time": "4/4/2017", "tags": ["cs.PL", "cs.AI", "F.3.2; G.3; I.2.5"], "title": "Deriving Probability Density Functions from Probabilistic Functional\n  Programs"}, {"abstract": "Research in analysis of microblogging platforms is experiencing a renewed\nsurge with a large number of works applying representation learning models for\napplications like sentiment analysis, semantic textual similarity computation,\nhashtag prediction, etc. Although the performance of the representation\nlearning models has been better than the traditional baselines for such tasks,\nlittle is known about the elementary properties of a tweet encoded within these\nrepresentations, or why particular representations work better for certain\ntasks. Our work presented here constitutes the first step in opening the\nblack-box of vector embeddings for tweets. Traditional feature engineering\nmethods for high-level applications have exploited various elementary\nproperties of tweets. We believe that a tweet representation is effective for\nan application because it meticulously encodes the application-specific\nelementary properties of tweets. To understand the elementary properties\nencoded in a tweet representation, we evaluate the representations on the\naccuracy to which they can model each of those properties such as tweet length,\npresence of particular words, hashtags, mentions, capitalization, etc. Our\nsystematic extensive study of nine supervised and four unsupervised tweet\nrepresentations against most popular eight textual and five social elementary\nproperties reveal that Bi-directional LSTMs (BLSTMs) and Skip-Thought Vectors\n(STV) best encode the textual and social properties of tweets respectively.\nFastText is the best model for low resource settings, providing very little\ndegradation with reduction in embedding size. Finally, we draw interesting\ninsights by correlating the model performance obtained for elementary property\nprediction tasks with the high-level downstream applications.", "authors": ["J Ganesh", "Manish Gupta", "Vasudeva Varma"], "category": "cs.CL", "comment": "Under review at ASONAM 2017; Initial version presented at NIPS 2016\n  workshop can be found at arXiv...", "img": "/static/thumbs/1704.00898v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00898v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00898v1", "published_time": "4/4/2017", "tags": ["cs.CL"], "title": "Interpretation of Semantic Tweet Representations"}, {"abstract": "We address the problem of camera-to-laser-scanner calibration using a\ncheckerboard and multiple image-laser scan pairs. Distinguishing which laser\npoints measure the checkerboard and which lie on the background is essential to\nany such system. We formulate the checkerboard extraction as a combinatorial\noptimization problem with a clear cut objective function. We propose a\nbranch-and-bound technique that deterministically and globally optimizes the\nobjective. Unlike what is available in the literature, the proposed method is\nnot heuristic and does not require assumptions such as constraints on the\nbackground or relying on discontinuity of the range measurements to partition\nthe data into line segments. The proposed approach is generic and can be\napplied to both 3D or 2D laser scanners as well as the cases where multiple\ncheckerboards are present. We demonstrate the effectiveness of the proposed\napproach by providing numerical simulations as well as experimental results.", "authors": ["Alireza Khosravian", "Tat-Jun Chin", "Ian Reid"], "category": "cs.RO", "comment": "To appear in IEEE Conference on Robotics and Automation 2017", "img": "/static/thumbs/1704.00887v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00887v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00887v1", "published_time": "4/4/2017", "tags": ["cs.RO", "cs.CV"], "title": "A Branch-and-Bound Algorithm for Checkerboard Extraction in Camera-Laser\n  Calibration"}, {"abstract": "We use the scattering network as a generic and fixed ini-tialization of the\nfirst layers of a supervised hybrid deep network. We show that early layers do\nnot necessarily need to be learned, providing the best results to-date with\npre-defined representations while being competitive with Deep CNNs. Using a\nshallow cascade of 1 x 1 convolutions, which encodes scattering coefficients\nthat correspond to spatial windows of very small sizes, permits to obtain\nAlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local\nencoding explicitly learns invariance w.r.t. rotations. Combining scattering\nnetworks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on\nimagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing\nonly 10 layers. We also find that hybrid architectures can yield excellent\nperformance in the small sample regime, exceeding their end-to-end\ncounterparts, through their ability to incorporate geometrical priors. We\ndemonstrate this on subsets of the CIFAR-10 dataset and on the STL-10 dataset.", "authors": ["Edouard Oyallon", "Eugene Belilovsky", "Sergey Zagoruyko"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1703.08961v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.08961v2", "num_discussion": 0, "originally_published_time": "3/27/2017", "pid": "1703.08961v2", "published_time": "4/4/2017", "tags": ["cs.CV", "cs.LG"], "title": "Scaling the Scattering Transform: Deep Hybrid Networks"}, {"abstract": "Kernel Ridge Regression (KRR) is a simple yet powerful technique for\nnon-parametric regression whose computation amounts to solving a linear system.\nThis system is usually dense and highly ill-conditioned. In addition, the\ndimensions of the matrix are the same as the number of data points, so direct\nmethods are unrealistic for large-scale datasets. In this paper, we propose a\npreconditioning technique for accelerating the solution of the aforementioned\nlinear system. The preconditioner is based on random feature maps, such as\nrandom Fourier features, which have recently emerged as a powerful technique\nfor speeding up and scaling the training of kernel-based methods, such as\nkernel ridge regression, by resorting to approximations. However, random\nfeature maps only provide crude approximations to the kernel function, so\ndelivering state-of-the-art results by directly solving the approximated system\nrequires the number of random features to be very large. We show that random\nfeature maps can be much more effective in forming preconditioners, since under\ncertain conditions a not-too-large number of random features is sufficient to\nyield an effective preconditioner. We empirically evaluate our method and show\nit is highly effective for datasets of up to one million training examples.", "authors": ["Haim Avron", "Kenneth L. Clarkson", "David P. Woodruff"], "category": "cs.NA", "comment": "", "img": "/static/thumbs/1611.03220v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.03220v3", "num_discussion": 0, "originally_published_time": "11/10/2016", "pid": "1611.03220v3", "published_time": "4/4/2017", "tags": ["cs.NA", "cs.DS", "cs.LG", "math.NA"], "title": "Faster Kernel Ridge Regression Using Sketching and Preconditioning"}, {"abstract": "Analyzing multivariate time series data is important for many applications\nsuch as automated control, fault diagnosis and anomaly detection. One of the\nkey challenges is to learn latent features automatically from dynamically\nchanging multivariate input. In visual recognition tasks, convolutional neural\nnetworks (CNNs) have been successful to learn generalized feature extractors\nwith shared parameters over the spatial domain. However, when high-dimensional\nmultivariate time series is given, designing an appropriate CNN model structure\nbecomes challenging because the kernels may need to be extended through the\nfull dimension of the input volume. To address this issue, we present two\nstructure learning algorithms for deep CNN models. Our algorithms exploit the\ncovariance structure over multiple time series to partition input volume into\ngroups. The first algorithm learns the group CNN structures explicitly by\nclustering individual input sequences. The second algorithm learns the group\nCNN structures implicitly from the error backpropagation. In experiments with\ntwo real-world datasets, we demonstrate that our group CNNs outperform existing\nCNN based regression methods.", "authors": ["Subin Yi", "Janghoon Ju", "Man-Ki Yoon", "Jaesik Choi"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1703.09938v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.09938v3", "num_discussion": 0, "originally_published_time": "3/29/2017", "pid": "1703.09938v3", "published_time": "4/4/2017", "tags": ["cs.LG"], "title": "Grouped Convolutional Neural Networks for Multivariate Time Series"}, {"abstract": "The increasing quality of smartphone cameras and variety of photo editing\napplications, in addition to the rise in popularity of image-centric social\nmedia, have all led to a phenomenal growth in mobile-based photography.\nAdvances in computer vision and machine learning techniques provide a large\nnumber of cloud-based services with the ability to provide content analysis,\nface recognition, and object detection facilities to third parties. These\ninferences and analytics might come with undesired privacy risks to the\nindividuals.\n  In this paper, we address a fundamental challenge: Can we utilize the local\nprocessing capabilities of modern smartphones efficiently to provide desired\nfeatures to approved analytics services, while protecting against undesired\ninference attacks and preserving privacy on the cloud? We propose a hybrid\narchitecture for a distributed deep learning model between the smartphone and\nthe cloud. We rely on the Siamese network and machine learning approaches for\nproviding privacy based on defined privacy constraints. We also use transfer\nlearning techniques to evaluate the proposed method. Using the latest deep\nlearning models for Face Recognition, Emotion Detection, and Gender\nClassification techniques, we demonstrate the effectiveness of our technique in\nproviding highly accurate classification results for the desired analytics,\nwhile proving strong privacy guarantees.", "authors": ["Seyed Ali Osia", "Ali Shahin Shamsabadi", "Ali Taheri", "Hamid R. Rabiee", "Nicholas D. Lane", "Hamed Haddadi"], "category": "cs.LG", "comment": "Technical Report", "img": "/static/thumbs/1703.02952v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.02952v4", "num_discussion": 0, "originally_published_time": "3/8/2017", "pid": "1703.02952v4", "published_time": "4/4/2017", "tags": ["cs.LG", "cs.CV"], "title": "A Hybrid Deep Learning Architecture for Privacy-Preserving Mobile\n  Analytics"}, {"abstract": "In most state-of-the-art hashing-based visual search systems, local image\ndescriptors of an image are first aggregated as a single feature vector. This\nfeature vector is then subjected to a hashing function that produces a binary\nhash code. In previous work, the aggregating and the hashing processes are\ndesigned independently. In this paper, we propose a novel framework where\nfeature aggregating and hashing are designed simultaneously and optimized\njointly. Specifically, our joint optimization produces aggregated\nrepresentations that can be better reconstructed by some binary codes. This\nleads to more discriminative binary hash codes and improved retrieval accuracy.\nIn addition, we also propose a fast version of the recently-proposed Binary\nAutoencoder to be used in our proposed framework. We perform extensive\nretrieval experiments on several benchmark datasets with both SIFT and\nconvolutional features. Our results suggest that the proposed framework\nachieves significant improvements over the state of the art.", "authors": ["Thanh-Toan Do", "Dang-Khoa Le Tan", "Trung T. Pham", "Ngai-Man Cheung"], "category": "cs.CV", "comment": "Accepted to CVPR 2017", "img": "/static/thumbs/1704.00860v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00860v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00860v1", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "Simultaneous Feature Aggregating and Hashing for Large-scale Image\n  Search"}, {"abstract": "The objective of this paper is to design an embedding method that maps local\nfeatures describing an image (e.g. SIFT) to a higher dimensional representation\nuseful for the image retrieval problem. First, motivated by the relationship\nbetween the linear approximation of a nonlinear function in high dimensional\nspace and the stateof-the-art feature representation used in image retrieval,\ni.e., VLAD, we propose a new approach for the approximation. The embedded\nvectors resulted by the function approximation process are then aggregated to\nform a single representation for image retrieval. Second, in order to make the\nproposed embedding method applicable to large scale problem, we further derive\nits fast version in which the embedded vectors can be efficiently computed,\ni.e., in the closed-form. We compare the proposed embedding methods with the\nstate of the art in the context of image search under various settings: when\nthe images are represented by medium length vectors, short vectors, or binary\nvectors. The experimental results show that the proposed embedding methods\noutperform existing the state of the art on the standard public image retrieval\nbenchmarks.", "authors": ["Thanh-Toan Do", "Ngai-Man Cheung"], "category": "cs.CV", "comment": "Accepted to TPAMI 2017. The implementation and precomputed features\n  of the proposed F-FAemb are re...", "img": "/static/thumbs/1605.06914v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06914v3", "num_discussion": 0, "originally_published_time": "5/23/2016", "pid": "1605.06914v3", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "Embedding based on function approximation for large scale image search"}, {"abstract": "This chapter describes the history of metaheuristics in five distinct\nperiods, starting long before the first use of the term and ending a long time\nin the future.", "authors": ["Kenneth Sorensen", "Marc Sevaux", "Fred Glover"], "category": "cs.AI", "comment": "27 pages, to appear in: R. Marti, P. Pardalos, and M. Resende, eds.,\n  Handbook of Heuristics, Sprin...", "img": "/static/thumbs/1704.00853v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00853v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00853v1", "published_time": "4/4/2017", "tags": ["cs.AI"], "title": "A History of Metaheuristics"}, {"abstract": "The Morse-Smale complex of a function $f$ decomposes the sample space into\ncells where $f$ is increasing or decreasing. When applied to nonparametric\ndensity estimation and regression, it provides a way to represent, visualize,\nand compare multivariate functions. In this paper, we present some statistical\nresults on estimating Morse-Smale complexes. This allows us to derive new\nresults for two existing methods: mode clustering and Morse-Smale regression.\nWe also develop two new methods based on the Morse-Smale complex: a\nvisualization technique for multivariate functions and a two-sample,\nmultivariate hypothesis test.", "authors": ["Yen-Chi Chen", "Christopher R. Genovese", "Larry Wasserman"], "category": "math.ST", "comment": "45 pages, 13 figures. Accepted to Electronic Journal of Statistics", "img": "/static/thumbs/1506.08826v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.08826v2", "num_discussion": 0, "originally_published_time": "6/29/2015", "pid": "1506.08826v2", "published_time": "4/4/2017", "tags": ["math.ST", "stat.ME", "stat.ML", "stat.TH", "62G20 (Primary), 62G05, 62G08 (Secondary)"], "title": "Statistical Inference using the Morse-Smale Complex"}, {"abstract": "Online action recognition aims to recognize actions from unsegmented streams\nof data in a continuous manner. One of the challenges in online recognition is\nthe accumulation of evidence for decision making. This paper presents a fast\nand efficient online method to recognize actions from a stream of noisy\nskeleton data. The method adopts a covariance descriptor calculated from\nskeleton data and is based on a novel method developed for incrementally\nlearning the covariance descriptors, referred to as weighted covariance\ndescriptors, so that past frames have less contributions to the descriptor and\ncurrent frames and informative frames such as key frames contributes more\ntowards the descriptor. The online recognition is achieved using an efficient\nnearest neighbour search against a set of trained actions. Experimental results\non MSRC-12 Kinect Gesture dataset and our newly collocated online action\nrecognition dataset have demonstrated the efficacy of the proposed method.", "authors": ["Chang Tang", "Wanqing Li", "Pichao Wang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1511.03028v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.03028v2", "num_discussion": 0, "originally_published_time": "11/10/2015", "pid": "1511.03028v2", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "Online Action Recognition based on Incremental Learning of Weighted\n  Covariance Descriptors"}, {"abstract": "As the type and the number of such venues increase, automated analysis of\nsentiment on textual resources has become an essential data mining task. In\nthis paper, we investigate the problem of mining opinions on the collection of\ninformal short texts. Both positive and negative sentiment strength of texts\nare detected. We focus on a non-English language that has few resources for\ntext mining. This approach would help enhance the sentiment analysis in\nlanguages where a list of opinionated words does not exist. We propose a new\nmethod projects the text into dense and low dimensional feature vectors\naccording to the sentiment strength of the words. We detect the mixture of\npositive and negative sentiments on a multi-variant scale. Empirical evaluation\nof the proposed framework on Turkish tweets shows that our approach gets good\nresults for opinion mining.", "authors": ["Esra Akbas"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.00016v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00016v2", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00016v2", "published_time": "4/4/2017", "tags": ["cs.CL", "cs.IR"], "title": "Opinion Mining on Non-English Short Text"}, {"abstract": "Building a voice conversion (VC) system from non-parallel speech corpora is\nchallenging but highly valuable in real application scenarios. In most\nsituations, the source and the target speakers do not repeat the same texts or\nthey may even speak different languages. In this case, one possible, although\nindirect, solution is to build a generative model for speech. Generative models\nfocus on explaining the observations with latent variables instead of learning\na pairwise transformation function, thereby bypassing the requirement of speech\nframe alignment. In this paper, we propose a non-parallel VC framework with a\nWasserstein generative adversarial network (W-GAN) that explicitly takes a\nVC-related objective into account. Experimental results corroborate the\ncapability of our framework for building a VC system from unaligned data, and\ndemonstrate improved conversion quality.", "authors": ["Chin-Cheng Hsu", "Hsin-Te Hwang", "Yi-Chiao Wu", "Yu Tsao", "Hsin-Min Wang"], "category": "cs.CL", "comment": "Submitted to INTERSPEECH 2017", "img": "/static/thumbs/1704.00849v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00849v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00849v1", "published_time": "4/4/2017", "tags": ["cs.CL"], "title": "Voice Conversion from Unaligned Corpora using Variational Autoencoding\n  Wasserstein Generative Adversarial Networks"}, {"abstract": "Automatic cell image segmentation methods in connectomics produce merge and\nsplit errors, which require correction through proofreading. Previous research\nhas identified the visual search for these errors as the bottleneck in\ninteractive proofreading. To aid error correction, we develop two classifiers\nthat automatically recommend candidate merges and splits to the user. These\nclassifiers use a convolutional neural network (CNN) that has been trained with\nerrors in automatic segmentations against expert-labeled ground truth. Our\nclassifiers detect potentially-erroneous regions by considering a large context\nregion around a segmentation boundary. Corrections can then be performed by a\nuser with yes/no decisions, which reduces variation of information 7.5x faster\nthan previous proofreading methods. We also present a fully-automatic mode that\nuses a probability threshold to make merge/split decisions. Extensive\nexperiments using the automatic approach and comparing performance of novice\nand expert users demonstrate that our method performs favorably against\nstate-of-the-art proofreading methods on different connectomics datasets.", "authors": ["Daniel Haehn", "Verena Kaynig", "James Tompkin", "Jeff W. Lichtman", "Hanspeter Pfister"], "category": "cs.CV", "comment": "Supplemental material available at\n  http://rhoana.org/guidedproofreading/supplemental.pdf", "img": "/static/thumbs/1704.00848v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00848v1", "num_discussion": 0, "originally_published_time": "4/4/2017", "pid": "1704.00848v1", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "Guided Proofreading of Automatic Segmentations for Connectomics"}, {"abstract": "This paper presents two visual trackers from the different paradigms of\nlearning and registration based tracking and evaluates their application in\nimage based visual servoing. They can track object motion with four degrees of\nfreedom (DoF) which, as we will show here, is sufficient for many fine\nmanipulation tasks. One of these trackers is a newly developed learning based\ntracker that relies on learning discriminative correlation filters while the\nother is a refinement of a recent 8 DoF RANSAC based tracker adapted with a new\nappearance model for tracking 4 DoF motion.\n  Both trackers are shown to provide superior performance to several state of\nthe art trackers on an existing dataset for manipulation tasks. Further, a new\ndataset with challenging sequences for fine manipulation tasks captured from\nrobot mounted eye-in-hand (EIH) cameras is also presented. These sequences have\na variety of challenges encountered during real tasks including jittery camera\nmovement, motion blur, drastic scale changes and partial occlusions.\nQuantitative and qualitative results on these sequences are used to show that\nthese two trackers are robust to failures while providing high precision that\nmakes them suitable for such fine manipulation tasks.", "authors": ["Mennatullah Siam", "Abhineet Singh", "Camilo Perez", "Martin Jagersand"], "category": "cs.CV", "comment": "accepted in CRV 2017", "img": "/static/thumbs/1703.01698v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01698v2", "num_discussion": 0, "originally_published_time": "3/6/2017", "pid": "1703.01698v2", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "4-DoF Tracking for Robot Fine Manipulation Tasks"}, {"abstract": "Deep learning has been demonstrated to achieve excellent results for image\nclassification and object detection. However, the impact of deep learning on\nvideo analysis (e.g. action detection and recognition) has been limited due to\ncomplexity of video data and lack of annotations. Previous convolutional neural\nnetworks (CNN) based video action detection approaches usually consist of two\nmajor steps: frame-level action proposal detection and association of proposals\nacross frames. Also, these methods employ two-stream CNN framework to handle\nspatial and temporal feature separately. In this paper, we propose an\nend-to-end deep network called Tube Convolutional Neural Network (T-CNN) for\naction detection in videos. The proposed architecture is a unified network that\nis able to recognize and localize action based on 3D convolution features. A\nvideo is first divided into equal length clips and for each clip a set of tube\nproposals are generated next based on 3D Convolutional Network (ConvNet)\nfeatures. Finally, the tube proposals of different clips are linked together\nemploying network flow and spatio-temporal action detection is performed using\nthese linked video proposals. Extensive experiments on several video datasets\ndemonstrate the superior performance of T-CNN for classifying and localizing\nactions in both trimmed and untrimmed videos compared to state-of-the-arts.", "authors": ["Rui Hou", "Chen Chen", "Mubarak Shah"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1703.10664v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10664v2", "num_discussion": 0, "originally_published_time": "3/30/2017", "pid": "1703.10664v2", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos"}, {"abstract": "Loyalty is an essential component of multi-community engagement. When users\nhave the choice to engage with a variety of different communities, they often\nbecome loyal to just one, focusing on that community at the expense of others.\nHowever, it is unclear how loyalty is manifested in user behavior, or whether\nloyalty is encouraged by certain community characteristics.\n  In this paper we operationalize loyalty as a user-community relation: users\nloyal to a community consistently prefer it over all others; loyal communities\nretain their loyal users over time. By exploring this relation using a large\ndataset of discussion communities from Reddit, we reveal that loyalty is\nmanifested in remarkably consistent behaviors across a wide spectrum of\ncommunities. Loyal users employ language that signals collective identity and\nengage with more esoteric, less popular content, indicating they may play a\ncurational role in surfacing new material. Loyal communities have denser\nuser-user interaction networks and lower rates of triadic closure, suggesting\nthat community-level loyalty is associated with more cohesive interactions and\nless fragmentation into subgroups. We exploit these general patterns to predict\nfuture rates of loyalty. Our results show that a user\u0027s propensity to become\nloyal is apparent from their first interactions with a community, suggesting\nthat some users are intrinsically loyal from the very beginning.", "authors": ["William L. Hamilton", "Justine Zhang", "Cristian Danescu-Niculescu-Mizil", "Dan Jurafsky", "Jure Leskovec"], "category": "cs.SI", "comment": "Extended version of a paper that will appear in the proceedings of\n  ICWSM 2017 (under the same titl...", "img": "/static/thumbs/1703.03386v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.03386v2", "num_discussion": 0, "originally_published_time": "3/9/2017", "pid": "1703.03386v2", "published_time": "4/4/2017", "tags": ["cs.SI", "cs.CL"], "title": "Loyalty in Online Communities"}, {"abstract": "We propose split-brain autoencoders, a straightforward modification of the\ntraditional autoencoder architecture, for unsupervised representation learning.\nThe method adds a split to the network, resulting in two disjoint sub-networks.\nEach sub-network is trained to perform a difficult task -- predicting one\nsubset of the data channels from another. Together, the sub-networks extract\nfeatures from the entire input signal. By forcing the network to solve\ncross-channel prediction tasks, we induce a representation within the network\nwhich transfers well to other, unseen tasks. This method achieves\nstate-of-the-art performance on several large-scale transfer learning\nbenchmarks.", "authors": ["Richard Zhang", "Phillip Isola", "Alexei A. Efros"], "category": "cs.CV", "comment": "Accepted to CVPR 2017", "img": "/static/thumbs/1611.09842v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.09842v2", "num_discussion": 0, "originally_published_time": "11/29/2016", "pid": "1611.09842v2", "published_time": "4/4/2017", "tags": ["cs.CV"], "title": "Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel\n  Prediction"}, {"abstract": "We propose the configurable rendering of massive quantities of photorealistic\nimages with ground truth for the purposes of training, benchmarking, and\ndiagnosing computer vision models. In contrast to the conventional\n(crowd-sourced) manual labeling of ground truth for a relatively modest number\nof RGB-D images captured by Kinect-like sensors, we devise a non-trivial\nconfigurable pipeline of algorithms capable of generating a potentially\ninfinite variety of indoor scenes using a stochastic grammar, specifically, one\nrepresented by an attributed spatial And-Or graph. We employ physics-based\nrendering to synthesize photorealistic RGB images while automatically\nsynthesizing detailed, per-pixel ground truth data, including visible surface\ndepth and normal, object identity and material information, as well as\nillumination. Our pipeline is configurable inasmuch as it enables the precise\ncustomization and control of important attributes of the generated scenes. We\ndemonstrate that our generated scenes achieve a performance similar to the NYU\nv2 Dataset on pre-trained deep learning models. By modifying pipeline\ncomponents in a controllable manner, we furthermore provide diagnostics on\ncommon scene understanding tasks; eg., depth and surface normal prediction,\nsemantic segmentation, etc.", "authors": ["Chenfanfu Jiang", "Yixin Zhu", "Siyuan Qi", "Siyuan Huang", "Jenny Lin", "Xingwen Guo", "Lap-Fai Yu", "Demetri Terzopoulos", "Song-Chun Zhu"], "category": "cs.CV", "comment": "12-page paper with 18-page supplementary file, 25 figures, submitted\n  to ICCV 2017. Chenfanfu Jiang...", "img": "/static/thumbs/1704.00112v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00112v2", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00112v2", "published_time": "4/4/2017", "tags": ["cs.CV", "stat.ML"], "title": "Configurable, Photorealistic Image Rendering and Ground Truth Synthesis\n  by Sampling Stochastic Grammars Representing Indoor Scenes"}, {"abstract": "This paper makes progress on several open theoretical issues related to\nGenerative Adversarial Networks. A definition is provided for what it means for\nthe training to generalize, and it is shown that generalization is not\nguaranteed for the popular distances between distributions such as\nJensen-Shannon or Wasserstein. We introduce a new metric called neural net\ndistance for which generalization does occur. We also show that an approximate\npure equilibrium in the 2-player game exists for a natural training objective\n(Wasserstein). Showing such a result has been an open problem (for any training\nobjective).\n  Finally, the above theoretical ideas lead us to propose a new training\nprotocol, MIX+GAN, which can be combined with any existing method. We present\nexperiments showing that it stabilizes and improves some existing methods.", "authors": ["Sanjeev Arora", "Rong Ge", "Yingyu Liang", "Tengyu Ma", "Yi Zhang"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1703.00573v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.00573v3", "num_discussion": 0, "originally_published_time": "3/2/2017", "pid": "1703.00573v3", "published_time": "4/4/2017", "tags": ["cs.LG"], "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)"}, {"abstract": "We introduce an algorithm for word-level text spotting that is able to\naccurately and reliably determine the bounding regions of individual words of\ntext \"in the wild\". Our system is formed by the cascade of two convolutional\nneural networks. The first network is fully convolutional and is in charge of\ndetecting areas containing text. This results in a very reliable but possibly\ninaccurate segmentation of the input image. The second network (inspired by the\npopular YOLO architecture) analyzes each segment produced in the first stage,\nand predicts oriented rectangular regions containing individual words. No\npost-processing (e.g. text line grouping) is necessary. With execution time of\n450 ms for a 1000-by-560 image on a Titan X GPU, our system achieves the\nhighest score to date among published algorithms on the ICDAR 2015 Incidental\nScene Text dataset benchmark.", "authors": ["Siyang Qin", "Roberto Manduchi"], "category": "cs.CV", "comment": "7 pages, 8 figures", "img": "/static/thumbs/1704.00834v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00834v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00834v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Cascaded Segmentation-Detection Networks for Word-Level Text Spotting"}, {"abstract": "Plausible reasoning concerns situations whose inherent lack of precision is\nnot quantified; that is, there are no degrees or levels of precision, and hence\nno use of numbers like probabilities. A hopefully comprehensive set of\nprinciples that clarifies what it means for a formal logic to do plausible\nreasoning is presented. A new propositional logic, called Propositional\nPlausible Logic (PPL), is defined and applied to some important examples. PPL\nis the only non-numeric non-monotonic logic we know of that satisfies all the\nprinciples and correctly reasons with all the examples. Some important results\nabout PPL are proved.", "authors": ["David Billington"], "category": "cs.AI", "comment": "58 pages. Updated n-die examples to n-lottery examples. Example 3.4\n  simplified. In Section 4 count...", "img": "/static/thumbs/1703.01697v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01697v2", "num_discussion": 0, "originally_published_time": "3/6/2017", "pid": "1703.01697v2", "published_time": "4/3/2017", "tags": ["cs.AI", "cs.LO", "I.2.4"], "title": "Principles and Examples of Plausible Reasoning and Propositional\n  Plausible Logic"}, {"abstract": "In this paper, we employ machine learning techniques to analyze seventeen\nseasons (1999-2000 to 2015-2016) of NBA regular season data from every team to\ndetermine the common characteristics among NBA playoff teams. Each team was\ncharacterized by 26 predictor variables and one binary response variable taking\non a value of \"TRUE\" if a team had made the playoffs, and value of \"FALSE\" if a\nteam had missed the playoffs. After fitting an initial classification tree to\nthis problem, this tree was then pruned which decreased the test error rate.\nFurther to this, a random forest of classification trees was grown which\nprovided a very accurate model from which a variable importance plot was\ngenerated to determine which predictor variables had the greatest influence on\nthe response variable. The result of this work was the conclusion that the most\nimportant factors in characterizing a team\u0027s playoff eligibility are a team\u0027s\nopponent number of assists per game, a team\u0027s opponent number of made two point\nshots per game, and a team\u0027s number of steals per game. This seems to suggest\nthat defensive factors as opposed to offensive factors are the most important\ncharacteristics shared among NBA playoff teams. We then use neural networks to\nclassify championship teams based on regular season data. From this, we show\nthat the most important factor in a team not winning a championship is that\nteam\u0027s opponent number of made three-point shots per game. This once again\nimplies that defensive characteristics are of great importance in not only\ndetermining a team\u0027s playoff eligibility, but certainly, one can conclude that\na lack of perimeter defense negatively impacts a team\u0027s championship chances in\na given season. Further, it is shown that made two-point shots and defensive\nrebounding are by far the most important factor in a team\u0027s chances at winning\na championship in a given season.", "authors": ["Ikjyot Singh Kohli"], "category": "stat.ML", "comment": "Updated contents to reflect most recent data and corrected typos", "img": "/static/thumbs/1604.05266v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.05266v7", "num_discussion": 0, "originally_published_time": "4/18/2016", "pid": "1604.05266v7", "published_time": "4/3/2017", "tags": ["stat.ML", "stat.AP"], "title": "Finding Common Characteristics Among NBA Playoff and Championship Teams:\n  A Machine Learning Approach"}, {"abstract": "Deforestation detection using satellite images can make an important\ncontribution to forest management. Current approaches can be broadly divided\ninto those that compare two images taken at similar periods of the year and\nthose that monitor changes by using multiple images taken during the growing\nseason. The CMFDA algorithm described in Zhu et al. (2012) is an algorithm that\nbuilds on the latter category by implementing a year-long, continuous,\ntime-series based approach to monitoring images. This algorithm was developed\nfor 30m resolution, 16-day frequency reflectance data from the Landsat\nsatellite. In this work we adapt the algorithm to 1km, 16-day frequency\nreflectance data from the modis sensor aboard the Terra satellite. The CMFDA\nalgorithm is composed of two submodels which are fitted on a pixel-by-pixel\nbasis. The first estimates the amount of surface reflectance as a function of\nthe day of the year. The second estimates the occurrence of a deforestation\nevent by comparing the last few predicted and real reflectance values. For this\ncomparison, the reflectance observations for six different bands are first\ncombined into a forest index. Real and predicted values of the forest index are\nthen compared and high absolute differences for consecutive observation dates\nare flagged as deforestation events. Our adapted algorithm also uses the two\nmodel framework. However, since the modis 13A2 dataset used, includes\nreflectance data for different spectral bands than those included in the\nLandsat dataset, we cannot construct the forest index. Instead we propose two\ncontrasting approaches: a multivariate and an index approach similar to that of\nCMFDA.", "authors": ["Emiliano Diaz"], "category": "stat.AP", "comment": "", "img": "/static/thumbs/1704.00829v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00829v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00829v1", "published_time": "4/3/2017", "tags": ["stat.AP", "cs.CV"], "title": "Online deforestation detection"}, {"abstract": "We extend Stochastic Gradient Variational Bayes to perform posterior\ninference for the weights of Stick-Breaking processes. This development allows\nus to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian\nnonparametric version of the variational autoencoder that has a latent\nrepresentation with stochastic dimensionality. We experimentally demonstrate\nthat the SB-VAE, and a semi-supervised variant, learn highly discriminative\nlatent representations that often outperform the Gaussian VAE\u0027s.", "authors": ["Eric Nalisnick", "Padhraic Smyth"], "category": "stat.ML", "comment": "ICLR 2017, Conference Track", "img": "/static/thumbs/1605.06197v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06197v3", "num_discussion": 0, "originally_published_time": "5/20/2016", "pid": "1605.06197v3", "published_time": "4/3/2017", "tags": ["stat.ML"], "title": "Stick-Breaking Variational Autoencoders"}, {"abstract": "Traditional Linear Genetic Programming (LGP) algorithms are based only on the\nselection mechanism to guide the search. Genetic operators combine or mutate\nrandom portions of the individuals, without knowing if the result will lead to\na fitter individual. Probabilistic Model Building Genetic Programming (PMB-GP)\nmethods were proposed to overcome this issue through a probability model that\ncaptures the structure of the fit individuals and use it to sample new\nindividuals. This work proposes the use of LGP with a Stochastic Context-Free\nGrammar (SCFG), that has a probability distribution that is updated according\nto selected individuals. We proposed a method for adapting the grammar into the\nlinear representation of LGP. Tests performed with the proposed probabilistic\nmethod, and with two hybrid approaches, on several symbolic regression\nbenchmark problems show that the results are statistically better than the\nobtained by the traditional LGP.", "authors": ["L\u00e9o Fran\u00e7oso Dal Piccol Sotto", "Vin\u00edcius Veloso de Melo"], "category": "cs.NE", "comment": "Genetic and Evolutionary Computation Conference (GECCO) 2017, Berlin,\n  Germany", "img": "/static/thumbs/1704.00828v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00828v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00828v1", "published_time": "4/3/2017", "tags": ["cs.NE", "math.PR", "stat.ML"], "title": "A Probabilistic Linear Genetic Programming with Stochastic Context-Free\n  Grammar for solving Symbolic Regression problems"}, {"abstract": "This report summarizes the tutorial presented by the author at NIPS 2016 on\ngenerative adversarial networks (GANs). The tutorial describes: (1) Why\ngenerative modeling is a topic worth studying, (2) how generative models work,\nand how GANs compare to other generative models, (3) the details of how GANs\nwork, (4) research frontiers in GANs, and (5) state-of-the-art image models\nthat combine GANs with other methods. Finally, the tutorial contains three\nexercises for readers to complete, and the solutions to these exercises.", "authors": ["Ian Goodfellow"], "category": "cs.LG", "comment": "v2-v4 are all typo fixes. No substantive changes relative to v1", "img": "/static/thumbs/1701.00160v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.00160v4", "num_discussion": 0, "originally_published_time": "12/31/2016", "pid": "1701.00160v4", "published_time": "4/3/2017", "tags": ["cs.LG"], "title": "NIPS 2016 Tutorial: Generative Adversarial Networks"}, {"abstract": "The majority of existing solutions to the Multi-Target Tracking (MTT) problem\ndo not combine cues in a coherent end-to-end fashion over a long period of\ntime. However, we present an online method that encodes long-term temporal\ndependencies across multiple cues. One key challenge of tracking methods is to\naccurately track occluded targets or those which share similar appearance\nproperties with surrounding objects. To address this challenge, we present a\nstructure of Recurrent Neural Networks (RNN) that jointly reasons on multiple\ncues over a temporal window. We are able to correct many data association\nerrors and recover observations from an occluded state. We demonstrate the\nrobustness of our data-driven approach by tracking multiple targets using their\nappearance, motion, and even interactions. Our method outperforms previous\nworks on multiple publicly available datasets including the challenging MOT\nbenchmark.", "authors": ["Amir Sadeghian", "Alexandre Alahi", "Silvio Savarese"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1701.01909v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.01909v2", "num_discussion": 1, "originally_published_time": "1/8/2017", "pid": "1701.01909v2", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term\n  Dependencies"}, {"abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn\ngenerative models mapping from simple latent distributions to arbitrarily\ncomplex data distributions has been demonstrated empirically, with compelling\nresults showing that the latent space of such generators captures semantic\nvariation in the data distribution. Intuitively, models trained to predict\nthese semantic latent representations given data may serve as useful feature\nrepresentations for auxiliary problems where semantics are relevant. However,\nin their existing form, GANs have no means of learning the inverse mapping --\nprojecting data back into the latent space. We propose Bidirectional Generative\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\ndemonstrate that the resulting learned feature representation is useful for\nauxiliary supervised discrimination tasks, competitive with contemporary\napproaches to unsupervised and self-supervised feature learning.", "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "category": "cs.LG", "comment": "Published as a conference paper at ICLR 2017. Changelog: (v7) Table 2\n  results improved 1-2% due to...", "img": "/static/thumbs/1605.09782v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.09782v7", "num_discussion": 0, "originally_published_time": "5/31/2016", "pid": "1605.09782v7", "published_time": "4/3/2017", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "title": "Adversarial Feature Learning"}, {"abstract": "In this paper, the idea of a new artificial intelligence based optimization\nalgorithm, which is inspired from the nature of vortex, has been provided\nbriefly. As also a bio-inspired computation algorithm, the idea is generally\nfocused on a typical vortex flow / behavior in nature and inspires from some\ndynamics that are occurred in the sense of vortex nature. Briefly, the\nalgorithm is also a swarm-oriented evolutional problem solution approach;\nbecause it includes many methods related to elimination of weak swarm members\nand trying to improve the solution process by supporting the solution space via\nnew swarm members. In order have better idea about success of the algorithm; it\nhas been tested via some benchmark functions. At this point, the obtained\nresults show that the algorithm can be an alternative to the literature in\nterms of single-objective optimization solution ways. Vortex Optimization\nAlgorithm (VOA) is the name suggestion by the authors; for this new idea of\nintelligent optimization approach.", "authors": ["Utku Kose", "Ahmet Arslan"], "category": "cs.AI", "comment": "7 pages, 1 figure, 2 tables", "img": "/static/thumbs/1704.00797v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00797v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00797v1", "published_time": "4/3/2017", "tags": ["cs.AI", "math.OC"], "title": "On the idea of a new artificial intelligence based optimization\n  algorithm inspired from the nature of vortex"}, {"abstract": "This paper introduce a software system including widely-used Swarm\nIntelligence algorithms or approaches to be used for the related scientific\nresearch studies associated with the subject area. The programmatic\ninfrastructure of the system allows working on a fast, easy-to-use, interactive\nplatform to perform Swarm Intelligence based studies in a more effective,\nefficient and accurate way. In this sense, the system employs all of the\nnecessary controls for the algorithms and it ensures an interactive platform on\nwhich computer users can perform studies on a wide spectrum of solution\napproaches associated with simple and also more advanced problems.", "authors": ["Utku Kose"], "category": "cs.AI", "comment": "6 pages, 2 figures, 1 table", "img": "/static/thumbs/1704.00795v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00795v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00795v1", "published_time": "4/3/2017", "tags": ["cs.AI", "cs.SE"], "title": "Design and development of a software system for swarm intelligence based\n  research studies"}, {"abstract": "Similarity-based approaches represent a promising direction for time series\nanalysis. However, many such methods rely on parameter tuning and have\nshortcomings if the time series are multivariate (MTS) and contain missing\ndata. In this paper, we address these challenges within the powerful context of\nkernel methods by proposing the robust \\emph{time series cluster kernel} (TCK).\nThe approach taken is to leverage the missing data handling properties of\nGaussian mixture models (GMM) augmented with informative prior distributions.\nAn ensemble learning approach is exploited to ensure robustness to parameters\nby combining the clustering results of many GMM to form the final kernel.\n  We evaluate the TCK on synthetic and real data and compare to other\nstate-of-the-art techniques. The experimental results demonstrate that the TCK\nis robust to parameter choices, provides competitive results for MTS without\nmissing data and outstanding results for missing data.", "authors": ["Karl \u00d8yvind Mikalsen", "Filippo Maria Bianchi", "Cristina Soguero-Ruiz", "Robert Jenssen"], "category": "stat.ML", "comment": "22 pages, 6 figures", "img": "/static/thumbs/1704.00794v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00794v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00794v1", "published_time": "4/3/2017", "tags": ["stat.ML", "cs.LG"], "title": "Time Series Cluster Kernel for Learning Similarities between\n  Multivariate Time Series with Missing Data"}, {"abstract": "Recurrent neural network models with an attention mechanism have proven to be\nextremely effective on a wide variety of sequence-to-sequence problems.\nHowever, the fact that soft attention mechanisms perform a pass over the entire\ninput sequence when producing each element in the output sequence precludes\ntheir use in online settings and results in a quadratic time complexity. Based\non the insight that the alignment between input and output sequence elements is\nmonotonic in many problems of interest, we propose an end-to-end differentiable\nmethod for learning monotonic alignments which, at test time, enables computing\nattention online and in linear time. We validate our approach on sentence\nsummarization, machine translation, and online speech recognition problems and\nachieve results competitive with existing sequence-to-sequence models.", "authors": ["Colin Raffel", "Thang Luong", "Peter J. Liu", "Ron J. Weiss", "Douglas Eck"], "category": "cs.LG", "comment": "19 pages, 5 figures (three full-page), including 4 pages of\n  supplementary material", "img": "/static/thumbs/1704.00784v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00784v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00784v1", "published_time": "4/3/2017", "tags": ["cs.LG", "cs.CL"], "title": "Online and Linear-Time Attention by Enforcing Monotonic Alignments"}, {"abstract": "I make some basic observations about hard takeoff, value alignment, and\ncoherent extrapolated volition, concepts which have been central in analyses of\nsuperintelligent AI systems.", "authors": ["Gopal P. Sarma"], "category": "cs.AI", "comment": "3 pages", "img": "/static/thumbs/1704.00783v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00783v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00783v1", "published_time": "4/3/2017", "tags": ["cs.AI", "cs.CY", "cs.LG"], "title": "Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated\n  Volition"}, {"abstract": "Increasing the capacity of recurrent neural networks (RNN) usually involves\naugmenting the size of the hidden layer, resulting in a significant increase of\ncomputational cost. An alternative is the recurrent neural tensor network\n(RNTN), which increases capacity by employing distinct hidden layer weights for\neach vocabulary word. The disadvantage of RNTNs is that memory usage scales\nlinearly with vocabulary size, which can reach millions for word-level language\nmodels. In this paper, we introduce restricted recurrent neural tensor networks\n(r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary\nwords while sharing a single set of weights for infrequent words. Perplexity\nevaluations using the Penn Treebank corpus show that r-RNTNs improve language\nmodel performance over standard RNNs using only a small fraction of the\nparameters of unrestricted RNTNs.", "authors": ["Alexandre Salle", "Aline Villavicencio"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.00774v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00774v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00774v1", "published_time": "4/3/2017", "tags": ["cs.CL"], "title": "Restricted Recurrent Neural Tensor Networks"}, {"abstract": "We provide a comparative study of several widely used off-policy estimators\n(Empirical Average, Basic Importance Sampling and Normalized Importance\nSampling), detailing the different regimes where they are individually\nsuboptimal. We then exhibit properties optimal estimators should possess. In\nthe case where examples have been gathered using multiple policies, we show\nthat fused estimators dominate basic ones but can still be improved.", "authors": ["Thomas Nedelec", "Nicolas Le Roux", "Vianney Perchet"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1704.00773v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00773v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00773v1", "published_time": "4/3/2017", "tags": ["stat.ML", "cs.LG"], "title": "A comparative study of counterfactual estimators"}, {"abstract": "The Support Vector Machine (SVM) is a powerful and widely used classification\nalgorithm. Its performance is well known to be impacted by a tuning parameter\nwhich is frequently selected by cross-validation. This paper uses the\nKarush-Kuhn-Tucker conditions to provide rigorous mathematical proof for new\ninsights into the behavior of SVM in the large and small tuning parameter\nregimes. These insights provide perhaps unexpected relationships between SVM\nand naive Bayes and maximal data piling directions. We explore how\ncharacteristics of the training data affect the behavior of SVM in many cases\nincluding: balanced vs. unbalanced classes, low vs. high dimension, separable\nvs. non-separable data. These results present a simple explanation of SVM\u0027s\nbehavior as a function of the tuning parameter. We also elaborate on the\ngeometry of complete data piling directions in high dimensional space. The\nresults proved in this paper suggest important implications for tuning SVM with\ncross-validation.", "authors": ["Iain Carmichael", "J. S. Marron"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1704.00767v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00767v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00767v1", "published_time": "4/3/2017", "tags": ["stat.ML", "cs.LG"], "title": "Geometric Insights into Support Vector Machine Behavior using the KKT\n  Conditions"}, {"abstract": "The convolutional neural network (CNN), which is one of the deep learning\nmodels, has seen much success in a variety of computer vision tasks. However,\ndesigning CNN architectures still requires expert knowledge and a lot of trial\nand error. In this paper, we attempt to automatically construct CNN\narchitectures for an image classification task based on Cartesian genetic\nprogramming (CGP). In our method, we adopt highly functional modules, such as\nconvolutional blocks and tensor concatenation, as the node functions in CGP.\nThe CNN structure and connectivity represented by the CGP encoding method are\noptimized to maximize the validation accuracy. To evaluate the proposed method,\nwe constructed a CNN architecture for the image classification task with the\nCIFAR-10 dataset. The experimental result shows that the proposed method can be\nused to automatically find the competitive CNN architecture compared with\nstate-of-the-art models.", "authors": ["Masanori Suganuma", "Shinichi Shirakawa", "Tomoharu Nagao"], "category": "cs.NE", "comment": "This paper has been accepted to GECCO 2017. This is the submitted\n  version on February 6, 2017", "img": "/static/thumbs/1704.00764v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00764v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00764v1", "published_time": "4/3/2017", "tags": ["cs.NE"], "title": "A Genetic Programming Approach to Designing Convolutional Neural Network\n  Architectures"}, {"abstract": "Given a user\u0027s query, traditional image search systems rank images according\nto its relevance to a single modality (e.g., image content or surrounding\ntext). Nowadays, an increasing number of images on the Internet are available\nwith associated meta data in rich modalities (e.g., titles, keywords, tags,\netc.), which can be exploited for better similarity measure with queries. In\nthis paper, we leverage visual and textual modalities for image search by\nlearning their correlation with input query. According to the intent of query,\nattention mechanism can be introduced to adaptively balance the importance of\ndifferent modalities. We propose a novel Attention guided Multi-modal\nCorrelation (AMC) learning method which consists of a jointly learned hierarchy\nof intra and inter-attention networks. Conditioned on query\u0027s intent,\nintra-attention networks (i.e., visual intra-attention network and language\nintra-attention network) attend on informative parts within each modality; a\nmulti-modal inter-attention network promotes the importance of the most\nquery-relevant modalities. In experiments, we evaluate AMC models on the search\nlogs from two real world image search engines and show a significant boost on\nthe ranking of user-clicked images in search results. Additionally, we extend\nAMC models to caption ranking task on COCO dataset and achieve competitive\nresults compared with recent state-of-the-arts.", "authors": ["Kan Chen", "Trung Bui", "Fang Chen", "Zhaowen Wang", "Ram Nevatia"], "category": "cs.CV", "comment": "CVPR 2017", "img": "/static/thumbs/1704.00763v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00763v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00763v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "AMC: Attention guided Multi-modal Correlation Learning for Image Search"}, {"abstract": "Recently, action proposal methods have played an important role in action\nrecognition tasks, as they reduce the search space dramatically. Most\nunsupervised action proposal methods tend to generate hundreds of action\nproposals which include many noisy, inconsistent, and unranked action\nproposals, while supervised action proposal methods take advantage of\npredefined object detectors (e.g., human detector) to refine and score the\naction proposals, but they require thousands of manual annotations to train.\n  Given the action proposals in a video, the goal of the proposed work is to\ngenerate a few better action proposals that are ranked properly. In our\napproach, we first divide action proposal into sub-proposal and then use\nDynamic Programming based graph optimization scheme to select the optimal\ncombinations of sub-proposals from different proposals and assign each new\nproposal a score. We propose a new unsupervised image-based actioness detector\nthat leverages web images and employs it as one of the node scores in our graph\nformulation. Moreover, we capture motion information by estimating the number\nof motion contours within each action proposal patch. The proposed method is an\nunsupervised method that neither needs bounding box annotations nor video level\nlabels, which is desirable with the current explosion of large-scale action\ndatasets. Our approach is generic and does not depend on a specific action\nproposal method. We evaluate our approach on several publicly available trimmed\nand un-trimmed datasets and obtain better performance compared to several\nproposal ranking methods. In addition, we demonstrate that properly ranked\nproposals produce significantly better action detection as compared to\nstate-of-the-art proposal based methods.", "authors": ["Waqas Sultani", "Dong Zhang", "Mubarak Shah"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00758v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00758v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00758v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Unsupervised Action Proposal Ranking through Proposal Recombination"}, {"abstract": "This article deals with a novel branch of Separation of Concerns, called\nMulti-Advisor Reinforcement Learning (MAd-RL), where a single-agent RL problem\nis distributed to $n$ learners, called advisors. Each advisor tries to solve\nthe problem with a different focus. Their advice is then communicated to an\naggregator, which is in control of the system. For the local training, three\noff-policy bootstrapping methods are proposed and analysed: local-max\nbootstraps with the local greedy action, rand-policy bootstraps with respect to\nthe random policy, and agg-policy bootstraps with respect to the aggregator\u0027s\ngreedy policy. MAd-RL is positioned as a generalisation of Reinforcement\nLearning with Ensemble methods. An experiment is held on a simplified version\nof the Ms. Pac-Man Atari game. The results confirm the theoretical relative\nstrengths and weaknesses of each method.", "authors": ["Romain Laroche", "Mehdi Fatemi", "Joshua Romoff", "Harm van Seijen"], "category": "cs.LG", "comment": "Submitted at UAI2017", "img": "/static/thumbs/1704.00756v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00756v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00756v1", "published_time": "4/3/2017", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "Multi-Advisor Reinforcement Learning"}, {"abstract": "Developments in semantic web technologies have promoted ontological encoding\nof knowledge from diverse domains. However, modelling many practical domains\nrequires more expressive representations schemes than what the standard\ndescription logics(DLs) support. We extend the DL SROIQ with constraint\nnetworks and grounded circumscription. Applications of constraint modelling\ninclude embedding ontologies with temporal or spatial information, while\ngrounded circumscription allows defeasible inference and closed world\nreasoning. This paper overcomes restrictions on existing constraint modelling\napproaches by introducing expressive constructs. Grounded circumscription\nallows concept and role minimization and is decidable for DL. We provide a\ngeneral and intuitive algorithm for the framework of grounded circumscription\nthat can be applied to a whole range of logics. We present the resulting logic:\nGC-SROIQ(C), and describe a tableau decision procedure for it.", "authors": ["Arjun Bhardwaj", "Sangeetha"], "category": "cs.AI", "comment": "For an improved formulation of the problem, which addresses critical\n  shortcomings of this paper, p...", "img": "/static/thumbs/1411.0406v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1411.0406v4", "num_discussion": 0, "originally_published_time": "11/3/2014", "pid": "1411.0406v4", "published_time": "4/3/2017", "tags": ["cs.AI"], "title": "GC-SROIQ(C) : Expressive Constraint Modelling and Grounded\n  Circumscription for SROIQ"}, {"abstract": "Theory of Mind is the ability to attribute mental states (beliefs, intents,\nknowledge, perspectives, etc.) to others and recognize that these mental states\nmay differ from one\u0027s own. Theory of Mind is critical to effective\ncommunication and to teams demonstrating higher collective performance. To\neffectively leverage the progress in Artificial Intelligence (AI) to make our\nlives more productive, it is important for humans and AI to work well together\nin a team. Traditionally, there has been much emphasis on research to make AI\nmore accurate, and (to a lesser extent) on having it better understand human\nintentions, tendencies, beliefs, and contexts. The latter involves making AI\nmore human-like and having it develop a theory of our minds.\n  In this work, we argue that for human-AI teams to be effective, humans must\nalso develop a theory of AI\u0027s mind - get to know its strengths, weaknesses,\nbeliefs, and quirks. We instantiate these ideas within the domain of Visual\nQuestion Answering (VQA). We find that using just a few examples(50), lay\npeople can be trained to better predict responses and oncoming failures of a\ncomplex VQA model. Surprisingly, we find that having access to the model\u0027s\ninternal states - its confidence in its top-k predictions, explicit or implicit\nattention maps which highlight regions in the image (and words in the question)\nthe model is looking at (and listening to) while answering a question about an\nimage - do not help people better predict its behavior", "authors": ["Arjun Chandrasekaran", "Deshraj Yadav", "Prithvijit Chattopadhyay", "Viraj Prabhu", "Devi Parikh"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00717v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00717v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00717v1", "published_time": "4/3/2017", "tags": ["cs.CV", "cs.AI", "cs.CL"], "title": "It Takes Two to Tango: Towards Theory of AI\u0027s Mind"}, {"abstract": "Recently, Convolutional Neural Networks have shown promising results for 3D\ngeometry prediction. They can make predictions from very little input data such\nas for example a single color image, depth map or a partial 3D volume. A major\nlimitation of such approaches is that they only predict a coarse resolution\nvoxel grid, which does not capture the surface of the objects well. We propose\na general framework, called hierarchical surface prediction (HSP), which\nfacilitates prediction of high resolution voxel grids. The main insight is that\nit is sufficient to predict high resolution voxels around the predicted\nsurfaces. The exterior and interior of the objects can be represented with\ncoarse resolution voxels. This allows us to predict significantly higher\nresolution voxel grids around the surface, from which triangle meshes can be\nextracted. Our approach is general and not dependent on a specific input type.\nIn our experiments, we show results for geometry prediction from color images,\ndepth images and shape completion from partial voxel grids. Our analysis shows\nthat the network is able to predict the surface more accurately than a low\nresolution prediction.", "authors": ["Christian H\u00e4ne", "Shubham Tulsiani", "Jitendra Malik"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00710v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00710v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00710v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Hierarchical Surface Prediction for 3D Object Reconstruction"}, {"abstract": "In this paper we develop a new framework that captures the common landscape\nunderlying the common non-convex low-rank matrix problems including matrix\nsensing, matrix completion and robust PCA. In particular, we show for all above\nproblems (including asymmetric cases): 1) all local minima are also globally\noptimal; 2) no high-order saddle points exists. These results explain why\nsimple algorithms such as stochastic gradient descent have global converge, and\nefficiently optimize these non-convex objective functions in practice. Our\nframework connects and simplifies the existing analyses on optimization\nlandscapes for matrix sensing and symmetric matrix completion. The framework\nnaturally leads to new results for asymmetric matrix completion and robust PCA.", "authors": ["Rong Ge", "Chi Jin", "Yi Zheng"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.00708v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00708v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00708v1", "published_time": "4/3/2017", "tags": ["cs.LG", "math.OC", "stat.ML"], "title": "No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified\n  Geometric Analysis"}, {"abstract": "Tissue segmentation is an important pre-requisite for efficient and accurate\ndiagnostics in digital pathology. However, it is well known that whole-slide\nscanners can fail in detecting all tissue regions, for example due to the\ntissue type, or due to weak staining because their tissue detection algorithms\nare not robust enough. In this paper, we introduce two different convolutional\nneural network architectures for whole slide image segmentation to accurately\nidentify the tissue sections. We also compare the algorithms to a published\ntraditional method. We collected 54 whole slide images with differing stains\nand tissue types from three laboratories to validate our algorithms. We show\nthat while the two methods do not differ significantly they outperform their\ntraditional counterpart (Jaccard index of 0.937 and 0.929 vs. 0.870, p \u003c 0.01).", "authors": ["P\u00e9ter B\u00e1ndi", "Rob van de Loo", "Milad Intezar", "Daan Geijs", "Francesco Ciompi", "Bram van Ginneken", "Jeroen van der Laak", "Geert Litjens"], "category": "cs.CV", "comment": "Accepted for poster presentation at the IEEE International Symposium\n  on Biomedical Imaging (ISBI) ...", "img": "/static/thumbs/1703.05990v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.05990v2", "num_discussion": 0, "originally_published_time": "3/17/2017", "pid": "1703.05990v2", "published_time": "4/3/2017", "tags": ["cs.CV", "cs.LG"], "title": "Comparison of Different Methods for Tissue Segmentation in\n  Histopathological Whole-Slide Images"}, {"abstract": "Graphs are widely used to model execution dependencies in applications. In\nparticular, the NP-complete problem of partitioning a graph under constraints\nreceives enormous attention by researchers because of its applicability in\nmultiprocessor scheduling. We identified the additional constraint of acyclic\ndependencies between blocks when mapping computer vision and imaging\napplications to a heterogeneous embedded multiprocessor. Existing algorithms\nand heuristics do not address this requirement and deliver results that are not\napplicable for our use-case. In this work, we show that this more constrained\nversion of the graph partitioning problem is NP-complete and present heuristics\nthat achieve a close approximation of the optimal solution found by an\nexhaustive search for small problem instances and much better scalability for\nlarger instances. In addition, we can show a positive impact on the schedule of\na real imaging application that improves communication volume and execution\ntime.", "authors": ["Orlando Moreira", "Merten Popp", "Christian Schulz"], "category": "cs.DS", "comment": "", "img": "/static/thumbs/1704.00705v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00705v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00705v1", "published_time": "4/3/2017", "tags": ["cs.DS", "cs.CV", "cs.DC"], "title": "Graph Partitioning with Acyclicity Constraints"}, {"abstract": "The design of spacecraft trajectories for missions visiting multiple\ncelestial bodies is here framed as a multi-objective bilevel optimization\nproblem. A comparative study is performed to assess the performance of\ndifferent Beam Search algorithms at tackling the combinatorial problem of\nfinding the ideal sequence of bodies. Special focus is placed on the\ndevelopment of a new hybridization between Beam Search and the Population-based\nAnt Colony Optimization algorithm. An experimental evaluation shows all\nalgorithms achieving exceptional performance on a hard benchmark problem. It is\nfound that a properly tuned deterministic Beam Search always outperforms the\nremaining variants. Beam P-ACO, however, demonstrates lower parameter\nsensitivity, while offering superior worst-case performance. Being an anytime\nalgorithm, it is then found to be the preferable choice for certain practical\napplications.", "authors": ["Lu\u00eds F. Sim\u00f5es", "Dario Izzo", "Evert Haasdijk", "A. E. Eiben"], "category": "cs.NE", "comment": "Code available at https://github.com/lfsimoes/beam_paco__gtoc5", "img": "/static/thumbs/1704.00702v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00702v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00702v1", "published_time": "4/3/2017", "tags": ["cs.NE", "physics.space-ph", "I.2.8"], "title": "Multi-rendezvous Spacecraft Trajectory Optimization with Beam P-ACO"}, {"abstract": "Using unitary (instead of general) matrices in artificial neural networks\n(ANNs) is a promising way to solve the gradient explosion/vanishing problem, as\nwell as to enable ANNs to learn long-term correlations in the data. This\napproach appears particularly promising for Recurrent Neural Networks (RNNs).\nIn this work, we present a new architecture for implementing an Efficient\nUnitary Neural Network (EUNNs); its main advantages can be summarized as\nfollows. Firstly, the representation capacity of the unitary space in an EUNN\nis fully tunable, ranging from a subspace of SU(N) to the entire unitary space.\nSecondly, the computational complexity for training an EUNN is merely\n$\\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on\nthe standard copying task, the pixel-permuted MNIST digit recognition benchmark\nas well as the Speech Prediction Test (TIMIT). We find that our architecture\nsignificantly outperforms both other state-of-the-art unitary RNNs and the LSTM\narchitecture, in terms of the final performance and/or the wall-clock training\nspeed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide\nvariety of applications.", "authors": ["Li Jing", "Yichen Shen", "Tena Dub\u010dek", "John Peurifoy", "Scott Skirlo", "Yann LeCun", "Max Tegmark", "Marin Solja\u010di\u0107"], "category": "cs.LG", "comment": "9 pages, 4 figures", "img": "/static/thumbs/1612.05231v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.05231v3", "num_discussion": 0, "originally_published_time": "12/15/2016", "pid": "1612.05231v3", "published_time": "4/3/2017", "tags": ["cs.LG", "cs.NE", "stat.ML"], "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application\n  to RNNs"}, {"abstract": "Co-localization is the problem of localizing objects of the same class using\nonly the set of images that contain them. This is a challenging task because\nthe object detector must be built without negative examples that can lead to\nmore informative supervision signals. The main idea of our method is to cluster\nthe feature space of a generically pre-trained CNN, to find a set of CNN\nfeatures that are consistently and highly activated for an object category,\nwhich we call category-consistent CNN features. Then, we propagate their\ncombined activation map using superpixel geodesic distances for\nco-localization. In our first set of experiments, we show that the proposed\nmethod achieves state-of-the-art performance on three related benchmarks:\nPASCAL 2007, PASCAL-2012, and the Object Discovery dataset. We also show that\nour method is able to detect and localize truly unseen categories, on six\nheld-out ImageNet categories with accuracy that is significantly higher than\nprevious state-of-the-art. Our intuitive approach achieves this success without\nany region proposals or object detectors, and can be based on a CNN that was\npre-trained purely on image classification tasks without further fine-tuning.", "authors": ["Hieu Le", "Chen-Ping Yu", "Gregory Zelinsky", "Dimitris Samaras"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.03236v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.03236v2", "num_discussion": 0, "originally_published_time": "12/10/2016", "pid": "1612.03236v2", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Co-localization with Category-Consistent CNN Features and Geodesic\n  Distance Propagation"}, {"abstract": "We present the 2017 DAVIS Challenge, a public competition specifically\ndesigned for the task of video object segmentation. Following the footsteps of\nother successful initiatives, such as ILSVRC and PASCAL VOC, which established\nthe avenue of research in the fields of scene classification and semantic\nsegmentation, the DAVIS Challenge comprises a dataset, an evaluation\nmethodology, and a public competition with a dedicated workshop co-located with\nCVPR 2017. The DAVIS Challenge follows up on the recent publication of DAVIS\n(Densely-Annotated VIdeo Segmentation), which has fostered the development of\nseveral novel state-of-the-art video object segmentation techniques. In this\npaper we describe the scope of the benchmark, highlight the main\ncharacteristics of the dataset and define the evaluation metrics of the\ncompetition.", "authors": ["Jordi Pont-Tuset", "Federico Perazzi", "Sergi Caelles", "Pablo Arbel\u00e1ez", "Alex Sorkine-Hornung", "Luc Van Gool"], "category": "cs.CV", "comment": "Challenge website: http://davischallenge.org", "img": "/static/thumbs/1704.00675v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00675v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00675v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "The 2017 DAVIS Challenge on Video Object Segmentation"}, {"abstract": "We proposed a deep learning method for interpretable diabetic retinopathy\n(DR) detection. The visual-interpretable feature of the proposed method is\nachieved by adding the regression activation map (RAM) after the global\naveraging pooling layer of the convolutional networks (CNN). With RAM, the\nproposed model can localize the discriminative regions of an retina image to\nshow the specific region of interest in terms of its severity level. We believe\nthis advantage of the proposed deep learning model is highly desired for DR\ndetection because in practice, users are not only interested with high\nprediction performance, but also keen to understand the insights of DR\ndetection and why the adopted learning model works. In the experiments\nconducted on a large scale of retina image dataset, we show that the proposed\nCNN model can achieve high performance on DR detection compared with the\nstate-of-the-art while achieving the merits of providing the RAM to highlight\nthe salient regions of the input image.", "authors": ["Zhiguang Wang", "Jianbo Yang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1703.10757v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10757v2", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1703.10757v2", "published_time": "4/3/2017", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "Diabetic Retinopathy Detection via Deep Convolutional Networks for\n  Discriminative Localization and Visual Explanation"}, {"abstract": "Despite the increasing use of social media platforms for information and news\ngathering, its unmoderated nature often leads to the emergence and spread of\nrumours, i.e. pieces of information that are unverified at the time of posting.\nAt the same time, the openness of social media platforms provides opportunities\nto study how users share and discuss rumours, and to explore how natural\nlanguage processing and data mining techniques may be used to find ways of\ndetermining their veracity. In this survey we introduce and discuss two types\nof rumours that circulate on social media; long-standing rumours that circulate\nfor long periods of time, and newly-emerging rumours spawned during fast-paced\nevents such as breaking news, where reports are released piecemeal and often\nwith an unverified status in their early stages. We provide an overview of\nresearch into social media rumours with the ultimate goal of developing a\nrumour classification system that consists of four components: rumour\ndetection, rumour tracking, rumour stance classification and rumour veracity\nclassification. We delve into the approaches presented in the scientific\nliterature for the development of each of these four components. We summarise\nthe efforts and achievements so far towards the development of rumour\nclassification systems and conclude with suggestions for avenues for future\nresearch in social media mining for detection and resolution of rumours.", "authors": ["Arkaitz Zubiaga", "Ahmet Aker", "Kalina Bontcheva", "Maria Liakata", "Rob Procter"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.00656v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00656v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00656v1", "published_time": "4/3/2017", "tags": ["cs.CL", "cs.HC", "cs.IR", "cs.SI"], "title": "Detection and Resolution of Rumours in Social Media: A Survey"}, {"abstract": "In this work we present a new approach to learn compressible representations\nin deep architectures with an end-to-end training strategy. Our method is based\non a soft relaxation of quantization and entropy, which we anneal to their\ndiscrete counterparts throughout training. We showcase this method for two\nchallenging applications: Image compression and neural network compression.\nWhile these tasks have typically been approached with different methods, our\nsoft-to-hard quantization approach gives state-of-the-art results for both.", "authors": ["Eirikur Agustsson", "Fabian Mentzer", "Michael Tschannen", "Lukas Cavigelli", "Radu Timofte", "Luca Benini", "Luc Van Gool"], "category": "cs.LG", "comment": "Supplementary visual examples available at:\n  http://www.vision.ee.ethz.ch/~aeirikur/compression/vis...", "img": "/static/thumbs/1704.00648v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00648v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00648v1", "published_time": "4/3/2017", "tags": ["cs.LG", "cs.CV"], "title": "Soft-to-Hard Vector Quantization for End-to-End Learned Compression of\n  Images and Neural Networks"}, {"abstract": "Much has been learned about plasticity of biological synapses from empirical\nstudies. Hebbian plasticity is driven by correlated activity of presynaptic and\npostsynaptic neurons. Synapses that converge onto the same neuron often behave\nas if they compete for a fixed resource; some survive the competition while\nothers are eliminated. To provide computational interpretations of these\naspects of synaptic plasticity, we formulate unsupervised learning as a\nzero-sum game between Hebbian excitation and anti-Hebbian inhibition in a\nneural network model. The game formalizes the intuition that Hebbian excitation\ntries to maximize correlations of neurons with their inputs, while anti-Hebbian\ninhibition tries to decorrelate neurons from each other. We further include a\nmodel of synaptic competition, which enables a neuron to eliminate all\nconnections except those from its most strongly correlated inputs. Through\nempirical studies, we show that this facilitates the learning of sensory\nfeatures that resemble parts of objects.", "authors": ["H. Sebastian Seung", "Jonathan Zung"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1704.00646v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00646v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00646v1", "published_time": "4/3/2017", "tags": ["cs.NE", "q-bio.NC"], "title": "A correlation game for unsupervised learning yields computational\n  interpretations of Hebbian excitation, anti-Hebbian inhibition, and synapse\n  elimination"}, {"abstract": "We derive a new asymptotic expansion for the global excess risk of a local\n$k$-nearest neighbour classifier, where the choice of $k$ may depend upon the\ntest point. This expansion elucidates conditions under which the dominant\ncontribution to the excess risk comes from the locus of points at which each\nclass label is equally likely to occur, but we also show that if these\nconditions are not satisfied, the dominant contribution may arise from the\ntails of the marginal distribution of the features. Moreover, we prove that,\nprovided the $d$-dimensional marginal distribution of the features has a finite\n$\\rho$th moment for some $\\rho \u003e 4$ (as well as other regularity conditions), a\nlocal choice of $k$ can yield a rate of convergence of the excess risk of\n$O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard\n$k$-nearest neighbour classifier, our theory would require $d \\geq 5$ and $\\rho\n\u003e 4d/(d-4)$ finite moments to achieve this rate. Our results motivate a new\n$k$-nearest neighbour classifier for semi-supervised learning problems, where\nthe unlabelled data are used to obtain an estimate of the marginal feature\ndensity, and fewer neighbours are used for classification when this density\nestimate is small. The potential improvements over the standard $k$-nearest\nneighbour classifier are illustrated both through our theory and via a\nsimulation study.", "authors": ["Timothy I. Cannings", "Thomas B. Berrett", "Richard J. Samworth"], "category": "math.ST", "comment": "42 pages", "img": "/static/thumbs/1704.00642v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00642v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00642v1", "published_time": "4/3/2017", "tags": ["math.ST", "cs.CV", "stat.ME", "stat.TH", "62G20"], "title": "Local nearest neighbour classification with applications to\n  semi-supervised learning"}, {"abstract": "Deep generative models trained with large amounts of unlabelled data have\nproven to be powerful within the domain of unsupervised learning. Many real\nlife data sets contain a small amount of labelled data points, that are\ntypically disregarded when training generative models. We propose the\nCluster-aware Generative Model, that uses unlabelled information to infer a\nlatent representation that models the natural clustering of the data, and\nadditional labelled data points to refine this clustering. The generative\nperformances of the model significantly improve when labelled information is\nexploited, obtaining a log-likelihood of -79.38 nats on permutation invariant\nMNIST, while also achieving competitive semi-supervised classification\naccuracies. The model can also be trained fully unsupervised, and still improve\nthe log-likelihood performance with respect to related methods.", "authors": ["Lars Maal\u00f8e", "Marco Fraccaro", "Ole Winther"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1704.00637v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00637v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00637v1", "published_time": "4/3/2017", "tags": ["stat.ML", "cs.AI", "cs.LG"], "title": "Semi-Supervised Generation with Cluster-aware Generative Models"}, {"abstract": "General human action recognition requires understanding of various visual\ncues. In this paper, we propose a network architecture that computes and\nintegrates the most important visual cues for action recognition: pose, motion,\nand the raw images. For the integration, we introduce a Markov chain model\nwhich adds cues successively. The resulting approach is efficient and\napplicable to action classification as well as to spatial and temporal action\nlocalization. The two contributions clearly improve the performance over\nrespective baselines. The overall approach achieves state-of-the-art action\nclassification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover,\nit yields state-of-the-art spatio-temporal action localization results on\nUCF101 and J-HMDB.", "authors": ["Mohammadreza Zolfaghari", "Gabriel L. Oliveira", "Nima Sedaghat", "Thomas Brox"], "category": "cs.CV", "comment": "10 pages, 7 figures, ICCV 2017 submission", "img": "/static/thumbs/1704.00616v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00616v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00616v1", "published_time": "4/3/2017", "tags": ["cs.CV", "cs.AI", "cs.HC", "cs.MM", "cs.NE"], "title": "Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance\n  for Action Classification and Detection"}, {"abstract": "Our article presents an audio-visual based multi-modal emotion classification\nsystem. Considering the fact of deep learning approaches to facial analysis\nhave recently demonstrated high performance, in our work, we use convolutional\nneural networks (CNNs) for emotion recognition in video, relying on temporal\naveraging and pooling operations reminiscent of widely used approaches for the\nspatial aggregation of information. In respect of time sequence, we extract the\nfeature from audio clips in the video and use RNN to propagate information. In\nthis work, we focus our presentation and experimental analysis on a fusion\nCNN-RNN architecture for facial expression analysis.", "authors": ["Lijie Fan", "Yunjie Ke"], "category": "cs.CV", "comment": "8 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1608.00859, arXiv:1503.08909 by oth...", "img": "/static/thumbs/1704.00570v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00570v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00570v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Spatiotemporal Networks for Video Emotion Recognition"}, {"abstract": "The input to a neural sequence-to-sequence model is often determined by an\nup-stream system, e.g. a word segmenter, part of speech tagger, or speech\nrecognizer. These up-stream models are potentially error-prone. Representing\ninputs through word lattices allows making this uncertainty explicit by\ncapturing alternative sequences and their posterior probabilities in a compact\nform. In this work, we extend the TreeLSTM (Tai et al., 2015) into a\nLatticeLSTM that is able to consume word lattices, and can be used as encoder\nin an attentional encoder-decoder model. We integrate lattice posterior scores\ninto this architecture by extending the TreeLSTM\u0027s child-sum and forget gates\nand introducing a bias term into the attention mechanism. We experiment with\nspeech translation lattices and report consistent improvements over baselines\nthat translate either the 1-best hypothesis or the lattice without posterior\nscores.", "authors": ["Matthias Sperber", "Graham Neubig", "Jan Niehues", "Alex Waibel"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.00559v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00559v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00559v1", "published_time": "4/3/2017", "tags": ["cs.CL"], "title": "Neural Lattice-to-Sequence Models for Uncertain Inputs"}, {"abstract": "In this work, a novel learning-based approach has been developed to generate\ndriving paths by integrating LIDAR point clouds, GPS-IMU information, and\nGoogle driving directions. The system is based on a fully convolutional neural\nnetwork that jointly learns to carry out perception and path generation from\nreal-world driving sequences and that is trained using automatically generated\ntraining examples. Several combinations of input data were tested in order to\nassess the performance gain provided by specific information modalities. The\nfully convolutional neural network trained using all the available sensors\ntogether with driving directions achieved the best MaxF score of 88.13% when\nconsidering a region of interest of 60x60 meters. By considering a smaller\nregion of interest, the agreement between predicted paths and ground-truth\nincreased to 92.60%. The positive results obtained in this work indicate that\nthe proposed system may help fill the gap between low-level scene parsing and\nbehavior-reflex approaches by generating outputs that are close to vehicle\ncontrol and at the same time human-interpretable.", "authors": ["Luca Caltagirone", "Mauro Bellone", "Lennart Svensson", "Mattias Wahde"], "category": "cs.CV", "comment": "Changed title, formerly \"Simultaneous Perception and Path Generation\n  Using Fully Convolutional Neu...", "img": "/static/thumbs/1703.08987v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.08987v2", "num_discussion": 0, "originally_published_time": "3/27/2017", "pid": "1703.08987v2", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "LIDAR-based Driving Path Generation Using Fully Convolutional Neural\n  Networks"}, {"abstract": "Dynamic Mode Decomposition (DMD) has emerged as a powerful tool for analyzing\nthe dynamics of non-linear systems from experimental datasets. Recently,\nseveral attempts have extended DMD to the context of low-rank approximations.\nThis extension is of particular interest for reduced-order modeling in various\napplicative domains, e.g. for climate prediction, to study molecular dynamics\nor micro-electromechanical devices. This low-rank extension takes the form of a\nnonconvex optimization problem. To the best of our knowledge, only sub-optimal\nalgorithms have been proposed in the literature to compute the solution of this\nproblem. In this paper, we prove that there exists a closed-form optimal\nsolution to this problem and design an effective algorithm to compute it based\non Singular Value Decomposition (SVD). Based on this solution, we then propose\nefficient procedures for reduced-order modeling and for the identification of\nthe the low-rank DMD modes and amplitudes. Experiments illustrates the gain in\nperformance of the proposed algorithm compared to state-of-the-art techniques.", "authors": ["Patrick H\u00e9as", "C\u00e9dric Herzet"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1610.02962v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.02962v3", "num_discussion": 0, "originally_published_time": "10/10/2016", "pid": "1610.02962v3", "published_time": "4/3/2017", "tags": ["stat.ML", "cs.NA"], "title": "Low-rank Approximation and Dynamic Mode Decomposition"}, {"abstract": "To ensure interpretability of extracted sources in tensor decomposition, we\nintroduce in this paper a dictionary-based tensor canonical polyadic\ndecomposition which enforces one factor to belong exactly to a known\ndictionary. A new formulation of sparse coding is proposed which enables high\ndimensional tensors dictionary-based canonical polyadic decomposition. The\nbenefits of using a dictionary in tensor decomposition models are explored both\nin terms of parameter identifiability and estimation accuracy. This is\nillustrated on the decomposition of simulated data and on the unmixing of\nhyperspectral images.", "authors": ["J\u00e9r\u00e9my E. Cohen", "Nicolas Gillis"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1704.00541v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00541v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00541v1", "published_time": "4/3/2017", "tags": ["stat.ML"], "title": "Dictionary-based Tensor Canonical Polyadic Decomposition"}, {"abstract": "For their ability to capture non-linearities in the data and to scale to\nlarge training sets, local Support Vector Machines (SVMs) have received a\nspecial attention during the past decade. In this paper, we introduce a new\nlocal SVM method, called L$^3$-SVMs, which clusters the input space, carries\nout dimensionality reduction by projecting the data on landmarks, and jointly\nlearns a linear combination of local models. Simple and effective, our\nalgorithm is also theoretically well-founded. Using the framework of Uniform\nStability, we show that our SVM formulation comes with generalization\nguarantees on the true risk. The experiments based on the simplest\nconfiguration of our model (i.e. landmarks randomly selected, linear\nprojection, linear kernel) show that L$^3$-SVMs is very competitive w.r.t. the\nstate of the art and opens the door to new exciting lines of research.", "authors": ["Valentina Zantedeschi", "R\u00e9mi Emonet", "Marc Sebban"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1703.00284v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.00284v2", "num_discussion": 0, "originally_published_time": "3/1/2017", "pid": "1703.00284v2", "published_time": "4/3/2017", "tags": ["stat.ML", "cs.LG"], "title": "L$^3$-SVMs: Landmarks-based Linear Local Support Vectors Machines"}, {"abstract": "This paper proposes a new extrinsic calibration of kaleidoscopic imaging\nsystem by estimating normals and distances of the mirrors. The problem to be\nsolved in this paper is a simultaneous estimation of all mirror parameters\nconsistent throughout multiple reflections. Unlike conventional methods\nutilizing a pair of direct and mirrored images of a reference 3D object to\nestimate the parameters on a per-mirror basis, our method renders the\nsimultaneous estimation problem into solving a linear set of equations. The key\ncontribution of this paper is to introduce a linear estimation of multiple\nmirror parameters from kaleidoscopic 2D projections of a single 3D point of\nunknown geometry. Evaluations with synthesized and real images demonstrate the\nperformance of the proposed algorithm in comparison with conventional methods.", "authors": ["Kosuke Takahashi", "Akihiro Miyata", "Shohei Nobuhara", "Takashi Matsuyama"], "category": "cs.CV", "comment": "to appear in CVPR 2017", "img": "/static/thumbs/1703.02826v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.02826v2", "num_discussion": 0, "originally_published_time": "3/8/2017", "pid": "1703.02826v2", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "A Linear Extrinsic Calibration of Kaleidoscopic Imaging System from\n  Single 3D Point"}, {"abstract": "In this paper, we propose a novel and elegant solution to \"Multi-Source\nNeural Machine Translation\" (MSNMT) which only relies on preprocessing a N-way\nmultilingual corpus without modifying the Neural Machine Translation (NMT)\narchitecture or training procedure. We simply concatenate the source sentences\nto form a single long multi-source input sentence while keeping the target side\nsentence as it is and train an NMT system using this preprocessed corpus. We\nevaluate our method in resource poor as well as resource rich settings and show\nits effectiveness (up to 4 BLEU using 2 source languages and up to 6 BLEU using\n5 source languages) by comparing against existing methods for MSNMT. We also\nprovide some insights on how the NMT system leverages multilingual information\nin such a scenario by visualizing attention.", "authors": ["Raj Dabre", "Fabien Cromieres", "Sadao Kurohashi"], "category": "cs.CL", "comment": "Added results for IWSLT corpus setting along with some typo\n  corrections, additional references and...", "img": "/static/thumbs/1702.06135v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.06135v3", "num_discussion": 0, "originally_published_time": "2/20/2017", "pid": "1702.06135v3", "published_time": "4/3/2017", "tags": ["cs.CL"], "title": "Enabling Multi-Source Neural Machine Translation By Concatenating Source\n  Sentences In Multiple Languages"}, {"abstract": "Recent advances have enabled 3d object reconstruction approaches using a\nsingle off-the-shelf RGB-D camera. Although these approaches are successful for\na wide range of object classes, they rely on stable and distinctive geometric\nor texture features. Many objects like mechanical parts, toys, household or\ndecorative articles, however, are textureless and characterized by minimalistic\nshapes that are simple and symmetric. Existing in-hand scanning systems and 3d\nreconstruction techniques fail for such symmetric objects in the absence of\nhighly distinctive features. In this work, we show that extracting 3d hand\nmotion for in-hand scanning effectively facilitates the reconstruction of even\nfeatureless and highly symmetric objects and we present an approach that fuses\nthe rich additional information of hands into a 3d reconstruction pipeline,\nsignificantly contributing to the state-of-the-art of in-hand scanning.", "authors": ["Dimitrios Tzionas", "Juergen Gall"], "category": "cs.CV", "comment": "International Conference on Computer Vision (ICCV) 2015,\n  http://files.is.tue.mpg.de/dtzionas/In-Ha...", "img": "/static/thumbs/1704.00529v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00529v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00529v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "3D Object Reconstruction from Hand-Object Interactions"}, {"abstract": "A deep neural network model is a powerful framework for learning\nrepresentations. Usually, it is used to learn the relation x to y by exploiting\nthe regularities in the input x but without considering the output\nrepresentation y. In structured output prediction problems, where the output is\nmulti-dimensional and where structural relations exist between the dimensions,\nthe network usually tends to overfit when the training data are limited. In\norder to overcome this issue and circumvent the large required data to output\naccurate predictions, we propose in this paper a regularization scheme for\ntraining neural networks for these particular tasks. Our proposed scheme aims\nat incorporating the learning of the output representation y in the training\nprocess while learning the mapping function x to y. Our proposition is a\nmulti-task framework containing two unsupervised tasks over the input and the\noutput data along with the supervised task. We experimented the use of the\noutput labels y without their corresponding input x.\n  We evaluate our framework on a facial landmark detection problem which is a\ntypical structured output prediction task. We show over two public challenging\ndatasets (LFPW and HELEN) that our regularization scheme improves the\ngeneralization of deep neural networks and accelerates their training. The use\nof unlabeled data is also explored, showing an additional improvement of the\nresults. We provide an opensource implementation\nhttps://github.com/sbelharbi/structured-output-ae of our framework.", "authors": ["Soufiane Belharbi", "Clement Chatelain", "Romain Herault", "Sebastien Adam"], "category": "cs.LG", "comment": "Submitted to Neurocomputing", "img": "/static/thumbs/1504.07550v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1504.07550v5", "num_discussion": 0, "originally_published_time": "4/28/2015", "pid": "1504.07550v5", "published_time": "4/3/2017", "tags": ["cs.LG", "stat.ML"], "title": "Multi-task Learning for Structured Output Prediction"}, {"abstract": "Predicting the target of visual search from eye fixation (gaze) data is a\nchallenging problem with many applications in human-computer interaction. In\ncontrast to previous work that has focused on individual instances as a search\ntarget, we propose the first approach to predict categories and attributes of\nsearch targets based on gaze data. However, state of the art models for\ncategorical recognition, in general, require large amounts of training data,\nwhich is prohibitive for gaze data. To address this challenge, we propose a\nnovel Gaze Pooling Layer that integrates gaze information into CNN-based\narchitectures as an attention mechanism - incorporating both spatial and\ntemporal aspects of human gaze behavior. We show that our approach is effective\neven when the gaze pooling layer is added to an already trained CNN, thus\neliminating the need for expensive joint data collection of visual and gaze\ndata. We propose an experimental setup and data set and demonstrate the\neffectiveness of our method for search target prediction based on gaze\nbehavior. We further study how to integrate temporal and spatial gaze\ninformation most effectively, and indicate directions for future research in\nthe gaze-based prediction of mental states.", "authors": ["Hosnieh Sattar", "Andreas Bulling", "Mario Fritz"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.10162v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.10162v3", "num_discussion": 0, "originally_published_time": "11/27/2016", "pid": "1611.10162v3", "published_time": "4/3/2017", "tags": ["cs.CV", "cs.HC", "q-bio.NC"], "title": "Predicting the Category and Attributes of Visual Search Targets Using\n  Deep Gaze Pooling"}, {"abstract": "There are two main streams in up-to-date image denoising algorithms:\nnon-local self similarity (NSS) prior based methods and convolutional neural\nnetwork (CNN) based methods. The NSS based methods are favorable on images with\nregular and repetitive patterns while the CNN based methods perform better on\nirregular structures. In this paper, we propose a block-matching convolutional\nneural network (BMCNN) method that combines NSS prior and CNN. Initially,\nsimilar local patches in the input image are integrated into a 3D block. In\norder to prevent the noise from messing up the block matching, we first apply\nan existing denoising algorithm on the noisy image. The denoised image is\nemployed as a pilot signal for the block matching, and then denoising function\nfor the block is learned by a CNN structure. Experimental results show that the\nproposed BMCNN algorithm achieves state-of-the-art performance. In detail,\nBMCNN can restore both repetitive and irregular structures.", "authors": ["Byeongyong Ahn", "Nam Ik Cho"], "category": "cs.CV", "comment": "11 pages, 9 figures", "img": "/static/thumbs/1704.00524v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00524v1", "num_discussion": 1, "originally_published_time": "4/3/2017", "pid": "1704.00524v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Block-Matching Convolutional Neural Network for Image Denoising"}, {"abstract": "Approximate Bayesian computation (ABC) is a method for Bayesian inference\nwhen the likelihood is unavailable but simulating from the model is possible.\nHowever, many ABC algorithms require a large number of simulations, which can\nbe costly. To reduce the computational cost, surrogate models and Bayesian\noptimisation (BO) have been proposed. Bayesian optimisation enables one to\nintelligently decide where to evaluate the model next, but standard BO\nstrategies are designed for optimisation and not specifically for ABC\ninference. Our paper addresses this gap in the literature. We propose a new\nacquisition rule that selects the next evaluation where the uncertainty in the\nposterior distribution is largest. Experiments show that the proposed method\noften produces the most accurate approximations, especially in high-dimensional\ncases or in the presence of strong prior information, compared to common\nalternatives.", "authors": ["Marko J\u00e4rvenp\u00e4\u00e4", "Michael U. Gutmann", "Aki Vehtari", "Pekka Marttinen"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1704.00520v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00520v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00520v1", "published_time": "4/3/2017", "tags": ["stat.ML", "stat.ME"], "title": "Efficient acquisition rules for model-based approximate Bayesian\n  computation"}, {"abstract": "The severity of sustained injury resulting from assault-related violence can\nbe minimised by reducing detection time. However, it has been shown that human\noperators perform poorly at detecting events found in video footage when\npresented with simultaneous feeds. We utilise computer vision techniques to\ndevelop an automated method of abnormal crowd detection that can aid a human\noperator in the detection of violent behaviour. We observed that behaviour in\ncity centre environments often occur in crowded areas, resulting in individual\nactions being occluded by other crowd members. We propose a real-time\ndescriptor that models crowd dynamics by encoding changes in crowd texture\nusing temporal summaries of Grey Level Co-Occurrence Matrix (GLCM) features. We\nintroduce a measure of inter-frame uniformity (IFU) and demonstrate that the\nappearance of violent behaviour changes in a less uniform manner when compared\nto other types of crowd behaviour. Our proposed method is computationally cheap\nand offers real-time description. Evaluating our method using a privately held\nCCTV dataset and the publicly available Violent Flows, UCF Web Abnormality, and\nUMN Abnormal Crowd datasets, we report a receiver operating characteristic\nscore of 0.9782, 0.9403, 0.8218 and 0.9956 respectively.", "authors": ["Kaelon Lloyd", "David Marshall", "Simon C. Moore", "Paul L. Rosin"], "category": "cs.CV", "comment": "Published under open access, 9 pages, 12 Figures", "img": "/static/thumbs/1605.05106v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.05106v2", "num_discussion": 0, "originally_published_time": "5/17/2016", "pid": "1605.05106v2", "published_time": "4/3/2017", "tags": ["cs.CV", "I.2.10; I.4.7; I.4.8"], "title": "Detecting Violent and Abnormal Crowd activity using Temporal Analysis of\n  Grey Level Co-occurrence Matrix (GLCM) Based Texture Measures"}, {"abstract": "Hand motion capture has been an active research topic in recent years,\nfollowing the success of full-body pose tracking. Despite similarities, hand\ntracking proves to be more challenging, characterized by a higher\ndimensionality, severe occlusions and self-similarity between fingers. For this\nreason, most approaches rely on strong assumptions, like hands in isolation or\nexpensive multi-camera systems, that limit the practical use. In this work, we\npropose a framework for hand tracking that can capture the motion of two\ninteracting hands using only a single, inexpensive RGB-D camera. Our approach\ncombines a generative model with collision detection and discriminatively\nlearned salient points. We quantitatively evaluate our approach on 14 new\nsequences with challenging interactions.", "authors": ["Dimitrios Tzionas", "Abhilash Srikantha", "Pablo Aponte", "Juergen Gall"], "category": "cs.CV", "comment": "German Conference on Pattern Recognition (GCPR) 2014,\n  http://files.is.tue.mpg.de/dtzionas/GCPR_201...", "img": "/static/thumbs/1704.00515v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00515v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00515v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Capturing Hand Motion with an RGB-D Sensor, Fusing a Generative Model\n  with Salient Points"}, {"abstract": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases\nin scientific articles and labelling them with respect to predefined types.\nAlthough important in practice, this task is so far underexplored, partly due\nto the lack of labelled data. To overcome this, we explore several auxiliary\ntasks, including semantic super-sense tagging and identification of multi-word\nexpressions, and cast the task as a multi-task learning problem with deep\nrecurrent neural networks. Our multi-task models perform significantly better\nthan previous state of the art approaches on two scientific KBC datasets,\nparticularly for long keyphrases.", "authors": ["Isabelle Augenstein", "Anders S\u00f8gaard"], "category": "cs.CL", "comment": "ACL 2017", "img": "/static/thumbs/1704.00514v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00514v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00514v1", "published_time": "4/3/2017", "tags": ["cs.CL", "cs.AI", "stat.ML"], "title": "Multi-Task Learning of Keyphrase Boundary Classification"}, {"abstract": "Recent study shows that a wide deep network can obtain accuracy comparable to\na deeper but narrower network. Compared to narrower and deeper networks, wide\nnetworks employ relatively less number of layers and have various important\nbenefits, such that they have less running time on parallel computing devices,\nand they are less affected by gradient vanishing problems. However, the\nparameter size of a wide network can be very large due to use of large width of\neach layer in the network. In order to keep the benefits of wide networks\nmeanwhile improve the parameter size and accuracy trade-off of wide networks,\nwe propose a binary tree architecture to truncate architecture of wide networks\nby reducing the width of the networks. More precisely, in the proposed\narchitecture, the width is continuously reduced from lower layers to higher\nlayers in order to increase the expressive capacity of network with a less\nincrease on parameter size. Also, to ease the gradient vanishing problem,\nfeatures obtained at different layers are concatenated to form the output of\nour architecture. By employing the proposed architecture on a baseline wide\nnetwork, we can construct and train a new network with same depth but\nconsiderably less number of parameters. In our experimental analyses, we\nobserve that the proposed architecture enables us to obtain better parameter\nsize and accuracy trade-off compared to baseline networks using various\nbenchmark image classification datasets. The results show that our model can\ndecrease the classification error of baseline from 20.43% to 19.22% on\nCifar-100 using only 28% of parameters that baseline has. Code is available at\nhttps://github.com/ZhangVision/bitnet.", "authors": ["Yan Zhang", "Mete Ozay", "Shuohao Li", "Takayuki Okatani"], "category": "cs.CV", "comment": "10 pages", "img": "/static/thumbs/1704.00509v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00509v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00509v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Truncating Wide Networks using Binary Tree Architectures"}, {"abstract": "We compare a set of convolutional neural network (CNN) architectures for the\ntask of segmenting and detecting human sperm cells in an image taken from a\nsemen sample. In contrast to previous work, samples are not stained or washed\nto allow for full sperm quality analysis, making analysis harder due to\nclutter. Our results indicate that training on full images is superior to\ntraining on patches when class-skew is properly handled. Full image training\nincluding up-sampling during training proves to be beneficial in deep CNNs for\npixel wise accuracy and detection performance. Predicted sperm cells are found\nby using connected components on the CNN predictions. We investigate\noptimization of a threshold parameter on the size of detected components. Our\nbest network achieves 93.87% precision and 91.89% recall on our test dataset\nafter thresholding outperforming a classical mage analysis approach.", "authors": ["Malte St\u00e6r Nissen", "Oswin Krause", "Kristian Almstrup", "S\u00f8ren Kj\u00e6rulff", "Torben Trindk\u00e6r Nielsen", "Mads Nielsen"], "category": "cs.CV", "comment": "Submitted for Scandinavian Conference on Image Analysis 2017", "img": "/static/thumbs/1704.00498v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00498v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00498v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Convolutional neural networks for segmentation and object detection of\n  human semen"}, {"abstract": "Benchmarking methods for 3d hand tracking is still an open problem due to the\ndifficulty of acquiring ground truth data. We introduce a new dataset and\nbenchmarking protocol that is insensitive to the accumulative error of other\nprotocols. To this end, we create testing frame pairs of increasing difficulty\nand measure the pose estimation error separately for each of them. This\napproach gives new insights and allows to accurately study the performance of\neach feature or method without employing a full tracking pipeline. Following\nthis protocol, we evaluate various directional distances in the context of\nsilhouette-based 3d hand tracking, expressed as special cases of a generalized\nChamfer distance form. An appropriate parameter setup is proposed for each of\nthem, and a comparative study reveals the best performing method in this\ncontext.", "authors": ["Dimitrios Tzionas", "Juergen Gall"], "category": "cs.CV", "comment": "German Conference on Pattern Recognition (GCPR) 2013,\n  http://files.is.tue.mpg.de/dtzionas/GCPR_201...", "img": "/static/thumbs/1704.00492v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00492v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00492v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "A Comparison of Directional Distances for Hand Pose Estimation"}, {"abstract": "Many datasets have multiple tables connected by key-foreign key dependencies.\nData scientists usually join all tables to bring in extra features from the\nso-called dimension tables. Unlike the statistical relational learning setting,\nsuch joins do not cause record duplications, which means regular IID models are\ntypically used. Recent work demonstrated the possibility of using foreign key\nfeatures as representatives for the dimension tables\u0027 features and eliminating\nthe latter a priori, potentially saving runtime and effort of data scientists.\nHowever, the prior work was restricted to linear models and it established a\ndichotomy of when dimension tables are safe to discard due to extra overfitting\ncaused by the use of foreign key features. In this work, we revisit that\nquestion for two popular high capacity models: decision tree and SVM with RBF\nkernel. Our extensive empirical and simulation-based analyses show that these\ntwo classifiers are surprisingly and counter-intuitively more robust to\ndiscarding dimension tables and face much less extra overfitting than linear\nmodels. We provide intuitive explanations for their behavior and identify new\nopen questions for further ML theoretical research. We also identify and\nresolve two key practical bottlenecks in using foreign key features.", "authors": ["Vraj Shah", "Arun Kumar", "Xiaojin Zhu"], "category": "cs.DB", "comment": "10 pages", "img": "/static/thumbs/1704.00485v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00485v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00485v1", "published_time": "4/3/2017", "tags": ["cs.DB", "cs.LG"], "title": "Stop That Join! Discarding Dimension Tables when Learning High Capacity\n  Classifiers"}, {"abstract": "Generative adversarial nets (GANs) are good at generating realistic images\nand have been extended for semi-supervised classification. However, under a\ntwo-player formulation, existing work shares competing roles of identifying\nfake samples and predicting labels via a single discriminator network, which\ncan lead to undesirable incompatibility. We present triple generative\nadversarial net (Triple-GAN), a flexible game-theoretical framework for\nclassification and class-conditional generation in semi-supervised learning.\nTriple-GAN consists of three players - a generator, a discriminator and a\nclassifier, where the generator and classifier characterize the conditional\ndistributions between images and labels, and the discriminator solely focuses\non identifying fake image-label pairs. With designed utilities, the\ndistributions characterized by the classifier and generator both concentrate to\nthe data distribution under nonparametric assumptions. We further propose\nunbiased regularization terms to make the classifier and generator strongly\ncoupled and some biased techniques to boost the performance of Triple-GAN in\npractice. Our results on several datasets demonstrate the promise in\nsemi-supervised learning, where Triple-GAN achieves comparable or superior\nperformance than state-of-the-art classification results among DGMs; it is also\nable to disentangle the classes and styles and transfer smoothly on the data\nlevel via interpolation on the latent space class-conditionally.", "authors": ["Chongxuan Li", "Kun Xu", "Jun Zhu", "Bo Zhang"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1703.02291v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.02291v2", "num_discussion": 0, "originally_published_time": "3/7/2017", "pid": "1703.02291v2", "published_time": "4/3/2017", "tags": ["cs.LG", "cs.CV"], "title": "Triple Generative Adversarial Nets"}, {"abstract": "We present a multilinear statistical model of the human tongue that captures\nanatomical and tongue pose related shape variations separately. The model is\nderived from 3D magnetic resonance imaging data of 11 speakers sustaining\nspeech related vocal tract configurations. The extraction is performed by using\na minimally supervised method that uses as basis an image segmentation approach\nand a template fitting technique. Furthermore, it uses image denoising to deal\nwith possibly corrupt data, palate surface information reconstruction to handle\npalatal tongue contacts, and a bootstrap strategy to refine the obtained\nshapes. Our evaluation concludes that limiting the degrees of freedom for the\nanatomical and speech related variations to 5 and 4, respectively, produces a\nmodel that can reliably register unknown data while avoiding overfitting\neffects. Furthermore, we show that it can be used to generate a plausible\ntongue animation by tracking sparse motion capture data.", "authors": ["Alexander Hewer", "Stefanie Wuhrer", "Ingmar Steiner", "Korin Richmond"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.05005v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.05005v2", "num_discussion": 0, "originally_published_time": "12/15/2016", "pid": "1612.05005v2", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "A Multilinear Tongue Model Derived from Speech Related MRI Data of the\n  Human Vocal Tract"}, {"abstract": "We illustrate the potential in statistics and machine learning of the\nChristoffel function, or more precisely, its empirical counterpart associated\nwith a counting measure uniformly supported on a finite set of points. Firstly,\nwe provide a thresholding scheme which allows to approximate the support of a\nmeasure from a finite subset of its moments with strong asymptotic guaranties.\nSecondly, we provide a consistency result which relates the empirical\nChristoffel function and its population counterpart in the limit of large\nsamples. Finally, we illustrate the relevance of our results on simulated and\nreal world datasets for several applications in statistics and machine\nlearning: (a) density and support estimation from finite samples, (b) outlier\nand novelty detection and (c) affine matching.", "authors": ["Jean-Bernard Lasserre", "Edouard Pauwels"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1701.02886v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.02886v2", "num_discussion": 0, "originally_published_time": "1/11/2017", "pid": "1701.02886v2", "published_time": "4/3/2017", "tags": ["cs.LG"], "title": "The empirical Christoffel function in Statistics and Machine Learning"}, {"abstract": "We implemented several multilabel classification algorithms in the machine\nlearning package mlr. The implemented methods are binary relevance, classifier\nchains, nested stacking, dependent binary relevance and stacking, which can be\nused with any base learner that is accessible in mlr. Moreover, there is access\nto the multilabel classification versions of randomForestSRC and rFerns. All\nthese methods can be easily compared by different implemented multilabel\nperformance measures and resampling methods in the standardized mlr framework.\nIn a benchmark experiment with several multilabel datasets, the performance of\nthe different methods is evaluated.", "authors": ["Philipp Probst", "Quay Au", "Giuseppe Casalicchio", "Clemens Stachl", "Bernd Bischl"], "category": "stat.ML", "comment": "18 pages, 2 figures, to be published in R Journal; reference\n  corrected", "img": "/static/thumbs/1703.08991v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.08991v2", "num_discussion": 0, "originally_published_time": "3/27/2017", "pid": "1703.08991v2", "published_time": "4/3/2017", "tags": ["stat.ML"], "title": "Multilabel Classification with R Package mlr"}, {"abstract": "To appear in the proceedings of LPAR 21.\n  Solving complex problems can involve non-trivial combinations of distinct\nknowledge bases and problem solvers. The Algebra of Modular Systems is a\nknowledge representation framework that provides a method for formally\nspecifying such systems in purely semantic terms. Formally, an expression of\nthe algebra defines a class of structures. Many expressive formalism used in\npractice solve the model expansion task, where a structure is given on the\ninput and an expansion of this structure in the defined class of structures is\nsearched (this practice overcomes the common undecidability problem for\nexpressive logics). In this paper, we construct a solver for the model\nexpansion task for a complex modular systems from an expression in the algebra\nand black-box propagators or solvers for the primitive modules. To this end, we\ndefine a general notion of propagators equipped with an explanation mechanism,\nan extension of the alge- bra to propagators, and a lazy conflict-driven\nlearning algorithm. The result is a framework for seamlessly combining solving\ntechnology from different domains to produce a solver for a combined system.", "authors": ["Bart Bogaerts", "Eugenia Ternovska", "David Mitchell"], "category": "cs.AI", "comment": "To appear in the proceedings of LPAR 21", "img": "/static/thumbs/1606.08130v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.08130v2", "num_discussion": 0, "originally_published_time": "6/27/2016", "pid": "1606.08130v2", "published_time": "4/3/2017", "tags": ["cs.AI", "cs.LO"], "title": "Propagators and Solvers for the Algebra of Modular Systems"}, {"abstract": "Clustering categorical distributions in the probability simplex is a\nfundamental primitive often met in applications dealing with histograms or\nmixtures of multinomials. Traditionally, the differential-geometric structure\nof the probability simplex has been used either by (i) setting the Riemannian\nmetric tensor to the Fisher information matrix of the categorical\ndistributions, or (ii) defining the information-geometric structure induced by\na smooth dissimilarity measure, called a divergence. In this paper, we\nintroduce a novel computationally-friendly non-Riemannian framework for\nmodeling the probability simplex: Hilbert simplex geometry. We discuss the pros\nand cons of those three statistical modelings, and compare them experimentally\nfor clustering tasks.", "authors": ["Frank Nielsen", "Ke Sun"], "category": "cs.LG", "comment": "18 pages", "img": "/static/thumbs/1704.00454v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00454v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00454v1", "published_time": "4/3/2017", "tags": ["cs.LG", "cs.CV"], "title": "Clustering in Hilbert simplex geometry"}, {"abstract": "We present an algorithm for building probabilistic rule lists that is two\norders of magnitude faster than previous work. Rule list algorithms are\ncompetitors for decision tree algorithms. They are associative classifiers, in\nthat they are built from pre-mined association rules. They have a logical\nstructure that is a sequence of IF-THEN rules, identical to a decision list or\none-sided decision tree. Instead of using greedy splitting and pruning like\ndecision tree algorithms, we fully optimize over rule lists, striking a\npractical balance between accuracy, interpretability, and computational speed.\nThe algorithm presented here uses a mixture of theoretical bounds (tight enough\nto have practical implications as a screening or bounding procedure),\ncomputational reuse, and highly tuned language libraries to achieve\ncomputational efficiency. Currently, for many practical problems, this method\nachieves better accuracy and sparsity than decision trees; further, in many\ncases, the computational time is practical and often less than that of decision\ntrees. The result is a probabilistic classifier (which estimates P(y = 1|x) for\neach x) that optimizes the posterior of a Bayesian hierarchical model over rule\nlists.", "authors": ["Hongyu Yang", "Cynthia Rudin", "Margo Seltzer"], "category": "cs.AI", "comment": "31 pages, 19 figures", "img": "/static/thumbs/1602.08610v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.08610v2", "num_discussion": 0, "originally_published_time": "2/27/2016", "pid": "1602.08610v2", "published_time": "4/3/2017", "tags": ["cs.AI"], "title": "Scalable Bayesian Rule Lists"}, {"abstract": "Purpose: To allow fast and high-quality reconstruction of clinical\naccelerated multi-coil MR data by learning a variational network that combines\nthe mathematical structure of variational models with deep learning.\n  Theory and Methods: Generalized compressed sensing reconstruction formulated\nas a variational model is embedded in an unrolled gradient descent scheme. All\nparameters of this formulation, including the prior model defined by filter\nkernels and activation functions as well as the data term weights, are learned\nduring an offline training procedure. The learned model can then be applied\nonline to previously unseen data.\n  Results: The variational network approach is evaluated on a clinical knee\nimaging protocol. The variational network reconstructions outperform standard\nreconstruction algorithms in terms of image quality and residual artifacts for\nall tested acceleration factors and sampling patterns.\n  Conclusion: Variational network reconstructions preserve the natural\nappearance of MR images as well as pathologies that were not included in the\ntraining data set. Due to its high computational performance, i.e.,\nreconstruction time of 193 ms on a single graphics card, and the omission of\nparameter tuning once the network is trained, this new approach to image\nreconstruction can easily be integrated into clinical workflow.", "authors": ["Kerstin Hammernik", "Teresa Klatzer", "Erich Kobler", "Michael P Recht", "Daniel K Sodickson", "Thomas Pock", "Florian Knoll"], "category": "cs.CV", "comment": "Submitted to Magnetic Resonance in Medicine", "img": "/static/thumbs/1704.00447v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00447v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00447v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Learning a Variational Network for Reconstruction of Accelerated MRI\n  Data"}, {"abstract": "We consider the stochastic bandit problem with a continuous set of arms, with\nthe expected reward function over the arms assumed to be fixed but unknown. We\nprovide two new Gaussian process-based algorithms for continuous bandit\noptimization-Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and\nderive corresponding regret bounds. Specifically, the bounds hold when the\nexpected reward function belongs to the reproducing kernel Hilbert space (RKHS)\nthat naturally corresponds to a Gaussian process kernel used as input by the\nalgorithms. Along the way, we derive a new self-normalized concentration\ninequality for vector- valued martingales of arbitrary, possibly infinite,\ndimension. Finally, experimental evaluation and comparisons to existing\nalgorithms on synthetic and real-world environments are carried out that\nhighlight the favorable gains of the proposed strategies in many cases.", "authors": ["Sayak Ray Chowdhury", "Aditya Gopalan"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.00445v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00445v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00445v1", "published_time": "4/3/2017", "tags": ["cs.LG"], "title": "On Kernelized Multi-armed Bandits"}, {"abstract": "Content-dense news report important factual information about an event in\ndirect, succinct manner. Information seeking applications such as information\nextraction, question answering and summarization normally assume all text they\ndeal with is content-dense. Here we empirically test this assumption on news\narticles from the business, U.S. international relations, sports and science\njournalism domains. Our findings clearly indicate that about half of the news\ntexts in our study are in fact not content-dense and motivate the development\nof a supervised content-density detector. We heuristically label a large\ntraining corpus for the task and train a two-layer classifying model based on\nlexical and unlexicalized syntactic features. On manually annotated data, we\ncompare the performance of domain-specific classifiers, trained on data only\nfrom a given news domain and a general classifier in which data from all four\ndomains is pooled together. Our annotation and prediction experiments\ndemonstrate that the concept of content density varies depending on the domain\nand that naive annotators provide judgement biased toward the stereotypical\ndomain label. Domain-specific classifiers are more accurate for domains in\nwhich content-dense texts are typically fewer. Domain independent classifiers\nreproduce better naive crowdsourced judgements. Classification prediction is\nhigh across all conditions, around 80%.", "authors": ["Yinfei Yang", "Ani Nenkova"], "category": "cs.CL", "comment": "In submission to JAIR", "img": "/static/thumbs/1704.00440v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00440v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00440v1", "published_time": "4/3/2017", "tags": ["cs.CL"], "title": "Combining Lexical and Syntactic Features for Detecting Content-dense\n  Texts in News"}, {"abstract": "Unconstrained face recognition performance evaluations have traditionally\nfocused on Labeled Faces in the Wild (LFW) dataset for imagery and the\nYouTubeFaces (YTF) dataset for videos in the last couple of years. Spectacular\nprogress in this field has resulted in a saturation on verification and\nidentification accuracies for those benchmark datasets. In this paper, we\npropose a unified learning framework named transferred deep feature fusion\ntargeting at the new IARPA Janus Bechmark A (IJB-A) face recognition dataset\nreleased by NIST face challenge. The IJB-A dataset includes real-world\nunconstrained faces from 500 subjects with full pose and illumination\nvariations which are much harder than the LFW and YTF datasets. Inspired by\ntransfer learning, we train two advanced deep convolutional neural networks\n(DCNN) with two different large datasets in source domain, respectively. By\nexploring the complementarity of two distinct DCNNs, deep feature fusion is\nutilized after feature extraction in target domain. Then, template specific\nlinear SVMs is adopted to enhance the discrimination of framework. Finally,\nmultiple matching scores corresponding different templates are merged as the\nfinal results. This simple unified framework outperforms the state-of-the-art\nby a wide margin on IJB-A dataset. Based on the proposed approach, we have\nsubmitted our IJB-A results to National Institute of Standards and Technology\n(NIST) for official evaluation.", "authors": ["Lin Xiong", "Jayashree Karlekar", "Jian Zhao", "Jiashi Feng", "Sugiri Pranata", "Shengmei Shen"], "category": "cs.CV", "comment": "10 pages, 7 figures", "img": "/static/thumbs/1704.00438v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00438v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00438v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "A Good Practice Towards Top Performance of Face Recognition: Transferred\n  Deep Feature Fusion"}, {"abstract": "Recently, DNN model compression based on network architecture design, e.g.,\nSqueezeNet, attracted a lot attention. No accuracy drop on image classification\nis observed on these extremely compact networks, compared to well-known models.\nAn emerging question, however, is whether these model compression techniques\nhurt DNN\u0027s learning ability other than classifying images on a single dataset.\nOur preliminary experiment shows that these compression methods could degrade\ndomain adaptation (DA) ability, though the classification performance is\npreserved. Therefore, we propose a new compact network architecture and\nunsupervised DA method in this paper. The DNN is built on a new basic module\nConv-M which provides more diverse feature extractors without significantly\nincreasing parameters. The unified framework of our DA method will\nsimultaneously learn invariance across domains, reduce divergence of feature\nrepresentations, and adapt label prediction. Our DNN has 4.1M parameters, which\nis only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN\nobtains GoogLeNet-level accuracy both on classification and DA, and our DA\nmethod slightly outperforms previous competitive ones. Put all together, our DA\nstrategy based on our DNN achieves state-of-the-art on sixteen of total\neighteen DA tasks on popular Office-31 and Office-Caltech datasets.", "authors": ["Chunpeng Wu", "Wei Wen", "Tariq Afzal", "Yongmei Zhang", "Yiran Chen", "Hai Li"], "category": "cs.CV", "comment": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR\u002717)", "img": "/static/thumbs/1703.04071v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.04071v4", "num_discussion": 0, "originally_published_time": "3/12/2017", "pid": "1703.04071v4", "published_time": "4/3/2017", "tags": ["cs.CV", "cs.AI", "cs.NE"], "title": "A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification\n  and Domain Adaptation"}, {"abstract": "Recently, Wi-Fi has caught tremendous attention for its ubiquity, and,\nmotivated by Wi-Fi\u0027s low cost and privacy preservation, researchers have been\nputting lots of investigation into its potential on action recognition and even\nperson identification. In this paper, we offer an comprehensive overview on\nthese two topics in Wi-Fi. Also, through looking at these two topics from an\nunprecedented perspective, we could achieve generality instead of designing\nspecific ad-hoc features for each scenario. Observing the great resemblance of\nChannel State Information (CSI, a fine-grained information captured from the\nreceived Wi-Fi signal) to texture, we proposed a brand-new framework based on\ncomputer vision methods. To minimize the effect of location dependency embedded\nin CSI, we propose a novel de-noising method based on Singular Value\nDecomposition (SVD) to eliminate the background energy and effectively extract\nthe channel information of signals reflected by human bodies. From the\nexperiments conducted, we demonstrate the feasibility and efficacy of the\nproposed methods. Also, we conclude factors that would affect the performance\nand highlight a few promising issues that require further deliberation.", "authors": ["Jen-Yin Chang", "Kuan-Ying Lee", "Yu-Lin Wei", "Kate Ching-Ju Lin", "Winston Hsu"], "category": "cs.CV", "comment": "10 pages, 10 figures, submit to IEEE Transactions on Multimedia", "img": "/static/thumbs/1608.05461v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.05461v2", "num_discussion": 0, "originally_published_time": "8/19/2016", "pid": "1608.05461v2", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "We Can \"See\" You via Wi-Fi - WiFi Action Recognition via Vision-based\n  Methods"}, {"abstract": "We present a system for converting a fully panoramic ($360^\\circ$) video into\na normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our\nsystem exploits visual saliency and semantics to non-uniformly sample in space\nand time for generating hyperlapses. In addition, users can optionally choose\nobjects of interest for customizing the hyperlapses. We first stabilize an\ninput $360^\\circ$ video by smoothing the rotation between adjacent frames and\nthen compute regions of interest and saliency scores. An initial hyperlapse is\ngenerated by optimizing the saliency and motion smoothness followed by the\nsaliency-aware frame selection. We further smooth the result using an efficient\n2D video stabilization approach that adaptively selects the motion model to\ngenerate the final hyperlapse. We validate the design of our system by showing\nresults for a variety of scenes and comparing against the state-of-the-art\nmethod through a user study.", "authors": ["Wei-Sheng Lai", "Yujia Huang", "Neel Joshi", "Chris Buehler", "Ming-Hsuan Yang", "Sing Bing Kang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1703.10798v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10798v2", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1703.10798v2", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Semantic-driven Generation of Hyperlapse from $360^\\circ$ Video"}, {"abstract": "We propose a new self-supervised CNN pre-training technique based on a novel\nauxiliary task called \"odd-one-out learning\". In this task, the machine is\nasked to identify the unrelated or odd element from a set of otherwise related\nelements. We apply this technique to self-supervised video representation\nlearning where we sample subsequences from videos and ask the network to learn\nto predict the odd video subsequence. The odd video subsequence is sampled such\nthat it has wrong temporal order of frames while the even ones have the correct\ntemporal order. Therefore, to generate a odd-one-out question no manual\nannotation is required. Our learning machine is implemented as multi-stream\nconvolutional neural network, which is learned end-to-end. Using odd-one-out\nnetworks, we learn temporal representations for videos that generalizes to\nother related tasks such as action recognition.\n  On action classification, our method obtains 60.3\\% on the UCF101 dataset\nusing only UCF101 data for training which is approximately 10% better than\ncurrent state-of-the-art self-supervised learning methods. Similarly, on HMDB51\ndataset we outperform self-supervised state-of-the art methods by 12.7% on\naction classification task.", "authors": ["Basura Fernando", "Hakan Bilen", "Efstratios Gavves", "Stephen Gould"], "category": "cs.CV", "comment": "Accepted in In IEEE International Conference on Computer Vision and\n  Pattern Recognition CVPR 2017", "img": "/static/thumbs/1611.06646v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.06646v3", "num_discussion": 0, "originally_published_time": "11/21/2016", "pid": "1611.06646v3", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Self-Supervised Video Representation Learning With Odd-One-Out Networks"}, {"abstract": "We propose a new task of unsupervised action detection by action matching.\nGiven two long videos, the objective is to temporally detect all pairs of\nmatching video segments. A pair of video segments are matched if they share the\nsame human action. The task is category independent---it does not matter what\naction is being performed---and no supervision is used to discover such video\nsegments. Unsupervised action detection by action matching allows us to align\nvideos in a meaningful manner. As such, it can be used to discover new action\ncategories or as an action proposal technique within, say, an action detection\npipeline. Moreover, it is a useful pre-processing step for generating video\nhighlights, e.g., from sports videos.\n  We present an effective and efficient method for unsupervised action\ndetection. We use an unsupervised temporal encoding method and exploit the\ntemporal consistency in human actions to obtain candidate action segments. We\nevaluate our method on this challenging task using three activity recognition\nbenchmarks, namely, the MPII Cooking activities dataset, the THUMOS15 action\ndetection benchmark and a new dataset called the IKEA dataset. On the MPII\nCooking dataset we detect action segments with a precision of 21.6% and recall\nof 11.7% over 946 long video pairs and over 5000 ground truth action segments.\nSimilarly, on THUMOS dataset we obtain 18.4% precision and 25.1% recall over\n5094 ground truth action segment pairs.", "authors": ["Basura Fernando", "Sareh Shirazi", "Stephen Gould"], "category": "cs.CV", "comment": "IEEE International Conference on Computer Vision and Pattern\n  Recognition CVPR 2017 Workshops", "img": "/static/thumbs/1612.00558v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.00558v2", "num_discussion": 0, "originally_published_time": "12/2/2016", "pid": "1612.00558v2", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Unsupervised Human Action Detection by Action Matching"}, {"abstract": "In this paper, we propose a new max-margin based discriminative feature\nlearning method. Specifically, we aim at learning a low-dimensional feature\nrepresentation, so as to maximize the global margin of the data and make the\nsamples from the same class as close as possible. In order to enhance the\nrobustness to noise, a $l_{2,1}$ norm constraint is introduced to make the\ntransformation matrix in group sparsity. In addition, for multi-class\nclassification tasks, we further intend to learn and leverage the correlation\nrelationships among multiple class tasks for assisting in learning\ndiscriminative features. The experimental results demonstrate the power of the\nproposed method against the related state-of-the-art methods.", "authors": ["Changsheng Li", "Qingshan Liu", "Weishan Dong", "Xin Zhang", "Lin Yang"], "category": "cs.LG", "comment": "Accepted by IEEE TNNLS", "img": "/static/thumbs/1412.4863v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1412.4863v2", "num_discussion": 0, "originally_published_time": "12/16/2014", "pid": "1412.4863v2", "published_time": "4/3/2017", "tags": ["cs.LG"], "title": "Max-Margin based Discriminative Feature Learning"}, {"abstract": "This paper addresses the task of AMR-to-text generation by leveraging\nsynchronous node replacement grammar. During training, graph-to-string rules\nare learned using a heuristic extraction algorithm. At test time, a graph\ntransducer is applied to collapse input AMRs and generate output sentences.\nEvaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which\nis the best reported so far.", "authors": ["Linfeng Song", "Xiaochang Peng", "Yue Zhang", "Zhiguo Wang", "Daniel Gildea"], "category": "cs.CL", "comment": "Accepted by ACL 2017", "img": "/static/thumbs/1702.00500v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.00500v3", "num_discussion": 0, "originally_published_time": "2/1/2017", "pid": "1702.00500v3", "published_time": "4/3/2017", "tags": ["cs.CL"], "title": "AMR-to-text Generation with Synchronous Node Replacement Grammar"}, {"abstract": "In the context of single-label classification, despite the huge success of\ndeep learning, the commonly used cross-entropy loss function ignores the\nintricate inter-class relationships that often exist in real-life tasks such as\nage classification. In this work, we propose to leverage these relationships\nbetween classes by training deep nets with the exact squared Earth Mover\u0027s\nDistance (also known as Wasserstein distance) for single-label classification.\nThe squared EMD loss uses the predicted probabilities of all classes and\npenalizes the miss-predictions according to a ground distance matrix that\nquantifies the dissimilarities between classes. We demonstrate that on datasets\nwith strong inter-class relationships such as an ordering between classes, our\nexact squared EMD losses yield new state-of-the-art results. Furthermore, we\npropose a method to automatically learn this matrix using the CNN\u0027s own\nfeatures during training. We show that our method can learn a ground distance\nmatrix efficiently with no inter-class relationship priors and yield the same\nperformance gain. Finally, we show that our method can be generalized to\napplications that lack strong inter-class relationships and still maintain\nstate-of-the-art performance. Therefore, with limited computational overhead,\none can always deploy the proposed loss function on any dataset over the\nconventional cross-entropy.", "authors": ["Le Hou", "Chen-Ping Yu", "Dimitris Samaras"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1611.05916v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.05916v4", "num_discussion": 0, "originally_published_time": "11/17/2016", "pid": "1611.05916v4", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Squared Earth Mover\u0027s Distance-based Loss for Training Deep Neural\n  Networks"}, {"abstract": "In this paper, we propose a novel multi-task learning (MTL) framework, called\nSelf-Paced Multi-Task Learning (SPMTL). Different from previous works treating\nall tasks and instances equally when training, SPMTL attempts to jointly learn\nthe tasks by taking into consideration the complexities of both tasks and\ninstances. This is inspired by the cognitive process of human brain that often\nlearns from the easy to the hard. We construct a compact SPMTL formulation by\nproposing a new task-oriented regularizer that can jointly prioritize the tasks\nand the instances. Thus it can be interpreted as a self-paced learner for MTL.\nA simple yet effective algorithm is designed for optimizing the proposed\nobjective function. An error bound for a simplified formulation is also\nanalyzed theoretically. Experimental results on toy and real-world datasets\ndemonstrate the effectiveness of the proposed approach, compared to the\nstate-of-the-art methods.", "authors": ["Changsheng Li", "Junchi Yan", "Fan Wei", "Weishan Dong", "Qingshan Liu", "Hongyuan Zha"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1604.01474v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.01474v2", "num_discussion": 0, "originally_published_time": "4/6/2016", "pid": "1604.01474v2", "published_time": "4/3/2017", "tags": ["cs.LG"], "title": "Self-Paced Multi-Task Learning"}, {"abstract": "This paper focuses on the problem of simultaneous sample and feature\nselection for machine learning in a fully unsupervised setting. Though most\nexisting works tackle these two problems separately that derives two\nwell-studied sub-areas namely active learning and feature selection, a unified\napproach is inspirational since they are often interleaved with each other.\nNoisy and high-dimensional features will bring adverse effect on sample\nselection, while `good\u0027 samples will be beneficial to feature selection. We\npresent a framework to jointly conduct active learning and feature selection\nbased on the CUR matrix decomposition. From the data reconstruction\nperspective, both the selected samples and features can best approximate the\noriginal dataset respectively, such that the selected samples characterized by\nthe selected features are very representative. Additionally our method is\none-shot without iteratively selecting samples for progressive labeling. Thus\nour model is especially suitable when the initial labeled samples are scarce or\ntotally absent, which existing works hardly address particularly for\nsimultaneous feature selection. To alleviate the NP-hardness of the raw\nproblem, the proposed formulation involves a convex but non-smooth optimization\nproblem. We solve it efficiently by an iterative algorithm, and prove its\nglobal convergence. Experiments on publicly available datasets validate that\nour method is promising compared with the state-of-the-arts.", "authors": ["Changsheng Li", "Xiangfeng Wang", "Weishan Dong", "Junchi Yan", "Qingshan Liu", "Hongyuan Zha"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1503.01239v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1503.01239v3", "num_discussion": 0, "originally_published_time": "3/4/2015", "pid": "1503.01239v3", "published_time": "4/3/2017", "tags": ["cs.LG"], "title": "Joint Active Learning and Feature Selection via CUR Matrix Decomposition"}, {"abstract": "Histopathology images are crucial to the study of complex diseases such as\ncancer. The histologic characteristics of nuclei play a key role in disease\ndiagnosis, prognosis and analysis. In this work, we propose a sparse\nConvolutional Autoencoder (CAE) for fully unsupervised, simultaneous nucleus\ndetection and feature extraction in histopathology tissue images. Our CAE\ndetects and encodes nuclei in image patches in tissue images into sparse\nfeature maps that encode both the location and appearance of nuclei. Our CAE is\nthe first unsupervised detection network for computer vision applications. The\npretrained nucleus detection and feature extraction modules in our CAE can be\nfine-tuned for supervised learning in an end-to-end fashion. We evaluate our\nmethod on four datasets and reduce the errors of state-of-the-art methods up to\n42%. We are able to achieve comparable performance with only 5% of the\nfully-supervised annotation cost.", "authors": ["Le Hou", "Vu Nguyen", "Dimitris Samaras", "Tahsin M. Kurc", "Yi Gao", "Tianhao Zhao", "Joel H. Saltz"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00406v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00406v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00406v1", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Sparse Autoencoder for Unsupervised Nucleus Detection and Representation\n  in Histopathology Images"}, {"abstract": "Traditional approaches to Semantic Role Labeling (SRL) depend heavily on\nmanual feature engineering. Recurrent neural network (RNN) with long-short-term\nmemory (LSTM) only treats sentence as sequence data and can not utilize higher\nlevel syntactic information. In this paper, we propose Syntax Aware LSTM\n(SA-LSTM) which gives RNN-LSTM ability to utilize higher level syntactic\ninformation gained from dependency relationship information. SA-LSTM also\nassigns different trainable weights to different types of dependency\nrelationship automatically. Experiment results on Chinese Proposition Bank\n(CPB) show that, even without pre-training or introducing any other extra\nsemantically annotated resources, our SA-LSTM model still outperforms the state\nof the art significantly base on Student\u0027s t-test ($p\u003c0.05$). Trained weights\nof types of dependency relationship form a stable and self-explanatory pattern.", "authors": ["Feng Qian", "Lei Sha", "Baobao Chang", "Lu-chen Liu", "Ming Zhang"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.00405v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00405v1", "num_discussion": 0, "originally_published_time": "4/3/2017", "pid": "1704.00405v1", "published_time": "4/3/2017", "tags": ["cs.CL"], "title": "Syntax Aware LSTM Model for Chinese Semantic Role Labeling"}, {"abstract": "In search engines, online marketplaces and other human-computer interfaces\nlarge collectives of individuals sequentially interact with numerous\nalternatives of varying quality. In these contexts, trial and error\n(exploration) is crucial for uncovering novel high-quality items or solutions,\nbut entails a high cost for individual users. Self-interested decision makers,\nare often better off imitating the choices of individuals who have already\nincurred the costs of exploration. Although imitation makes sense at the\nindividual level, it deprives the group of additional information that could\nhave been gleaned by individual explorers. In this paper we show that in such\nproblems, preference diversity can function as a welfare enhancing mechanism.\nIt leads to a consistent increase in the quality of the consumed alternatives\nthat outweighs the increased cost of search for the users.", "authors": ["Pantelis P. Analytis", "Hrvoje Stojic", "Alexandros Gelastopoulos", "Mehdi Moussa\u00efd"], "category": "cs.AI", "comment": "4 pages, 1 figure, originally presented at the collected intelligence\n  (CI) conference in June 2017", "img": "/static/thumbs/1703.10970v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10970v2", "num_discussion": 0, "originally_published_time": "3/28/2017", "pid": "1703.10970v2", "published_time": "4/3/2017", "tags": ["cs.AI", "cs.MA"], "title": "Diversity of preferences can increase collective welfare in sequential\n  exploration problems"}, {"abstract": "We introduce a new problem of generating an image based on a small number of\nkey local patches without any geometric prior. In this work, key local patches\nare defined as informative regions of the target object or scene. This is a\nchallenging problem since it requires generating realistic images and\npredicting locations of parts at the same time. We construct adversarial\nnetworks to tackle this problem. A generator network generates a fake image as\nwell as a mask based on the encoder-decoder framework. On the other hand, a\ndiscriminator network aims to detect fake images. The network is trained with\nthree losses to consider spatial, appearance, and adversarial information. The\nspatial loss determines whether the locations of predicted parts are correct.\nInput patches are restored in the output image without much modification due to\nthe appearance loss. The adversarial loss ensures output images are realistic.\nThe proposed network is trained without supervisory signals since no labels of\nkey parts are required. Experimental results on six datasets demonstrate that\nthe proposed algorithm performs favorably on challenging objects and scenes.", "authors": ["Donghoon Lee", "Sangdoo Yun", "Sungjoon Choi", "Hwiyeon Yoo", "Ming-Hsuan Yang", "Songhwai Oh"], "category": "cs.CV", "comment": "16 pages", "img": "/static/thumbs/1703.10730v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10730v2", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1703.10730v2", "published_time": "4/3/2017", "tags": ["cs.CV"], "title": "Unsupervised Holistic Image Generation from Key Local Patches"}, {"abstract": "Self-adaptive parameters are increasingly used in the field of Evolutionary\nRobotics, as they allow key evolutionary rates to vary autonomously in a\ncontext-sensitive manner throughout the optimisation process. A significant\nlimitation to self-adaptive mutation is that rates can be set unfavourably,\nwhich hinders convergence. Rate restarts are typically employed to remedy this,\nbut thus far have only been applied in Evolutionary Robotics for mutation-only\nalgorithms. This paper focuses on the level at which evolutionary rate restarts\nare applied in population-based algorithms with more than 1 evolutionary\noperator. After testing on a real hexacopter hovering task, we conclude that\nindividual-level restarting results in higher fitness solutions without fitness\nstagnation, and population restarts provide a more stable rate evolution.\nWithout restarts, experiments can become stuck in suboptimal controller/rate\ncombinations which can be difficult to escape from.", "authors": ["Gerard David Howard"], "category": "cs.NE", "comment": "8 pages", "img": "/static/thumbs/1703.10754v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10754v2", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1703.10754v2", "published_time": "4/3/2017", "tags": ["cs.NE", "cs.RO"], "title": "On Self-Adaptive Mutation Restarts for Evolutionary Robotics with Real\n  Rotorcraft"}, {"abstract": "Deep learning has shown to be effective for robust and real-time monocular\nimage relocalisation. In particular, PoseNet is a deep convolutional neural\nnetwork which learns to regress the 6-DOF camera pose from a single image. It\nlearns to localize using high level features and is robust to difficult\nlighting, motion blur and unknown camera intrinsics, where point based SIFT\nregistration fails. However, it was trained using a naive loss function, with\nhyper-parameters which require expensive tuning. In this paper, we give the\nproblem a more fundamental theoretical treatment. We explore a number of novel\nloss functions for learning camera pose which are based on geometry and scene\nreprojection error. Additionally we show how to automatically learn an optimal\nweighting to simultaneously regress position and orientation. By leveraging\ngeometry, we demonstrate that our technique significantly improves PoseNet\u0027s\nperformance across datasets ranging from indoor rooms to a small city.", "authors": ["Alex Kendall", "Roberto Cipolla"], "category": "cs.CV", "comment": "CVPR 2017", "img": "/static/thumbs/1704.00390v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00390v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00390v1", "published_time": "4/2/2017", "tags": ["cs.CV"], "title": "Geometric loss functions for camera pose regression with deep learning"}, {"abstract": "Analyzing videos of human actions involves understanding the temporal\nrelationships among video frames. CNNs are the current state-of-the-art methods\nfor action recognition in videos. However, the CNN architectures currently\nbeing used have difficulty in capturing these relationships. State-of-the-art\naction recognition approaches rely on traditional local optical flow estimation\nmethods to pre-compute the motion information for CNNs. Such a two-stage\napproach is computationally expensive, storage demanding, and not end-to-end\ntrainable. In this paper, we present a novel CNN architecture that implicitly\ncaptures motion information. Our method is 10x faster than a two-stage\napproach, does not need to cache flow information, and is end-to-end trainable.\nExperimental results on UCF101 and HMDB51 show that it achieves competitive\naccuracy with the two-stage approaches.", "authors": ["Yi Zhu", "Zhenzhong Lan", "Shawn Newsam", "Alexander G. Hauptmann"], "category": "cs.CV", "comment": "under review at ICCV 2017", "img": "/static/thumbs/1704.00389v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00389v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00389v1", "published_time": "4/2/2017", "tags": ["cs.CV", "cs.LG", "cs.MM"], "title": "Hidden Two-Stream Convolutional Networks for Action Recognition"}, {"abstract": "Many complex systems can be represented as networks, and the problem of\nnetwork comparison is becoming increasingly relevant. There are many techniques\nfor network comparison, from simply comparing network summary statistics to\nsophisticated but computationally costly alignment-based approaches. Yet it\nremains challenging to accurately cluster networks that are of a different size\nand density, but hypothesized to be structurally similar. In this paper, we\naddress this problem by introducing a new network comparison methodology that\nis aimed at identifying common organizational principles in networks. The\nmethodology is simple, intuitive and applicable in a wide variety of settings\nranging from the functional classification of proteins to tracking the\nevolution of a world trade network.", "authors": ["Anatol E. Wegner", "Luis Ospina-Forero", "Robert E. Gaunt", "Charlotte M. Deane", "Gesine Reinert"], "category": "stat.ML", "comment": "26 pages, 7 figures", "img": "/static/thumbs/1704.00387v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00387v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00387v1", "published_time": "4/2/2017", "tags": ["stat.ML", "cs.SI", "physics.soc-ph"], "title": "Identifying networks with common organizational principles"}, {"abstract": "Near-sensor data analytics is a promising direction for IoT endpoints, as it\nminimizes energy spent on communication and reduces network load - but it also\nposes security concerns, as valuable data is stored or sent over the network at\nvarious stages of the analytics pipeline. Using encryption to protect sensitive\ndata at the boundary of the on-chip analytics engine is a way to address data\nsecurity issues. To cope with the combined workload of analytics and encryption\nin a tight power envelope, we propose Fulmine, a System-on-Chip based on a\ntightly-coupled multi-core cluster augmented with specialized blocks for\ncompute-intensive data processing and encryption functions, supporting software\nprogrammability for regular computing tasks. The Fulmine SoC, fabricated in\n65nm technology, consumes less than 20mW on average at 0.8V achieving an\nefficiency of up to 70pJ/B in encryption, 50pJ/px in convolution, or up to\n25MIPS/mW in software. As a strong argument for real-life flexible application\nof our platform, we show experimental results for three secure analytics use\ncases: secure autonomous aerial surveillance with a state-of-the-art deep CNN\nconsuming 3.16pJ per equivalent RISC op; local CNN-based face detection with\nsecured remote recognition in 5.74pJ/op; and seizure detection with encrypted\ndata collection from EEG within 12.7pJ/op.", "authors": ["Francesco Conti", "Robert Schilling", "Pasquale Davide Schiavone", "Antonio Pullini", "Davide Rossi", "Frank Kagan G\u00fcrkaynak", "Michael Muehlberghuber", "Michael Gautschi", "Igor Loi", "Germain Haugou", "Stefan Mangard", "Luca Benini"], "category": "cs.AR", "comment": "14 pages, 12 figures, submitted to IEEE Transactions on Circuits and\n  Systems - I: Regular Papers. ...", "img": "/static/thumbs/1612.05974v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.05974v2", "num_discussion": 0, "originally_published_time": "12/18/2016", "pid": "1612.05974v2", "published_time": "4/2/2017", "tags": ["cs.AR", "cs.LG"], "title": "An IoT Endpoint System-on-Chip for Secure and Energy-Efficient\n  Near-Sensor Analytics"}, {"abstract": "From doctors diagnosing patients to judges setting bail, experts often base\ntheir decisions on experience and intuition rather than on statistical models.\nWhile understandable, relying on intuition over models has often been found to\nresult in inferior outcomes. Here we present a new method,\nselect-regress-and-round, for constructing simple rules that perform well for\ncomplex decisions. These rules take the form of a weighted checklist, can be\napplied mentally, and nonetheless rival the performance of modern machine\nlearning algorithms. Our method for creating these rules is itself simple, and\ncan be carried out by practitioners with basic statistics knowledge. We\ndemonstrate this technique with a detailed case study of judicial decisions to\nrelease or detain defendants while they await trial. In this application, as in\nmany policy settings, the effects of proposed decision rules cannot be directly\nobserved from historical data: if a rule recommends releasing a defendant that\nthe judge in reality detained, we do not observe what would have happened under\nthe proposed action. We address this key counterfactual estimation problem by\ndrawing on tools from causal inference. We find that simple rules significantly\noutperform judges and are on par with decisions derived from random forests\ntrained on all available features. Generalizing to 22 varied decision-making\ndomains, we find this basic result replicates. We conclude with an analytical\nframework that helps explain why these simple decision rules perform as well as\nthey do.", "authors": ["Jongbin Jung", "Connor Concannon", "Ravi Shroff", "Sharad Goel", "Daniel G. Goldstein"], "category": "stat.AP", "comment": "", "img": "/static/thumbs/1702.04690v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.04690v3", "num_discussion": 0, "originally_published_time": "2/15/2017", "pid": "1702.04690v3", "published_time": "4/2/2017", "tags": ["stat.AP", "stat.ML"], "title": "Simple rules for complex decisions"}, {"abstract": "One of the most important problems in machine translation (MT) evaluation is\nto evaluate the similarity between translation hypotheses with different\nsurface forms from the reference, especially at the segment level. We propose\nto use word embeddings to perform word alignment for segment-level MT\nevaluation. We performed experiments with three types of alignment methods\nusing word embeddings. We evaluated our proposed methods with various\ntranslation datasets. Experimental results show that our proposed methods\noutperform previous word embeddings-based methods.", "authors": ["Junki Matsuo", "Mamoru Komachi", "Katsuhito Sudoh"], "category": "cs.CL", "comment": "5 pages", "img": "/static/thumbs/1704.00380v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00380v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00380v1", "published_time": "4/2/2017", "tags": ["cs.CL"], "title": "Word-Alignment-Based Segment-Level Machine Translation Evaluation using\n  Word Embeddings"}, {"abstract": "Despite the recent success of deep neural networks in various applications,\ndesigning and training deep neural networks is still among the greatest\nchallenges in the field. In this work, we address the challenge of designing\nand training feedforward Multilayer Perceptrons (MLPs) from a smooth\noptimisation perspective. By characterising the critical point conditions of an\nMLP based loss function, we identify conditions to eliminate local optima of\nthe corresponding cost function. By studying the Hessian structure of the cost\nfunction at the global minima, we develop an approximate Newton\u0027s MLP\nalgorithm. Our results are demonstrated on an analysis of MLPs with only one\nhidden layer, and numerically evaluated on the benchmark problem of four region\nclassification.", "authors": ["Hao Shen"], "category": "cs.LG", "comment": "18 pages, 2 figures, submitted for publication", "img": "/static/thumbs/1611.05827v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.05827v2", "num_discussion": 0, "originally_published_time": "11/17/2016", "pid": "1611.05827v2", "published_time": "4/2/2017", "tags": ["cs.LG", "cs.AI", "cs.NE", "math.OC"], "title": "A Smooth Optimisation Perspective on Designing and Training Feedforward\n  Multilayer Perceptrons"}, {"abstract": "The robust PCA problem, wherein, given an input data matrix that is the\nsuperposition of a low-rank matrix and a sparse matrix, we aim to separate out\nthe low-rank and sparse components, is a well-studied problem in machine\nlearning. One natural question that arises is that, as in the inductive\nsetting, if features are provided as input as well, can we hope to do better?\nAnswering this in the affirmative, the main goal of this paper is to study the\nrobust PCA problem while incorporating feature information. In contrast to\nprevious works in which recovery guarantees are based on the convex relaxation\nof the problem, we propose a simple iterative algorithm based on\nhard-thresholding of appropriate residuals. Under weaker assumptions than\nprevious works, we prove the global convergence of our iterative procedure;\nmoreover, it admits a much faster convergence rate and lesser computational\ncomplexity per iteration. In practice, through systematic synthetic and real\ndata simulations, we confirm our theoretical findings regarding improvements\nobtained by using feature information.", "authors": ["U. N. Niranjan", "Arun Rajkumar", "Theja Tulabandhula"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.00367v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00367v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00367v1", "published_time": "4/2/2017", "tags": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "title": "Provable Inductive Robust PCA via Iterative Hard Thresholding"}, {"abstract": "Concept drift is a major issue that greatly affects the accuracy and\nreliability of many real-world applications of machine learning. We argue that\nto tackle concept drift it is important to develop the capacity to describe and\nanalyze it. We propose tools for this purpose, arguing for the importance of\nquantitative descriptions of drift in marginal distributions. We present\nquantitative drift analysis techniques along with methods for communicating\ntheir results. We demonstrate their effectiveness by application to three\nreal-world learning tasks.", "authors": ["Geoffrey I. Webb", "Loong Kuan Lee", "Fran\u00e7ois Petitjean", "Bart Goethals"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.00362v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00362v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00362v1", "published_time": "4/2/2017", "tags": ["cs.LG"], "title": "Understanding Concept Drift"}, {"abstract": "In this paper, we consider stochastic dual coordinate (SDCA) {\\em without}\nstrongly convex assumption or convex assumption. We show that SDCA converges\nlinearly under mild conditions termed restricted strong convexity. This covers\na wide array of popular statistical models including Lasso, group Lasso, and\nlogistic regression with $\\ell_1$ regularization, corrected Lasso and linear\nregression with SCAD regularizer. This significantly improves previous\nconvergence results on SDCA for problems that are not strongly convex. As a by\nproduct, we derive a dual free form of SDCA that can handle general\nregularization term, which is of interest by itself.", "authors": ["Chao Qu", "Huan Xu"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1701.07808v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.07808v4", "num_discussion": 0, "originally_published_time": "1/26/2017", "pid": "1701.07808v4", "published_time": "4/2/2017", "tags": ["stat.ML", "cs.LG"], "title": "Linear convergence of SDCA in statistical estimation"}, {"abstract": "We introduce a variational method for multi-view shape-from-shading under\nnatural illumination. The key idea is to couple PDE-based solutions for\nsingle-image based shape-from-shading problems across multiple images and\nmultiple color channels by means of a variational formulation. Rather than\nalternatingly solving the individual SFS problems and optimizing the\nconsistency across images and channels which is known to lead to suboptimal\nresults, we propose an efficient solution of the coupled problem by means of an\nADMM algorithm. In numerous experiments on both simulated and real imagery, we\ndemonstrate that the proposed fusion of multiple-view reconstruction and\nshape-from-shading provides highly accurate dense reconstructions without the\nneed to compute dense correspondences. With the proposed variational\nintegration across multiple views shape-from-shading techniques become\napplicable to challenging real-world reconstruction problems, giving rise to\nhighly detailed geometry even in areas of smooth brightness variation and\nlacking texture.", "authors": ["Yvain Qu\u00e9au", "Jean M\u00e9lou", "Jean-Denis Durou", "Daniel Cremers"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00337v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00337v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00337v1", "published_time": "4/2/2017", "tags": ["cs.CV"], "title": "Dense Multi-view 3D-reconstruction Without Dense Correspondences"}, {"abstract": "This contribution deals with image restoration in optical systems with\ncoherent illumination, which is an important topic in astronomy, coherent\nmicroscopy and radar imaging. Such optical systems suffer from wavefront\ndistortions, which are caused by imperfect imaging components and conditions.\nKnown image restoration algorithms work well for incoherent imaging, they fail\nin case of coherent images. In this paper a novel wavefront correction\nalgorithm is presented, which allows image restoration under coherent\nconditions. In most coherent imaging systems, especially in astronomy, the\nwavefront deformation is known. Using this information, the proposed algorithm\nallows a high quality restoration even in case of severe wavefront distortions.\nWe present two versions of this algorithm, which are an evolution of the\nGerchberg-Saxton and the Hybrid-Input-Output algorithm. The algorithm is\nverified on simulated and real microscopic images.", "authors": ["Claudius Zelenka", "Reinhard Koch"], "category": "astro-ph.IM", "comment": "To appear in the proceedings of the 23rd International Conference on\n  Pattern Recognition (ICPR 201...", "img": "/static/thumbs/1704.00331v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00331v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00331v1", "published_time": "4/2/2017", "tags": ["astro-ph.IM", "cs.CV"], "title": "Restoration of Images with Wavefront Aberrations"}, {"abstract": "We systematically study the deep representation of random weight CNN\n(convolutional neural network) using the DeCNN (deconvolutional neural network)\narchitecture. We first fix the weights of an untrained CNN, and for each layer\nof its feature representation, we train a corresponding DeCNN to reconstruct\nthe input image. As compared with the pre-trained CNN, the DeCNN trained on a\nrandom weight CNN can reconstruct images more quickly and accurately, no matter\nwhich type of random distribution for the CNN\u0027s weights. It reveals that every\nlayer of the random CNN can retain photographically accurate information about\nthe image. We then let the DeCNN be untrained, i.e. the overall CNN-DeCNN\narchitecture uses only random weights. Strikingly, we can reconstruct all\nposition information of the image for low layer representations but the colors\nchange. For high layer representations, we can still capture the rough contours\nof the image. We also change the number of feature maps and the shape of the\nfeature maps and gain more insight on the random function of the CNN-DeCNN\nstructure. Our work reveals that the purely random CNN-DeCNN architecture\nsubstantially contributes to the geometric and photometric invariance due to\nthe intrinsic symmetry and invertible structure, but it discards the\ncolormetric information due to the random projection.", "authors": ["Yao Shu", "Man Zhu", "Kun He", "John Hopcroft", "Pan Zhou"], "category": "cs.CV", "comment": "7 pages, 12 Figures, submitted to a 2017 Conference", "img": "/static/thumbs/1704.00330v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00330v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00330v1", "published_time": "4/2/2017", "tags": ["cs.CV"], "title": "Understanding Deep Representations through Random Weights"}, {"abstract": "In this paper we present an on-manifold sequence-to-sequence learning\napproach to motion estimation using visual and inertial sensors. It is to the\nbest of our knowledge the first end-to-end trainable method for visual-inertial\nodometry which performs fusion of the data at an intermediate\nfeature-representation level. Our method has numerous advantages over\ntraditional approaches. Specifically, it eliminates the need for tedious manual\nsynchronization of the camera and IMU as well as eliminating the need for\nmanual calibration between the IMU and camera. A further advantage is that our\nmodel naturally and elegantly incorporates domain specific information which\nsignificantly mitigates drift. We show that our approach is competitive with\nstate-of-the-art traditional methods when accurate calibration data is\navailable and can be trained to outperform them in the presence of calibration\nand synchronization errors.", "authors": ["Ronald Clark", "Sen Wang", "Hongkai Wen", "Andrew Markham", "Niki Trigoni"], "category": "cs.CV", "comment": "AAAI-17", "img": "/static/thumbs/1701.08376v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.08376v2", "num_discussion": 0, "originally_published_time": "1/29/2017", "pid": "1701.08376v2", "published_time": "4/2/2017", "tags": ["cs.CV"], "title": "VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning\n  Problem"}, {"abstract": "This paper presents two novel approaches for people counting in crowded and\nopen environments that combine the information gathered by multiple views.\nMultiple camera are used to expand the field of view as well as to mitigate the\nproblem of occlusion that commonly affects the performance of counting methods\nusing single cameras. The first approach is regarded as a direct approach and\nit attempts to segment and count each individual in the crowd. For such an aim,\ntwo head detectors trained with head images are employed: one based on support\nvector machines and another based on Adaboost perceptron. The second approach,\nregarded as an indirect approach employs learning algorithms and statistical\nanalysis on the whole crowd to achieve counting. For such an aim, corner points\nare extracted from groups of people in a foreground image and computed by a\nlearning algorithm which estimates the number of people in the scene. Both\napproaches count the number of people on the scene and not only on a given\nimage or video frame of the scene. The experimental results obtained on the\nbenchmark PETS2009 video dataset show that proposed indirect method surpasses\nother methods with improvements of up to 46.7% and provides accurate counting\nresults for the crowded scenes. On the other hand, the direct method shows high\nerror rates due to the fact that the latter has much more complex problems to\nsolve, such as segmentation of heads.", "authors": ["Fabio Dittrich", "Luiz E. S. de Oliveira", "Alceu S. Britto Jr.", "Alessandro L. Koerich"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00326v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00326v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00326v1", "published_time": "4/2/2017", "tags": ["cs.CV"], "title": "People Counting in Crowded and Outdoor Scenes using an Hybrid\n  Multi-Camera Approach"}, {"abstract": "In this paper, we present a new algorithm for parallel Monte Carlo tree\nsearch (MCTS). It is based on the pipeline pattern and allows flexible\nmanagement of the control flow of the operations in parallel MCTS. The pipeline\npattern provides for the first structured parallel programming approach to\nMCTS. Moreover, we propose a new lock-free tree data structure for parallel\nMCTS which removes synchronization overhead. The Pipeline Pattern for Parallel\nMCTS algorithm (called 3PMCTS), scales very well to higher numbers of cores\nwhen compared to the existing methods.", "authors": ["S. Ali Mirsoleimani", "Aske Plaat", "Jaap van den Herik", "Jos Vermaseren"], "category": "cs.AI", "comment": "9 pages", "img": "/static/thumbs/1704.00325v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00325v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00325v1", "published_time": "4/2/2017", "tags": ["cs.AI"], "title": "Structured Parallel Programming for Monte Carlo Tree Search"}, {"abstract": "Many efforts have been dedicated to identifying restrictions on ontologies\nexpressed as tuple-generating dependencies (tgds), a.k.a. existential rules,\nthat lead to the decidability for the problem of answering ontology-mediated\nqueries (OMQs). This has given rise to three families of formalisms: guarded,\nnon-recursive, and sticky sets of tgds. In this work, we study the containment\nproblem for OMQs expressed in such formalisms, which is a key ingredient for\nsolving static analysis tasks associated with them. Our main contribution is\nthe development of specially tailored techniques for OMQ containment under the\nclasses of tgds stated above. This enables us to obtain sharp complexity bounds\nfor the problems at hand, which in turn allow us to delimitate its practical\napplicability. We also apply our techniques to pinpoint the complexity of\nproblems associated with two emerging applications of OMQ containment:\ndistribution over components and UCQ rewritability of OMQs.", "authors": ["Pablo Barcelo", "Gerald Berger", "Andreas Pieris"], "category": "cs.DB", "comment": "", "img": "/static/thumbs/1703.07994v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.07994v2", "num_discussion": 0, "originally_published_time": "3/23/2017", "pid": "1703.07994v2", "published_time": "4/2/2017", "tags": ["cs.DB", "cs.AI", "cs.LO"], "title": "Containment for Rule-Based Ontology-Mediated Queries"}, {"abstract": "Discrminative trackers, employ a classification approach to separate the\ntarget from its background. To cope with variations of the target shape and\nappearance, the classifier is updated online with different samples of the\ntarget and the background. Sample selection, labeling and updating the\nclassifier is prone to various sources of errors that drift the tracker. We\nintroduce the use of an efficient version space shrinking strategy to reduce\nthe labeling errors and enhance its sampling strategy by measuring the\nuncertainty of the tracker about the samples. The proposed tracker, utilize an\nensemble of classifiers that represents different hypotheses about the target,\ndiversify them using boosting to provide a larger and more consistent coverage\nof the version-space and tune the classifiers\u0027 weights in voting. The proposed\nsystem adjusts the model update rate by promoting the co-training of the\nshort-memory ensemble with a long-memory oracle. The proposed tracker\noutperformed state-of-the-art trackers on different sequences bearing various\ntracking challenges.", "authors": ["Kourosh Meshgi", "Shigeyuki Oba", "Shin Ishii"], "category": "cs.CV", "comment": "CRV\u002717 Conference", "img": "/static/thumbs/1704.00299v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00299v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00299v1", "published_time": "4/2/2017", "tags": ["cs.CV"], "title": "Efficient Version-Space Reduction for Visual Tracking"}, {"abstract": "Online knowledge repositories typically rely on their users or dedicated\neditors to evaluate the reliability of their content. These evaluations can be\nviewed as noisy measurements of both information reliability and information\nsource trustworthiness. Can we leverage these noisy evaluations, often biased,\nto distill a robust, unbiased and interpretable measure of both notions?\n  In this paper, we argue that the temporal traces left by these noisy\nevaluations give cues on the reliability of the information and the\ntrustworthiness of the sources. Then, we propose a temporal point process\nmodeling framework that links these temporal traces to robust, unbiased and\ninterpretable notions of information reliability and source trustworthiness.\nFurthermore, we develop an efficient convex optimization procedure to learn the\nparameters of the model from historical traces. Experiments on real-world data\ngathered from Wikipedia and Stack Overflow show that our modeling framework\naccurately predicts evaluation events, provides an interpretable measure of\ninformation reliability and source trustworthiness, and yields interesting\ninsights about real-world events.", "authors": ["Behzad Tabibian", "Isabel Valera", "Mehrdad Farajtabar", "Le Song", "Bernhard Sch\u00f6lkopf", "Manuel Gomez-Rodriguez"], "category": "cs.SI", "comment": "Accepted at 26th World Wide Web conference (WWW-17)", "img": "/static/thumbs/1610.07472v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.07472v3", "num_discussion": 0, "originally_published_time": "10/24/2016", "pid": "1610.07472v3", "published_time": "4/2/2017", "tags": ["cs.SI", "stat.ML"], "title": "Distilling Information Reliability and Source Trustworthiness from\n  Digital Traces"}, {"abstract": "Reprogramming matter may sound far-fetched, but we have been doing it with\nincreasing power and staggering efficiency for at least 60 years, and for\ncenturies we have been paving the way toward the ultimate reprogrammed fate of\nthe universe, the vessel of all programs. How will we be doing it in 60 years\u0027\ntime and how will it impact life and the purpose both of machines and of\nhumans?", "authors": ["Hector Zenil"], "category": "cs.CY", "comment": "Invited contribution to `Computing in the year 2065\u0027, A. Adamatzky\n  (Ed.), Springer Verlag", "img": "/static/thumbs/1704.00725v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00725v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00725v1", "published_time": "4/2/2017", "tags": ["cs.CY", "cs.AI"], "title": "Reprogramming Matter, Life, and Purpose"}, {"abstract": "Recent progress in advanced driver assistance systems and the race towards\nautonomous vehicles is mainly driven by two factors: (1) increasingly\nsophisticated algorithms that interpret the environment around the vehicle and\nreact accordingly, and (2) the continuous improvements of sensor technology\nitself. In terms of cameras, these improvements typically include higher\nspatial resolution, which as a consequence requires more data to be processed.\nThe trend to add multiple cameras to cover the entire surrounding of the\nvehicle is not conducive in that matter. At the same time, an increasing number\nof special purpose algorithms need access to the sensor input data to correctly\ninterpret the various complex situations that can occur, particularly in urban\ntraffic.\n  By observing those trends, it becomes clear that a key challenge for vision\narchitectures in intelligent vehicles is to share computational resources. We\nbelieve this challenge should be faced by introducing a representation of the\nsensory data that provides compressed and structured access to all relevant\nvisual content of the scene. The Stixel World discussed in this paper is such a\nrepresentation. It is a medium-level model of the environment that is\nspecifically designed to compress information about obstacles by leveraging the\ntypical layout of outdoor traffic scenes. It has proven useful for a multitude\nof automotive vision applications, including object detection, tracking,\nsegmentation, and mapping.\n  In this paper, we summarize the ideas behind the model and generalize it to\ntake into account multiple dense input streams: the image itself, stereo depth\nmaps, and semantic class probability maps that can be generated, e.g., by CNNs.\nOur generalization is embedded into a novel mathematical formulation for the\nStixel model. We further sketch how the free parameters of the model can be\nlearned using structured SVMs.", "authors": ["Marius Cordts", "Timo Rehfeld", "Lukas Schneider", "David Pfeiffer", "Markus Enzweiler", "Stefan Roth", "Marc Pollefeys", "Uwe Franke"], "category": "cs.CV", "comment": "Accepted for publication in Image and Vision Computing", "img": "/static/thumbs/1704.00280v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00280v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00280v1", "published_time": "4/2/2017", "tags": ["cs.CV"], "title": "The Stixel world: A medium-level representation of traffic scenes"}, {"abstract": "In this paper we investigate the use of discriminative model learning through\nConvolutional Neural Networks (CNNs) for SAR image despeckling. The network\nuses a residual learning strategy, hence it does not recover the filtered\nimage, but the speckle component, which is then subtracted from the noisy one.\nTraining is carried out by considering a large multitemporal SAR image properly\ndespeckled through 3D filtering, in order to approximate a {\\em clean} image.\nExperimental results, both on synthetic and real SAR data, show the method to\nachieve performance that improve with respect to state-of-the-art techniques.", "authors": ["G. Chierchia", "D. Cozzolino", "G. Poggi", "L. Verdoliva"], "category": "cs.CV", "comment": "Accepted at 2017 IEEE International Geoscience and Remote Sensing\n  Symposium, Fort Worth, Texas, Ju...", "img": "/static/thumbs/1704.00275v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00275v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00275v1", "published_time": "4/2/2017", "tags": ["cs.CV"], "title": "SAR image despeckling through convolutional neural networks"}, {"abstract": "Rapidly-exploring Random Tree Star(RRT*) is a recently proposed extension of\nRapidly-exploring Random Tree (RRT) algorithm that provides a collision-free,\nasymptotically optimal path regardless of obstacle\u0027s geometry in a given\nenvironment. However, one of the limitations in the RRT* algorithm is slow\nconvergence to optimal path solution. As a result, it consumes high memory as\nwell as time due to a large number of iterations utilised in achieving optimal\npath solution. To overcome these limitations, we propose the Potential Function\nBased-RRT* (P-RRT*) that incorporates the Artificial Potential Field Algorithm\nin RRT*. The proposed algorithm allows a considerable decrease in the number of\niterations and thus leads to more efficient memory utilization and an\naccelerated convergence rate. In order to illustrate the usefulness of the\nproposed algorithm in terms of space execution and convergence rate, this paper\npresents rigorous simulation based comparisons between the proposed techniques\nand RRT* under different environmental conditions. Moreover, both algorithms\nare also tested and compared under non-holonomic differential constraints.", "authors": ["Ahmed Hussain Qureshi", "Yasar Ayaz"], "category": "cs.RO", "comment": "This paper introduces a novel algorithm called P-RRT*. The work has\n  been published in Springer Aut...", "img": "/static/thumbs/1704.00264v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00264v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00264v1", "published_time": "4/2/2017", "tags": ["cs.RO", "cs.AI"], "title": "Potential Functions based Sampling Heuristic For Optimal Path Planning"}, {"abstract": "A grand goal of computer vision is to build systems that learn visual\nrepresentations over time that can be applied to many tasks. In this paper, we\ninvestigate a vision-language embedding as a core representation and show that\nit leads to better cross-task transfer than standard multi-task learning. In\nparticular, the task of visual recognition is aligned to the task of visual\nquestion answering by forcing each to use the same word-region embeddings. We\nshow this leads to greater inductive transfer from recognition to VQA than\nstandard multitask learning. Visual recognition also improves, especially for\ncategories that have relatively few recognition training labels but appear\noften in the VQA setting. Thus, our paper takes a small step towards creating\nmore general vision systems by showing the benefit of interpretable, flexible,\nand trainable core representations.", "authors": ["Tanmay Gupta", "Kevin Shih", "Saurabh Singh", "Derek Hoiem"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00260v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00260v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00260v1", "published_time": "4/2/2017", "tags": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "title": "Aligned Image-Word Representations Improve Inductive Transfer Across\n  Vision-Language Tasks"}, {"abstract": "Recent works have proved that synthetic parallel data generated by existing\ntranslation models can be an effective solution to various neural machine\ntranslation (NMT) issues. In this study, we construct NMT systems using only\nsynthetic parallel data. As an effective alternative to real parallel data, we\nalso present a new type of synthetic parallel corpus. The proposed pseudo\nparallel data are distinct from previous approaches in that real and synthetic\nsentences are mixed on both sides of sentence pairs. Experiments on\nCzech-German and French-German translations demonstrate the efficacy of the\nproposed pseudo parallel corpus that guarantees not only both balanced and\ncompetitive performance for bidirectional translation but also substantial\nimprovement with the aid of a real parallel corpus.", "authors": ["Jaehong Park", "Byunggook Na", "Sungroh Yoon"], "category": "cs.CL", "comment": "10 pages, 2 figures, 3 tables", "img": "/static/thumbs/1704.00253v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00253v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00253v1", "published_time": "4/2/2017", "tags": ["cs.CL"], "title": "Building a Neural Machine Translation System Using Only Synthetic\n  Parallel Data"}, {"abstract": "Deep convolutional neural networks (CNN) have recently been shown to generate\npromising results for aesthetics assessment. However, the performance of these\ndeep CNN methods is often compromised by the constraint that the neural network\nonly takes the fixed-size input. To accommodate this requirement, input images\nneed to be transformed via cropping, warping, or padding, which often alter\nimage composition, reduce image resolution, or cause image distortion. Thus the\naesthetics of the original images is impaired because of potential loss of fine\ngrained details and holistic image layout. However, such fine grained details\nand holistic image layout is critical for evaluating an image\u0027s aesthetics. In\nthis paper, we present an Adaptive Layout-Aware Multi-Patch Convolutional\nNeural Network (A-Lamp CNN) architecture for photo aesthetic assessment. This\nnovel scheme is able to accept arbitrary sized images, and learn from both\nfined grained details and holistic image layout simultaneously. To enable\ntraining on these hybrid inputs, we extend the method by developing a dedicated\ndouble-subnet neural network structure, i.e. a Multi-Patch subnet and a\nLayout-Aware subnet. We further construct an aggregation layer to effectively\ncombine the hybrid features from these two subnets. Extensive experiments on\nthe large-scale aesthetics assessment benchmark (AVA) demonstrate significant\nperformance improvement over the state-of-the-art in photo aesthetic\nassessment.", "authors": ["Shuang Ma", "Jing Liu", "Chang Wen Chen"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00248v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00248v1", "num_discussion": 0, "originally_published_time": "4/2/2017", "pid": "1704.00248v1", "published_time": "4/2/2017", "tags": ["cs.CV"], "title": "A-Lamp: Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural\n  Network for Photo Aesthetic Assessment"}, {"abstract": "Since the events of the Arab Spring, there has been increased interest in\nusing social media to anticipate social unrest. While efforts have been made\ntoward automated unrest prediction, we focus on filtering the vast volume of\ntweets to identify tweets relevant to unrest, which can be provided to\ndownstream users for further analysis. We train a supervised classifier that is\nable to label Arabic language tweets as relevant to unrest with high\nreliability. We examine the relationship between training data size and\nperformance and investigate ways to optimize the model building process while\nminimizing cost. We also explore how confidence thresholds can be set to\nachieve desired levels of performance.", "authors": ["Alan Mishler", "Kevin Wonus", "Wendy Chambers", "Michael Bloodgood"], "category": "cs.CL", "comment": "7 pages, 8 figures, 3 tables; published in Proceedings of the 2017\n  IEEE 11th International Confere...", "img": "/static/thumbs/1702.06216v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.06216v2", "num_discussion": 0, "originally_published_time": "2/20/2017", "pid": "1702.06216v2", "published_time": "4/1/2017", "tags": ["cs.CL", "cs.IR", "cs.LG", "stat.ML", "H.3.3; I.2.6; I.2.7; I.5.4"], "title": "Filtering Tweets for Social Unrest"}, {"abstract": "It has been well demonstrated that adversarial examples, i.e., natural images\nwith visually imperceptible perturbations added, generally exist for deep\nnetworks to fail on image classification. In this paper, we extend adversarial\nexamples to semantic segmentation and object detection which are much more\ndifficult. Our observation is that both segmentation and detection are based on\nclassifying multiple targets on an image (e.g., the basic target is a pixel or\na receptive field in segmentation, and an object proposal in detection), which\ninspires us to optimize a loss function over a set of pixels/proposals for\ngenerating adversarial perturbations. Based on this idea, we propose a novel\nalgorithm named Dense Adversary Generation (DAG), which generates a large\nfamily of adversarial examples, and applies to a wide range of state-of-the-art\ndeep networks for segmentation and detection. We also find that the adversarial\nperturbations can be transferred across networks with different training data,\nbased on different architectures, and even for different recognition tasks. In\nparticular, the transferability across networks with the same architecture is\nmore significant than in other cases. Besides, summing up heterogeneous\nperturbations often leads to better transfer performance, which provides an\neffective method of black-box adversarial attack.", "authors": ["Cihang Xie", "Jianyu Wang", "Zhishuai Zhang", "Yuyin Zhou", "Lingxi Xie", "Alan Yuille"], "category": "cs.CV", "comment": "Submitted to ICCV 2017 (10 pages, 6 figures)", "img": "/static/thumbs/1703.08603v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.08603v2", "num_discussion": 0, "originally_published_time": "3/24/2017", "pid": "1703.08603v2", "published_time": "4/1/2017", "tags": ["cs.CV"], "title": "Adversarial Examples for Semantic Segmentation and Object Detection"}, {"abstract": "Deep learning methods exhibit promising performance for predictive modeling\nin healthcare, but two important challenges remain: -Data insufficiency:Often\nin healthcare predictive modeling, the sample size is insufficient for deep\nlearning methods to achieve satisfactory results. -Interpretation:The\nrepresentations learned by deep learning methods should align with medical\nknowledge. To address these challenges, we propose a GRaph-based Attention\nModel, GRAM that supplements electronic health records (EHR) with hierarchical\ninformation inherent to medical ontologies. Based on the data volume and the\nontology structure, GRAM represents a medical concept as a combination of its\nancestors in the ontology via an attention mechanism. We compared predictive\nperformance (i.e. accuracy, data needs, interpretability) of GRAM to various\nmethods including the recurrent neural network (RNN) in two sequential\ndiagnoses prediction tasks and one heart failure prediction task. Compared to\nthe basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely\nobserved in the training data and 3% improved area under the ROC curve for\npredicting heart failure using an order of magnitude less training data.\nAdditionally, unlike other methods, the medical concept representations learned\nby GRAM are well aligned with the medical ontology. Finally, GRAM exhibits\nintuitive attention behaviors by adaptively generalizing to higher level\nconcepts when facing data insufficiency at the lower level concepts.", "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1611.07012v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.07012v3", "num_discussion": 1, "originally_published_time": "11/21/2016", "pid": "1611.07012v3", "published_time": "4/1/2017", "tags": ["cs.LG", "stat.ML"], "title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning"}, {"abstract": "The orbital debris problem presents an opportunity for inter-agency and\ninternational cooperation toward the mutually beneficial goals of debris\nprevention, mitigation, remediation, and improved space situational awareness\n(SSA). Achieving these goals requires sharing orbital debris and other SSA\ndata. Toward this, I present an ontological architecture for the orbital debris\ndomain, taking steps in the creation of an orbital debris ontology (ODO). The\npurpose of this ontological system is to (I) represent general orbital debris\nand SSA domain knowledge, (II) structure, and standardize where needed, orbital\ndata and terminology, and (III) foster semantic interoperability and\ndata-sharing. In doing so I hope to (IV) contribute to solving the orbital\ndebris problem, improving peaceful global SSA, and ensuring safe space travel\nfor future generations.", "authors": ["Robert J. Rovetto"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1704.01014v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01014v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.01014v1", "published_time": "4/1/2017", "tags": ["cs.AI", "cs.DB"], "title": "An Ontological Architecture for Orbital Debris Data"}, {"abstract": "Many real-world applications are characterized by a number of conflicting\nperformance measures. As optimizing in a multi-objective setting leads to a set\nof non-dominated solutions, a preference function is required for selecting the\nsolution with the appropriate trade-off between the objectives. The question\nis: how good do estimations of these objectives have to be in order for the\nsolution maximizing the preference function to remain unchanged? In this paper,\nwe introduce the concept of preference radius to characterize the robustness of\nthe preference function and provide guidelines for controlling the quality of\nestimations in the multi-objective setting. More specifically, we provide a\ngeneral formulation of multi-objective optimization under the bandits setting.\nWe show how the preference radius relates to the optimal gap and use it to\nprovide a theoretical analysis of the Thompson sampling algorithm from\nmultivariate normal priors. We finally present experiments to support the\ntheoretical results and highlight the fact that one cannot simply scalarize\nmulti-objective problems into single-objective problems.", "authors": ["Audrey Durand", "Christian Gagn\u00e9"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1701.01095v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.01095v2", "num_discussion": 0, "originally_published_time": "1/4/2017", "pid": "1701.01095v2", "published_time": "4/1/2017", "tags": ["cs.LG", "stat.ML"], "title": "Estimating Quality in Multi-Objective Bandits Optimization"}, {"abstract": "In this paper two sequential algorithms for learning analysis operators are\npresented, which are built upon the same optimisation principle underlying both\nAnalysis K-SVD and Analysis SimCO and use a stochastic gradient descent\napproach similar to ASimCO. The sequential analysis operator learning (SAOL)\nalgorithm is based on projected gradient descent with an appropriately chosen\nstep size while the implicit SAOL (ISAOL) algorithm avoids choosing a step size\naltogether by using a strategy inspired by the implicit Euler scheme for\nsolving ordinary differential equations. Both algorithms are tested on\nsynthetic and image data in comparison to Analysis SimCO and found to give\nslightly better recovery rates resp. decay of the objective function. In a\nfinal denoising experiment the presented algorithms are again shown to perform\nwell in comparison to the state of the art algorithm ASimCO.", "authors": ["Michael Sandbichler", "Karin Schnass"], "category": "cs.LG", "comment": "10 pages, 11 figures", "img": "/static/thumbs/1704.00227v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00227v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00227v1", "published_time": "4/1/2017", "tags": ["cs.LG", "math.NA"], "title": "Sequential Learning of Analysis Operators"}, {"abstract": "The intersection of causal inference and machine learning is a rapidly\nadvancing field. We propose a new approach, the method of direct estimation,\nthat draws on both traditions in order to obtain nonparametric estimates of\ntreatment effects. The approach focuses on estimating the effect of\nfluctuations in a treatment variable on an outcome. A tensor-spline\nimplementation enables rich interactions between functional bases allowing for\nthe approach to capture treatment/covariate interactions. We show how new\ninnovations in Bayesian sparse modeling readily handle the proposed framework,\nand then document its performance in simulation and applied examples.\nFurthermore we show how the method of direct estimation can easily extend to\nstructural estimators commonly used in a variety of disciplines, like\ninstrumental variables, mediation analysis, and sequential g-estimation.", "authors": ["Marc Ratkovic", "Dustin Tingley"], "category": "stat.ML", "comment": "Unpublished manuscript", "img": "/static/thumbs/1703.05849v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.05849v2", "num_discussion": 0, "originally_published_time": "3/16/2017", "pid": "1703.05849v2", "published_time": "4/1/2017", "tags": ["stat.ML", "stat.ME", "62G08, 46N30, 62P20, 62P25"], "title": "Causal Inference through the Method of Direct Estimation"}, {"abstract": "Implicit discourse relation classification is of great challenge due to the\nlack of connectives as strong linguistic cues, which motivates the use of\nannotated implicit connectives to improve the recognition. We propose a feature\nimitation framework in which an implicit relation network is driven to learn\nfrom another neural network with access to connectives, and thus encouraged to\nextract similarly salient features for accurate classification. We develop an\nadversarial model to enable an adaptive imitation scheme through competition\nbetween the implicit network and a rival feature discriminator. Our method\neffectively transfers discriminability of connectives to the implicit features,\nand achieves state-of-the-art performance on the PDTB benchmark.", "authors": ["Lianhui Qin", "Zhisong Zhang", "Hai Zhao", "Zhiting Hu", "Eric P. Xing"], "category": "cs.CL", "comment": "To appear in ACL2017", "img": "/static/thumbs/1704.00217v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00217v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00217v1", "published_time": "4/1/2017", "tags": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "title": "Adversarial Connective-exploiting Networks for Implicit Discourse\n  Relation Classification"}, {"abstract": "As the title suggests, we will describe (and justify through the presentation\nof some of the relevant mathematics) prediction methodologies for sensor\nmeasurements. This exposition will mainly be concerned with the mathematics\nrelated to modeling the sensor measurements.", "authors": ["Robert A. Murphy"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1704.00207v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00207v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00207v1", "published_time": "4/1/2017", "tags": ["cs.NE", "60J70"], "title": "A Brownian Motion Model and Extreme Belief Machine for Modeling Sensor\n  Data Measurements"}, {"abstract": "We show that a simple modification of the 1-nearest neighbor classifier\nyields a strongly Bayes consistent learner. Prior to this work, the only\nstrongly Bayes consistent proximity-based method was the k-nearest neighbor\nclassifier, for k growing appropriately with sample size. We will argue that a\nmargin-regularized 1-NN enjoys considerable statistical and algorithmic\nadvantages over the k-NN classifier. These include user-friendly finite-sample\nerror bounds, as well as time- and memory-efficient learning and test-point\nevaluation algorithms with a principled speed-accuracy tradeoff. Encouraging\nempirical results are reported.", "authors": ["Aryeh Kontorovich", "Roi Weiss"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1407.0208v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1407.0208v3", "num_discussion": 0, "originally_published_time": "7/1/2014", "pid": "1407.0208v3", "published_time": "4/1/2017", "tags": ["cs.LG", "stat.ML"], "title": "A Bayes consistent 1-NN classifier"}, {"abstract": "Owing to the success of deep learning techniques for tasks such as Q/A and\ntext-based dialog, there is an increasing demand for AI agents in several\ndomains such as retail, travel, entertainment, etc. that can carry on\nmultimodal conversations with humans employing both text and images within a\ndialog seamlessly. However, deep learning research is this area has been\nlimited primarily due to the lack of availability of large-scale, open\nconversation datasets. To overcome this bottleneck, in this paper we introduce\nthe task of multi-modal, domain-aware conversations, and propose the MMD\nbenchmark dataset to- wards this task. This dataset was gathered by working in\nclose coordination with large number of domain experts in the retail domain and\nconsists of over 150K conversation sessions between shoppers and sales agents.\nWith this dataset, we propose 5 new sub-tasks for multimodal conversations\nalong with their evaluation methodology. We also propose two novel multi-modal\ndeep learning models in the encode- attend-decode paradigm and demonstrate\ntheir performance on two of the sub-tasks, namely text response generation and\nbest image response selection. These experiments serve to establish baseline\nperformance numbers and open new directions of research for each of these\nsub-tasks.", "authors": ["Amrita Saha", "Mitesh Khapra", "Karthik Sankaranarayanan"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.00200v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00200v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00200v1", "published_time": "4/1/2017", "tags": ["cs.CL"], "title": "Multimodal Dialogs (MMD): A large-scale dataset for studying multimodal\n  domain-aware conversations"}, {"abstract": "This paper presents new properties of image separation by perceptrons. For\nexample, surprisingly, separation of a new image from a very large set of known\nimages is almost always possible in moderately high dimensions by linear\nperceptrons. This observation is supported by numerical experiments and new\nresults on linear separability of random $M$-element sets in $\\mathbb{R}^n$.\nBased on fundamental properties of measure concentration, we show that for\n$M\u003ca\\exp(b{n})$ random $M$-element sets in $\\mathbb{R}^n$ are linearly\nseparable with probability $p$, $p\u003e1-\\vartheta$, where $1\u003e\\vartheta\u003e0$ is a\ngiven small constant. Exact values of $a,b\u003e0$ depend on the probability\ndistribution that determines how the random $M$-element sets are drawn, and on\nthe constant $\\vartheta$. The results are particularly important for the\ndevelopment, analysis, and assessment of machine learning methods and\nalgorithms in high dimension. Immediate applications include non-iterative\none-shot correction of unavoidable mistakes of legacy Artificial Intelligence\nsystems. Theoretical statements are illustrated with numerical examples.", "authors": ["A. N. Gorban", "I. Y. Tyukin"], "category": "cs.LG", "comment": "5 pages, added numerical experiments, extended bibliography,\n  corrected misprints", "img": "/static/thumbs/1703.01203v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01203v2", "num_discussion": 0, "originally_published_time": "3/3/2017", "pid": "1703.01203v2", "published_time": "4/1/2017", "tags": ["cs.LG", "cs.AI", "68T10", "I.2.6"], "title": "Stochastic Separation Theorems"}, {"abstract": "The purpose of this manuscript is to derive new convergence results for\nseveral subgradient methods for minimizing nonsmooth convex functions with\nH\\\"olderian growth. The growth condition is satisfied in many applications and\nincludes functions with quadratic growth and functions with weakly sharp minima\nas special cases. To this end there are four main contributions. First, for a\nconstant and sufficiently small stepsize, we show that the subgradient method\nachieves linear convergence up to a certain region including the optimal set\nwith error of the order of the stepsize. Second, we derive nonergodic\nconvergence rates for the subgradient method under nonsummable decaying\nstepsizes. Thirdly if appropriate problem parameters are known we derive a\npossibly-summable stepsize which obtains a much faster convergence rate.\nFinally we develop a novel \"descending stairs\" stepsize which obtains this\nfaster convergence rate but also obtains linear convergence for the special\ncase of weakly sharp functions. We also develop a variant of the \"descending\nstairs\" stepsize which achieves essentially the same convergence rate without\nrequiring an error bound constant which is difficult to estimate in practice.", "authors": ["Patrick R. Johnstone", "Pierre Moulin"], "category": "math.OC", "comment": "49 pages", "img": "/static/thumbs/1704.00196v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00196v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00196v1", "published_time": "4/1/2017", "tags": ["math.OC", "cs.LG", "cs.NA", "math.NA"], "title": "Faster Subgradient Methods for Functions with H\u00f6lderian Growth"}, {"abstract": "Categorical variables are a natural choice for representing discrete\nstructure in the world. However, stochastic neural networks rarely use\ncategorical latent variables due to the inability to backpropagate through\nsamples. In this work, we present an efficient gradient estimator that replaces\nthe non-differentiable sample from a categorical distribution with a\ndifferentiable sample from a novel Gumbel-Softmax distribution. This\ndistribution has the essential property that it can be smoothly annealed into a\ncategorical distribution. We show that our Gumbel-Softmax estimator outperforms\nstate-of-the-art gradient estimators on structured output prediction and\nunsupervised generative modeling tasks with categorical latent variables, and\nenables large speedups on semi-supervised classification.", "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1611.01144v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.01144v4", "num_discussion": 0, "originally_published_time": "11/3/2016", "pid": "1611.01144v4", "published_time": "4/1/2017", "tags": ["stat.ML", "cs.LG"], "title": "Categorical Reparameterization with Gumbel-Softmax"}, {"abstract": "Many of the state-of-the-art algorithms for gesture recognition are based on\nConditional Random Fields (CRFs). Successful approaches, such as the\nLatent-Dynamic CRFs, extend the CRF by incorporating latent variables, whose\nvalues are mapped to the values of the labels. In this paper we propose a novel\nmethodology to set the latent values according to the gesture complexity. We\nuse an heuristic that iterates through the samples associated with each label\nvalue, stimating their complexity. We then use it to assign the latent values\nto the label values. We evaluate our method on the task of recognizing human\ngestures from video streams. The experiments were performed in binary datasets,\ngenerated by grouping different labels. Our results demonstrate that our\napproach outperforms the arbitrary one in many cases, increasing the accuracy\nby up to 10%.", "authors": ["Manoel Horta Ribeiro", "Bruno Teixeira", "Ant\u00f4nio Ot\u00e1vio Fernandes", "Wagner Meira Jr.", "Erickson R. Nascimento"], "category": "cs.CV", "comment": "Conference paper published at 2016 29th SIBGRAPI, Conference on\n  Graphics, Patterns and Images (SIB...", "img": "/static/thumbs/1704.00180v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00180v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00180v1", "published_time": "4/1/2017", "tags": ["cs.CV"], "title": "Complexity-Aware Assignment of Latent Values in Discriminative Models\n  for Accurate Gesture Recognition"}, {"abstract": "Citation sentiment analysis is an important task in scientific paper\nanalysis. Existing machine learning techniques for citation sentiment analysis\nare focusing on labor-intensive feature engineering, which requires large\nannotated corpus. As an automatic feature extraction tool, word2vec has been\nsuccessfully applied to sentiment analysis of short texts. In this work, I\nconducted empirical research with the question: how well does word2vec work on\nthe sentiment analysis of citations? The proposed method constructed sentence\nvectors (sent2vec) by averaging the word embeddings, which were learned from\nAnthology Collections (ACL-Embeddings). I also investigated polarity-specific\nword embeddings (PS-Embeddings) for classifying positive and negative\ncitations. The sentence vectors formed a feature space, to which the examined\ncitation sentence was mapped to. Those features were input into classifiers\n(support vector machines) for supervised classification. Using\n10-cross-validation scheme, evaluation was conducted on a set of annotated\ncitations. The results showed that word embeddings are effective on classifying\npositive and negative citations. However, hand-crafted features performed\nbetter for the overall classification.", "authors": ["Haixia Liu"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.00177v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00177v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00177v1", "published_time": "4/1/2017", "tags": ["cs.CL"], "title": "Sentiment Analysis of Citations Using Word2vec"}, {"abstract": "Regression based methods are widely used for 3D and 2D human pose estimation,\nbut the performance is not satisfactory. One problem is that the structural\ninformation of the pose is not well exploited in the existing methods. In this\nwork, we propose a structure-aware regression approach. It adopts a\nreparameterized pose representation using bones instead of joints. It exploits\nthe joint connection structure and proposes a compositional loss function that\nencodes the long range interactions in the pose. It is simple, effective, and\ngeneral. Comprehensive evaluation validates the effectiveness of our approach.\nIt significantly advances the state-of-the-art on Human3.6M and achieves\nstate-of-the-art results on MPII, in a unified framework for 3D and 2D pose\nregression.", "authors": ["Xiao Sun", "Jiaxiang Shang", "Shuang Liang", "Yichen Wei"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00159v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00159v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00159v1", "published_time": "4/1/2017", "tags": ["cs.CV"], "title": "Compositional Human Pose Regression"}, {"abstract": "We introduce a novel validation framework to measure the true robustness of\nlearning models for real-world applications by creating source-inclusive and\nsource-exclusive partitions in a dataset via clustering. We develop a\nrobustness metric derived from source-aware lower and upper bounds of model\naccuracy even when data source labels are not readily available. We clearly\ndemonstrate that even on a well-explored dataset like MNIST, challenging\ntraining scenarios can be constructed under the proposed assessment framework\nfor two separate yet equally important applications: i) more rigorous learning\nmodel comparison and ii) dataset adequacy evaluation. In addition, our findings\nnot only promise a more complete identification of trade-offs between model\ncomplexity, accuracy and robustness but can also help researchers optimize\ntheir efforts in data collection by identifying the less robust and more\nchallenging class labels.", "authors": ["Ozsel Kilinc", "Ismail Uysal"], "category": "cs.LG", "comment": "Submitted to UAI 2017", "img": "/static/thumbs/1704.00158v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00158v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00158v1", "published_time": "4/1/2017", "tags": ["cs.LG"], "title": "Clustering-based Source-aware Assessment of True Robustness for Learning\n  Models"}, {"abstract": "Of late, weakly supervised object detection is with great importance in\nobject recognition. Based on deep learning, weakly supervised detectors have\nachieved many promising results. However, compared with fully supervised\ndetection, it is more challenging to train deep network based detectors in a\nweakly supervised manner. Here we formulate weakly supervised detection as a\nMultiple Instance Learning (MIL) problem, where instance classifiers (object\ndetectors) are put into the network as hidden nodes. We propose a novel online\ninstance classifier refinement algorithm to integrate MIL and the instance\nclassifier refinement procedure into a single deep network, and train the\nnetwork end-to-end with only image-level supervision, i.e., without object\nlocation information. More precisely, instance labels inferred from weak\nsupervision are propagated to their spatially overlapped instances to refine\ninstance classifier online. The iterative instance classifier refinement\nprocedure is implemented using multiple streams in deep network, where each\nstream supervises its latter stream. Weakly supervised object detection\nexperiments are carried out on the challenging PASCAL VOC 2007 and 2012\nbenchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the\nprevious state-of-the-art.", "authors": ["Peng Tang", "Xinggang Wang", "Xiang Bai", "Wenyu Liu"], "category": "cs.CV", "comment": "Accepted by CVPR 2017, IEEE Conference on Computer Vision and Pattern\n  Recognition 2017", "img": "/static/thumbs/1704.00138v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00138v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00138v1", "published_time": "4/1/2017", "tags": ["cs.CV"], "title": "Multiple Instance Detection Network with Online Instance Classifier\n  Refinement"}, {"abstract": "Programming languages themselves have a limited number of reserved keywords\nand character based tokens that define the language specification. However,\nprogrammers have a rich use of natural language within their code through\ncomments, text literals and naming entities. The programmer defined names that\ncan be found in source code are a rich source of information to build a high\nlevel understanding of the project. The goal of this paper is to apply topic\nmodeling to names used in over 13.6 million repositories and perceive the\ninferred topics. One of the problems in such a study is the occurrence of\nduplicate repositories not officially marked as forks (obscure forks). We show\nhow to address it using the same identifiers which are extracted for topic\nmodeling.\n  We open with a discussion on naming in source code, we then elaborate on our\napproach to remove exact duplicate and fuzzy duplicate repositories using\nLocality Sensitive Hashing on the bag-of-words model and then discuss our work\non topic modeling; and finally present the results from our data analysis\ntogether with open-access to the source code, tools and datasets.", "authors": ["Vadim Markovtsev", "Eiso Kant"], "category": "cs.PL", "comment": "11 pages", "img": "/static/thumbs/1704.00135v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00135v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00135v1", "published_time": "4/1/2017", "tags": ["cs.PL", "cs.CL"], "title": "Topic modeling of public repositories at scale using names in source\n  code"}, {"abstract": "Language Models based on recurrent neural networks have dominated recent\nimage caption generation tasks. In this paper, we introduce a Language CNN\nmodel which is suitable for statistical language modeling tasks and shows\ncompetitive performance in image captioning. In contrast to previous models\nwhich predict next word based on one previous word and hidden state, our\nlanguage CNN is fed with all the previous words and can model the long-range\ndependencies of history words, which are critical for image captioning. The\neffectiveness of our approach is validated on two datasets MS COCO and\nFlickr30K. Our extensive experimental results show that our method outperforms\nthe vanilla recurrent neural network based language models and is competitive\nwith the state-of-the-art methods.", "authors": ["Jiuxiang Gu", "Gang Wang", "Jianfei Cai", "Tsuhan Chen"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.07086v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.07086v2", "num_discussion": 0, "originally_published_time": "12/21/2016", "pid": "1612.07086v2", "published_time": "4/1/2017", "tags": ["cs.CV", "cs.LG"], "title": "An Empirical Study of Language CNN for Image Captioning"}, {"abstract": "In this paper, scalable Whole Slide Imaging (sWSI), a novel high-throughput,\ncost-effective and robust whole slide imaging system on both Android and iOS\nplatforms is introduced and analyzed. With sWSI, most mainstream smartphone\nconnected to a optical eyepiece of any manually controlled microscope can be\nautomatically controlled to capture sequences of mega-pixel fields of views\nthat are synthesized into giga-pixel virtual slides. Remote servers carry out\nthe majority of computation asynchronously to support clients running at\nsatisfying frame rates without sacrificing image quality nor robustness. A\ntypical 15x15mm sample can be digitized in 30 seconds with 4X or in 3 minutes\nwith 10X object magnification, costing under $1. The virtual slide quality is\nconsidered comparable to existing high-end scanners thus satisfying for\nclinical usage by surveyed pathologies. The scan procedure with features such\nas supporting magnification up to 100x, recoding z-stacks,\nspecimen-type-neutral and giving real-time feedback, is deemed\nwork-flow-friendly and reliable.", "authors": ["Shuoxin Ma", "Tan Wang"], "category": "physics.bio-ph", "comment": "", "img": "/static/thumbs/1704.01088v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01088v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.01088v1", "published_time": "4/1/2017", "tags": ["physics.bio-ph", "cs.CV"], "title": "sWSI: A Low-cost and Commercial-quality Whole Slide Imaging System on\n  Android and iOS Smartphones"}, {"abstract": "Global recruitment into radical Islamic movements has spurred renewed\ninterest in the appeal of political extremism. Is the appeal a rational\nresponse to material conditions or is it the expression of psychological and\npersonality disorders associated with aggressive behavior, intolerance,\nconspiratorial imagination, and paranoia? Empirical answers using surveys have\nbeen limited by lack of access to extremist groups, while field studies have\nlacked psychological measures and failed to compare extremists with contrast\ngroups. We revisit the debate over the appeal of extremism in the U.S. context\nby comparing publicly available Twitter messages written by over 355,000\npolitical extremist followers with messages written by non-extremist U.S.\nusers. Analysis of text-based psychological indicators supports the moral\nfoundation theory which identifies emotion as a critical factor in determining\npolitical orientation of individuals. Extremist followers also differ from\nothers in four of the Big Five personality traits.", "authors": ["Meysam Alizadeh", "Ingmar Weber", "Claudio Cioffi-Revilla", "Santo Fortunato", "Michael Macy"], "category": "cs.CL", "comment": "11 pages, 3 figures", "img": "/static/thumbs/1704.00119v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00119v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00119v1", "published_time": "4/1/2017", "tags": ["cs.CL", "cs.CY", "cs.SI", "physics.soc-ph"], "title": "Psychological and Personality Profiles of Political Extremists"}, {"abstract": "We revisit the stochastic limited-memory BFGS (L-BFGS) algorithm. By\nproposing a new framework for analyzing convergence, we theoretically improve\nthe (linear) convergence rates and computational complexities of the stochastic\nL-BFGS algorithms in previous works. In addition, we propose several practical\nacceleration strategies to speed up the empirical performance of such\nalgorithms. We also provide theoretical analyses for most of the strategies.\nExperiments on large-scale logistic and ridge regression problems demonstrate\nthat our proposed strategies yield significant improvements via-\\`a-vis\ncompeting state-of-the-art algorithms.", "authors": ["Renbo Zhao", "William B. Haskell", "Vincent Y. F. Tan"], "category": "math.OC", "comment": "", "img": "/static/thumbs/1704.00116v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00116v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00116v1", "published_time": "4/1/2017", "tags": ["math.OC", "cs.IT", "math.IT", "stat.ML"], "title": "Stochastic L-BFGS Revisited: Improved Convergence Rates and Practical\n  Acceleration Strategies"}, {"abstract": "Data quality assessment and data cleaning are context-dependent activities.\nMotivated by this observation, we propose the Ontological Multidimensional Data\nModel (OMD model), which can be used to model and represent contexts as\nlogic-based ontologies. The data under assessment is mapped into the context,\nfor additional analysis, processing, and quality data extraction. The resulting\ncontexts allow for the representation of dimensions, and multidimensional data\nquality assessment becomes possible. At the core of a multidimensional context\nwe include a generalized multidimensional data model and a Datalog+/- ontology\nwith provably good properties in terms of query answering. These main\ncomponents are used to represent dimension hierarchies, dimensional\nconstraints, dimensional rules, and define predicates for quality data\nspecification. Query answering relies upon and triggers navigation through\ndimension hierarchies, and becomes the basic tool for the extraction of quality\ndata. The OMD model is interesting per se, beyond applications to data quality.\nIt allows for a logic-based, and computationally tractable representation of\nmultidimensional data, extending previous multidimensional data models with\nadditional expressive power and functionalities.", "authors": ["Leopoldo Bertossi", "Mostafa Milani"], "category": "cs.DB", "comment": "Journal submission. Extended version of RuleML\u002715 paper", "img": "/static/thumbs/1704.00115v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00115v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00115v1", "published_time": "4/1/2017", "tags": ["cs.DB", "cs.AI"], "title": "Ontological Multidimensional Data Models and Contextual Data Qality"}, {"abstract": "We present a novel method for variable selection in regression models when\ncovariates are measured with error. The iterative algorithm we propose,\nMEBoost, follows a path defined by estimating equations that correct for\ncovariate measurement error. Via simulation, we evaluated our method and\ncompare its performance to the recently-proposed Convex Conditioned Lasso\n(CoCoLasso) and to the \"naive\" Lasso which does not correct for measurement\nerror. Increasing the degree of measurement error increased prediction error\nand decreased the probability of accurate covariate selection, but this loss of\naccuracy was least pronounced when using MEBoost. We illustrate the use of\nMEBoost in practice by analyzing data from the Box Lunch Study, a clinical\ntrial in nutrition where several variables are based on self-report and hence\nmeasured with error.", "authors": ["Benjamin Brown", "Timothy Weaver", "Julian Wolfson"], "category": "stat.CO", "comment": "", "img": "/static/thumbs/1701.02349v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.02349v2", "num_discussion": 0, "originally_published_time": "1/9/2017", "pid": "1701.02349v2", "published_time": "4/1/2017", "tags": ["stat.CO", "stat.ML"], "title": "MEBoost: Variable Selection in the Presence of Measurement Error"}, {"abstract": "Suppose that we observe $y \\in \\mathbb{R}^n$ and $X \\in \\mathbb{R}^{n \\times\nm}$ in the following errors-in-variables model: \\begin{eqnarray*} y \u0026 = \u0026 X_0\n\\beta^* +\\epsilon \\\\ X \u0026 = \u0026 X_0 + W, \\end{eqnarray*} where $X_0$ is an $n\n\\times m$ design matrix with independent subgaussian row vectors, $\\epsilon \\in\n\\mathbb{R}^n$ is a noise vector and $W$ is a mean zero $n \\times m$ random\nnoise matrix with independent subgaussian column vectors, independent of $X_0$\nand $\\epsilon$. This model is significantly different from those analyzed in\nthe literature in the sense that we allow the measurement error for each\ncovariate to be a dependent vector across its $n$ observations. Such error\nstructures appear in the science literature when modeling the trial-to-trial\nfluctuations in response strength shared across a set of neurons.\n  Under sparsity and restrictive eigenvalue type of conditions, we show that\none is able to recover a sparse vector $\\beta^* \\in \\mathbb{R}^m$ from the\nmodel given a single observation matrix $X$ and the response vector $y$. We\nestablish consistency in estimating $\\beta^*$ and obtain the rates of\nconvergence in the $\\ell_q$ norm, where $q = 1, 2$. We show error bounds which\napproach that of the regular Lasso and the Dantzig selector in case the errors\nin $W$ are tending to 0. We analyze the convergence rates of the gradient\ndescent methods for solving the nonconvex programs and show that the composite\ngradient descent algorithm is guaranteed to converge at a geometric rate to a\nneighborhood of the global minimizers: the size of the neighborhood is bounded\nby the statistical error in the $\\ell_2$ norm. Our analysis reveals interesting\nconnections between computational and statistical efficiency and the\nconcentration of measure phenomenon in random matrix theory. We provide\nsimulation evidence illuminating the theoretical predictions.", "authors": ["Mark Rudelson", "Shuheng Zhou"], "category": "stat.ML", "comment": "8 Figures, one table, 99 pages. arXiv admin note: text overlap with\n  arXiv:1502.02355", "img": "/static/thumbs/1611.04701v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.04701v2", "num_discussion": 0, "originally_published_time": "11/15/2016", "pid": "1611.04701v2", "published_time": "4/1/2017", "tags": ["stat.ML", "math.ST", "stat.TH"], "title": "Errors-in-variables models with dependent measurements"}, {"abstract": "Ensembles of neural networks are known to be much more robust and accurate\nthan individual networks. However, training multiple deep networks for model\naveraging is computationally expensive. In this paper, we propose a method to\nobtain the seemingly contradictory goal of ensembling multiple neural networks\nat no additional training cost. We achieve this goal by training a single\nneural network, converging to several local minima along its optimization path\nand saving the model parameters. To obtain repeated rapid convergence, we\nleverage recent work on cyclic learning rate schedules. The resulting\ntechnique, which we refer to as Snapshot Ensembling, is simple, yet\nsurprisingly effective. We show in a series of experiments that our approach is\ncompatible with diverse network architectures and learning tasks. It\nconsistently yields lower error rates than state-of-the-art single models at no\nadditional training cost, and compares favorably with traditional network\nensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain\nerror rates of 3.4% and 17.4% respectively.", "authors": ["Gao Huang", "Yixuan Li", "Geoff Pleiss", "Zhuang Liu", "John E. Hopcroft", "Kilian Q. Weinberger"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.00109v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00109v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00109v1", "published_time": "4/1/2017", "tags": ["cs.LG"], "title": "Snapshot Ensembles: Train 1, get M for free"}, {"abstract": "Motivated by e-commerce, we study the online assortment optimization problem.\nThe seller offers an assortment, i.e. a subset of products, to each arriving\ncustomer, who then purchases one or no product from her offered assortment. A\ncustomer\u0027s purchase decision is governed by the underlying MultiNomial Logit\n(MNL) choice model. The seller aims to maximize the total revenue in a finite\nsales horizon, subject to resource constraints and uncertainty in the MNL\nchoice model. We first propose an efficient online policy which incurs a regret\n$\\tilde{O}(T^{2/3})$, where $T$ is the number of customers in the sales\nhorizon. Then, we propose a UCB policy that achieves a regret\n$\\tilde{O}(T^{1/2})$. Both regret bounds are sublinear in the number of\nassortments.", "authors": ["Wang Chi Cheung", "David Simchi-Levi"], "category": "cs.LG", "comment": "16 pages, 2 figures", "img": "/static/thumbs/1704.00108v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00108v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00108v1", "published_time": "4/1/2017", "tags": ["cs.LG"], "title": "Assortment Optimization under Unknown MultiNomial Logit Choice Models"}, {"abstract": "We describe a method to produce a network where current methods such as\nDeepFool have great difficulty producing adversarial samples. Our construction\nsuggests some insights into how deep networks work. We provide a reasonable\nanalyses that our construction is difficult to defeat, and show experimentally\nthat our method is hard to defeat using several standard networks and datasets.\nWe use our method to produce a system that can reliably detect whether an image\nis a picture of a real scene or not. Our system applies to images captured with\ndepth maps (RGBD images) and checks if a pair of image and depth map is\nconsistent. It relies on the relative difficulty of producing naturalistic\ndepth maps for images in post processing. We demonstrate that our system is\nrobust to adversarial examples built from currently known attacking approaches.", "authors": ["Jiajun Lu", "Theerasit Issaranon", "David Forsyth"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00103v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00103v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00103v1", "published_time": "4/1/2017", "tags": ["cs.CV"], "title": "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly"}, {"abstract": "In this paper, we establish a baseline for object symmetry detection in\ncomplex backgrounds by presenting a new benchmark and an end-to-end deep\nlearning approach, opening up a promising direction for symmetry detection in\nthe wild. The new benchmark, named Sym-PASCAL, spans challenges including\nobject diversity, multi-objects, part-invisibility, and various complex\nbackgrounds that are far beyond those in existing datasets. The proposed\nsymmetry detection approach, named Side-output Residual Network (SRN),\nleverages output Residual Units (RUs) to fit the errors between the object\nsymmetry groundtruth and the outputs of RUs. By stacking RUs in a\ndeep-to-shallow manner, SRN exploits the \u0027flow\u0027 of errors among multiple scales\nto ease the problems of fitting complex outputs with limited layers,\nsuppressing the complex backgrounds, and effectively matching object symmetry\nof different scales. Experimental results validate both the benchmark and its\nchallenging aspects related to realworld images, and the state-of-the-art\nperformance of our symmetry detection approach. The benchmark and the code for\nSRN are publicly available at https://github.com/KevinKecc/SRN.", "authors": ["Wei Ke", "Jie Chen", "Jianbin Jiao", "Guoying Zhao", "Qixiang Ye"], "category": "cs.CV", "comment": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition, 2017", "img": "/static/thumbs/1703.02243v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.02243v2", "num_discussion": 0, "originally_published_time": "3/7/2017", "pid": "1703.02243v2", "published_time": "4/1/2017", "tags": ["cs.CV"], "title": "SRN: Side-output Residual Network for Object Symmetry Detection in the\n  Wild"}, {"abstract": "This paper studies a problem of inverse visual path planning: creating a\nvisual scene from a first person action. Our conjecture is that the spatial\narrangement of a first person visual scene is deployed to afford an action, and\ntherefore, the action can be inversely used to synthesize a new scene such that\nthe action is feasible. As a proof-of-concept, we focus on linking visual\nexperiences induced by walking.\n  A key innovation of this paper is a concept of ActionTunnel---a 3D virtual\ntunnel along the future trajectory encoding what the wearer will visually\nexperience as moving into the scene. This connects two distinctive first person\nimages through similar walking paths. Our method takes a first person image\nwith a user defined future trajectory and outputs a new image that can afford\nthe future motion. The image is created by combining present and future\nActionTunnels in 3D where the missing pixels in adjoining area are computed by\na generative adversarial network. Our work can provide a travel across\ndifferent first person experiences in diverse real world scenes.", "authors": ["Shan Su", "Jianbo Shi", "Hyun Soo Park"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00098v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00098v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00098v1", "published_time": "4/1/2017", "tags": ["cs.CV"], "title": "Customizing First Person Image Through Desired Actions"}, {"abstract": "In this work, we propose a method to infer high dynamic range illumination\nfrom a single, limited field-of-view, low dynamic range photograph of an indoor\nscene. Inferring scene illumination from a single photograph is a challenging\nproblem. The pixel intensities observed in a photograph are a complex function\nof scene geometry, reflectance properties, and illumination. We introduce an\nend-to-end solution to this problem and propose a deep neural network that\ntakes the limited field-of-view photo as input and produces an environment map\nas a panorama and a light mask prediction over the panorama as the output. Our\ntechnique does not require special image capture or user input. We preprocess\nstandard low dynamic range panoramas by introducing novel light source\ndetection and warping methods on the panorama, and use the results with\ncorresponding limited field-of-view crops as training data. Our method does not\nrely on any assumptions on scene appearance, geometry, material properties, or\nlighting. This allows us to automatically recover high-quality illumination\nestimates that significantly outperform previous state-of-the-art methods.\nConsequently, using our illumination estimates for applications like 3D object\ninsertion lead to results that are photo-realistic, which we demonstrate over a\nlarge set of examples and via a user study.", "authors": ["Marc-Andr\u00e9 Gardner", "Kalyan Sunkavalli", "Ersin Yumer", "Xiaohui Shen", "Emiliano Gambaretto", "Christian Gagn\u00e9", "Jean-Fran\u00e7ois Lalonde"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00090v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00090v1", "num_discussion": 0, "originally_published_time": "4/1/2017", "pid": "1704.00090v1", "published_time": "4/1/2017", "tags": ["cs.CV"], "title": "Learning to Predict Indoor Illumination from a Single Image"}, {"abstract": "Estimating positions of world points from features observed in images is a\nkey problem in 3D reconstruction, image mosaicking, simultaneous localization\nand mapping and structure from motion. We consider a special instance in which\nthere is a dominant ground plane $\\mathcal{G}$ viewed from a parallel viewing\nplane $\\mathcal{S}$ above it. Such instances commonly arise, for example, in\naerial photography.\n  Consider a world point $g \\in \\mathcal{G}$ and its worst case reconstruction\nuncertainty $\\varepsilon(g,\\mathcal{S})$ obtained by merging \\emph{all}\npossible views of $g$ chosen from $\\mathcal{S}$. We first show that one can\npick two views $s_p$ and $s_q$ such that the uncertainty\n$\\varepsilon(g,\\{s_p,s_q\\})$ obtained using only these two views is almost as\ngood as (i.e. within a small constant factor of) $\\varepsilon(g,\\mathcal{S})$.\nNext, we extend the result to the entire ground plane $\\mathcal{G}$ and show\nthat one can pick a small subset of $\\mathcal{S\u0027} \\subseteq \\mathcal{S}$ (which\ngrows only linearly with the area of $\\mathcal{G}$) and still obtain a constant\nfactor approximation, for every point $g \\in \\mathcal{G}$, to the minimum worst\ncase estimate obtained by merging all views in $\\mathcal{S}$.\n  Our results provide a view selection mechanism with provable performance\nguarantees which can drastically increase the speed of scene reconstruction\nalgorithms. In addition to theoretical results, we demonstrate their\neffectiveness in an application where aerial imagery is used for monitoring\nfarms and orchards.", "authors": ["Cheng Peng", "Volkan Isler"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00085v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00085v1", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00085v1", "published_time": "3/31/2017", "tags": ["cs.CV"], "title": "Optimal Reconstruction with a Small Number of Views"}, {"abstract": "Adaptive tracking-by-detection approaches are popular for tracking arbitrary\nobjects. They treat the tracking problem as a classification task and use\nonline learning techniques to update the object model. However, these\napproaches are heavily invested in the efficiency and effectiveness of their\ndetectors. Evaluating a massive number of samples for each frame (e.g.,\nobtained by a sliding window) forces the detector to trade the accuracy in\nfavor of speed. Furthermore, misclassification of borderline samples in the\ndetector introduce accumulating errors in tracking. In this study, we propose a\nco-tracking based on the efficient cooperation of two detectors: a rapid\nadaptive exemplar-based detector and another more sophisticated but slower\ndetector with a long-term memory. The sampling labeling and co-learning of the\ndetectors are conducted by an uncertainty sampling unit, which improves the\nspeed and accuracy of the system. We also introduce a budgeting mechanism which\nprevents the unbounded growth in the number of examples in the first detector\nto maintain its rapid response. Experiments demonstrate the efficiency and\neffectiveness of the proposed tracker against its baselines and its superior\nperformance against state-of-the-art trackers on various benchmark videos.", "authors": ["Kourosh Meshgi", "Maryam Sadat Mirzaei", "Shigeyuki Oba", "Shin Ishii"], "category": "cs.CV", "comment": "Submitted to IEEE ICSIPA\u00272017", "img": "/static/thumbs/1704.00083v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00083v1", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00083v1", "published_time": "3/31/2017", "tags": ["cs.CV"], "title": "Efficient Asymmetric Co-Tracking using Uncertainty Sampling"}, {"abstract": "This paper proposes a geodesic-distance-based feature that encodes global\ninformation for improved video segmentation algorithms. The feature is a joint\nhistogram of intensity and geodesic distances, where the geodesic distances are\ncomputed as the shortest paths between superpixels via their boundaries. We\nalso incorporate adaptive voting weights and spatial pyramid configurations to\ninclude spatial information into the geodesic histogram feature and show that\nthis further improves results. The feature is generic and can be used as part\nof various algorithms. In experiments, we test the geodesic histogram feature\nby incorporating it into two existing video segmentation frameworks. This leads\nto significantly better performance in 3D video segmentation benchmarks on two\ndatasets.", "authors": ["Hieu Le", "Vu Nguyen", "Chen-Ping Yu", "Dimitris Samaras"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.00077v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00077v1", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00077v1", "published_time": "3/31/2017", "tags": ["cs.CV"], "title": "Geodesic Distance Histogram Feature for Video Segmentation"}, {"abstract": "We quantitatively investigate how machine learning models leak information\nabout the individual data records on which they were trained. We focus on the\nbasic membership inference attack: given a data record and black-box access to\na model, determine if the record was in the model\u0027s training dataset. To\nperform membership inference against a target model, we make adversarial use of\nmachine learning and train our own inference model to recognize differences in\nthe target model\u0027s predictions on the inputs that it trained on versus the\ninputs that it did not train on.\n  We empirically evaluate our inference techniques on classification models\ntrained by commercial \"machine learning as a service\" providers such as Google\nand Amazon. Using realistic datasets and classification tasks, including a\nhospital discharge dataset whose membership is sensitive from the privacy\nperspective, we show that these models can be vulnerable to membership\ninference attacks. We then investigate the factors that influence this leakage\nand evaluate mitigation strategies.", "authors": ["Reza Shokri", "Marco Stronati", "Congzheng Song", "Vitaly Shmatikov"], "category": "cs.CR", "comment": "In the proceedings of the IEEE Symposium on Security and Privacy,\n  2017", "img": "/static/thumbs/1610.05820v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.05820v2", "num_discussion": 0, "originally_published_time": "10/18/2016", "pid": "1610.05820v2", "published_time": "3/31/2017", "tags": ["cs.CR", "cs.LG", "stat.ML"], "title": "Membership Inference Attacks against Machine Learning Models"}, {"abstract": "An exciting branch of machine learning research focuses on methods for\nlearning, optimizing, and integrating unknown functions that are difficult or\ncostly to evaluate. A popular Bayesian approach to this problem uses a Gaussian\nprocess (GP) to construct a posterior distribution over the function of\ninterest given a set of observed measurements, and selects new points to\nevaluate using the statistics of this posterior. Here we extend these methods\nto exploit derivative information from the unknown function. We describe\nmethods for Bayesian optimization (BO) and Bayesian quadrature (BQ) in settings\nwhere first and second derivatives may be evaluated along with the function\nitself. We perform sampling-based inference in order to incorporate uncertainty\nover hyperparameters, and show that both hyperparameter and function\nuncertainty decrease much more rapidly when using derivative information.\nMoreover, we introduce techniques for overcoming ill-conditioning issues that\nhave plagued earlier methods for gradient-enhanced Gaussian processes and\nkriging. We illustrate the efficacy of these methods using applications to real\nand simulated Bayesian optimization and quadrature problems, and show that\nexploting derivatives can provide substantial gains over standard methods.", "authors": ["Anqi Wu", "Mikio C. Aoi", "Jonathan W. Pillow"], "category": "stat.ML", "comment": "20 pages, 8 figures", "img": "/static/thumbs/1704.00060v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00060v1", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00060v1", "published_time": "3/31/2017", "tags": ["stat.ML"], "title": "Exploiting gradients and Hessians in Bayesian optimization and Bayesian\n  quadrature"}, {"abstract": "This paper presents the Frames dataset (Frames is available at\nhttp://datasets.maluuba.com/Frames), a corpus of 1369 human-human dialogues\nwith an average of 15 turns per dialogue. We developed this dataset to study\nthe role of memory in goal-oriented dialogue systems. Based on Frames, we\nintroduce a task called frame tracking, which extends state tracking to a\nsetting where several states are tracked simultaneously. We propose a baseline\nmodel for this task. We show that Frames can also be used to study memory in\ndialogue management and information presentation through natural language\ngeneration.", "authors": ["Layla El Asri", "Hannes Schulz", "Shikhar Sharma", "Jeremie Zumer", "Justin Harris", "Emery Fine", "Rahul Mehrotra", "Kaheer Suleman"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.00057v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00057v1", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00057v1", "published_time": "3/31/2017", "tags": ["cs.CL"], "title": "Frames: A Corpus for Adding Memory to Goal-Oriented Dialogue Systems"}, {"abstract": "We present a novel cross-lingual transfer method for paradigm completion, the\ntask of mapping a lemma to its inflected forms, using a neural encoder-decoder\nmodel, the state of the art for the monolingual task. We use labeled data from\na high-resource language to increase performance on a low-resource language. In\nexperiments on 21 language pairs from four different language families, we\nobtain up to 58% higher accuracy than without transfer and show that even\nzero-shot and one-shot learning are possible. We further find that the degree\nof language relatedness strongly influences the ability to transfer\nmorphological knowledge.", "authors": ["Katharina Kann", "Ryan Cotterell", "Hinrich Sch\u00fctze"], "category": "cs.CL", "comment": "Accepted at ACL 2017", "img": "/static/thumbs/1704.00052v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00052v1", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00052v1", "published_time": "3/31/2017", "tags": ["cs.CL"], "title": "One-Shot Neural Cross-Lingual Transfer for Paradigm Completion"}, {"abstract": "This paper proposes to tackle open-domain question answering using Wikipedia\nas the unique knowledge source: the answer to any factoid question is a text\nspan in a Wikipedia article. This task of machine reading at scale combines the\nchallenges of document retrieval - finding the relevant articles - with that of\nmachine comprehension of text - identifying the answer spans from those\narticles. Our approach combines a search component based on bigram hashing and\nTF-IDF matching with a multi-layer recurrent neural network model trained to\ndetect answers in Wikipedia paragraphs. Our experiments on multiple existing QA\ndatasets indicate that (1) both modules are highly competitive with respect to\nexisting counterparts and (2) multitask learning using distant supervision on\ntheir combination is an effective complete system on this challenging task.", "authors": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.00051v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00051v1", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00051v1", "published_time": "3/31/2017", "tags": ["cs.CL"], "title": "Reading Wikipedia to Answer Open-Domain Questions"}, {"abstract": "In this paper, we propose an end-to-end neural network (NN) based EEG-speech\n(NES) modeling framework, in which three network structures are developed to\nmap imagined EEG signals to phonemes. The proposed NES models incorporate a\nlanguage model based EEG feature extraction layer, an acoustic feature mapping\nlayer, and a restricted Boltzmann machine (RBM) based the feature learning\nlayer. The NES models can jointly realize the representation of multichannel\nEEG signals and the projection of acoustic speech signals. Among three proposed\nNES models, two augmented networks utilize spoken EEG signals as either bias or\ngate information to strengthen the feature learning and translation of imagined\nEEG signals. Experimental results show that all three proposed NES models\noutperform the baseline support vector machine (SVM) method on EEG-speech\nclassification. With respect to binary classification, our approach achieves\ncomparable results relative to deep believe network approach.", "authors": ["Pengfei Sun", "Jun Qin"], "category": "cs.SD", "comment": "10 pages", "img": "/static/thumbs/1612.05369v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.05369v2", "num_discussion": 0, "originally_published_time": "12/16/2016", "pid": "1612.05369v2", "published_time": "3/31/2017", "tags": ["cs.SD", "cs.LG"], "title": "Neural networks based EEG-Speech Models"}];
var msg = "Showing most recent Arxiv papers:";
var render_format = "recent";
var username = "";
var numresults = "28247";
var show_prompt = "no";

var urlq = ''; // global will be read in to QueryString when load is done

// when page loads...
$(document).ready(function(){

	urlq = QueryString.q;

  // display message, if any
  if(msg !== '') { d3.select("#rtable").append('div').classed('msg', true).html(msg); }

  // add papers to #rtable
	var done = addPapers(10, false);
  if(done) { $("#loadmorebtn").hide(); }

  // set up inifinite scrolling for adding more papers
  $(window).on('scroll', function(){
    var scrollTop = $(document).scrollTop();
    var windowHeight = $(window).height();
    var bodyHeight = $(document).height() - windowHeight;
    var scrollPercentage = (scrollTop / bodyHeight);
    if(scrollPercentage > 0.9) {
      var done = addPapers(5, true);
      if(done) { $("#loadmorebtn").hide(); }
    }
  });

  // just in case scrolling is broken somehow, provide a button handler explicit
  $("#loadmorebtn").on('click', function(){
    var done = addPapers(5, true);
    if(done) { $("#loadmorebtn").hide(); }
  });

	if(!(typeof urlq == 'undefined')) {
		d3.select("#qfield").attr('value', urlq.replace(/\+/g, " "));
	}

  var vf = QueryString.vfilter; if(typeof vf === 'undefined') { vf = 'all'; }
  var tf = QueryString.timefilter; if(typeof tf === 'undefined') { tf = 'week'; }
  var link_endpoint = '/';
  if(render_format === 'recent') { link_endpoint = ''; }
  if(render_format === 'top') { link_endpoint = 'top'; }
  if(render_format === 'recommend') { link_endpoint = 'recommend'; }
  if(render_format === 'toptwtr') { link_endpoint = 'toptwtr'; }
  if(render_format === 'discussions') { link_endpoint = 'discussions'; }

  var time_ranges = ['day', '3days', 'week', 'month', 'year', 'alltime'];
  var time_txt = {'day':'Last day', '3days': 'Last 3 days', 'week': 'Last week', 'month': 'Last month', 'year': 'Last year', 'alltime': 'All time'}
  var time_range = tf;

  // set up time filtering options
  if(render_format === 'recommend' || render_format === 'top' || render_format === 'recent') {
    // insert version filtering options for these views
    var elt = d3.select('#recommend-time-choice');
    var vflink = vf === 'all' ? '1' : 'all'; // toggle only showing v1 or not
    if(render_format === 'recent') {
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'&vfilter='+vflink); // leave out timefilter from this page
    } else {
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range+'&vfilter='+vflink);
    }
    var delt = aelt.append('div').classed('vchoice', true).html('Only show v1');
    if(vf === '1') { delt.classed('vchoice-selected', true); }
  }

  if(render_format === 'recommend' || render_format === 'top') {
    // insert time filtering options for these two views
    var elt = d3.select('#recommend-time-choice');
    elt.append('div').classed('fdivider', true).html('|');
    for(var i=0;i<time_ranges.length;i++) {
      var time_range = time_ranges[i];
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range+'&vfilter='+vf);
      var delt = aelt.append('div').classed('timechoice', true).html(time_txt[time_range]);
      if(tf == time_range) { delt.classed('timechoice-selected', true); } // also render as chosen
    }
  }

  // time choices for top tweets
  if(render_format === 'toptwtr') {
    var tf = QueryString.timefilter; if(typeof tf === 'undefined') { tf = 'day'; } // default here is day
    var time_ranges = ['day', 'week', 'month'];
    var elt = d3.select('#recommend-time-choice');
    for(var i=0;i<time_ranges.length;i++) {
      var time_range = time_ranges[i];
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range);
      var delt = aelt.append('div').classed('timechoice', true).html(time_txt[time_range]);
      if(tf == time_range) { delt.classed('timechoice-selected', true); } // also render as chosen
    }
  }

  var xb = $("#xbanner");
  if(xb.length !== 0) {
    xb.click(function(){ $("#banner").slideUp('fast'); })
  }

  // in top tab: color current choice
  if( render_format === 'recent') { d3.select('#tabrecent').classed('tab-selected', true); }
  if( render_format === 'top') { d3.select('#tabtop').classed('tab-selected', true); }
  if( render_format === 'toptwtr') { d3.select('#tabtwtr').classed('tab-selected', true); }
  if( render_format === 'discussions') { d3.select('#tabdiscussions').classed('tab-selected', true); }
  if( render_format === 'recommend') { d3.select('#tabrec').classed('tab-selected', true); }
  if( render_format === 'library') { d3.select('#tablib').classed('tab-selected', true); }

  $("#goaway").on('click', function(){
    $("#prompt").slideUp('fast');
    $.post("/goaway", {}).done(function(data){ });
  });
});

</script>
</head>

<body>
<a href="https://github.com/karpathy/arxiv-sanity-preserver"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

<div id ="titdiv">

  <!-- User account information on top right -->
  <div id="userinfo">

    <form action="/login" method="post">
      User:
      <input type="text" name="username" class="input-no-border">
      Pass:
      <input type="password" name="password" class="input-no-border">
      <input type="submit" value="Login or Create" class="btn-fancy">
    </form>

  </div>

  <!-- Site information/banner on top left -->
	<a href="/">
	<div id="tittxt">
		<h1>Arxiv Sanity Preserver</h1>
		Built in spare time by <a href="https://twitter.com/karpathy">@karpathy</a> to accelerate research.<br>
		Serving last 28247 papers from cs.[CV|CL|LG|AI|NE]/stat.ML
	</div>
	</a>
</div>

<div id="flashesdiv">



</div>


<div id="banner">
  <div style="float:right;cursor:pointer;" id="xbanner">X</div>
  New to arxiv-sanity? Check out the <a href="https://youtu.be/S2GY3gh6qC8" target="_blank">introduction video</a>.
</div>


<div id="sbox">
  <form action="/search" method="get">
  	<input name="q" type="text" id="qfield">
  </form>
  <div id="search_hint"></div>
</div>



<div id="pagebar">
  <div class="pagelink" id="tabrecent"><a href="/">most recent</a></div>
  <div class="pagelink" id="tabtop"><a href="/top">top recent</a></div>
  <div class="pagelink" id="tabtwtr"><a href="/toptwtr">top hype</a></div>
  <div class="pagelink" id="tabdiscussions"><a href="/discussions">recent discussions</a></div>
  <div class="pagelink" id="tabrec"><a href="/recommend">recommended</a></div>
  <div class="pagelink" id="tablib"><a href="/library">library</a></div>
</div>

<!-- this div will be rendered into dynamcially at init with JS -->
<div id="recommend-time-choice" class="centerdiv"></div>

<div id="maindiv">

<div id="rtable"></div>

<div id="loadmore">
  <button id="loadmorebtn">Load more</button>
</div>

</div>

<br><br><br><br><br><br>
</body>

</html>"""
